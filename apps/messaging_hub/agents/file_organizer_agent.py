"""ãƒ•ã‚¡ã‚¤ãƒ«æ•´ç†ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ.

ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®åˆ†æã€ãƒ•ã‚¡ã‚¤ãƒ«æ•´ç†ã€é‡è¤‡æ¤œå‡ºã€ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’æ‹…å½“ã€‚

ä½¿ç”¨ä¾‹:
    >>> agent = FileOrganizerAgent(gateway)
    >>> analysis = await agent.analyze_directory("~/Downloads")
    >>> result = await agent.organize("~/Downloads", dry_run=True)
"""

from __future__ import annotations

import hashlib
import logging
from collections import defaultdict
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import TYPE_CHECKING, Any


if TYPE_CHECKING:
    from agentflow.skills.gateway import SkillGateway


@dataclass
class DirectoryAnalysis:
    """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåˆ†æçµæœ.

    Attributes:
        path: åˆ†æãƒ‘ã‚¹
        total_files: ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°
        total_dirs: ç·ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ•°
        total_size_bytes: ç·ã‚µã‚¤ã‚ºï¼ˆãƒã‚¤ãƒˆï¼‰
        by_category: ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ
        by_extension: æ‹¡å¼µå­åˆ¥çµ±è¨ˆ
        old_files: å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ
        large_files: å¤§ãã„ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ
        empty_dirs: ç©ºãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒªã‚¹ãƒˆ
        recommendations: æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
    """

    path: str
    total_files: int = 0
    total_dirs: int = 0
    total_size_bytes: int = 0
    by_category: dict[str, dict[str, Any]] = field(default_factory=dict)
    by_extension: dict[str, int] = field(default_factory=dict)
    old_files: list[dict[str, Any]] = field(default_factory=list)
    large_files: list[dict[str, Any]] = field(default_factory=list)
    empty_dirs: list[str] = field(default_factory=list)
    recommendations: list[str] = field(default_factory=list)
    analyzed_at: datetime = field(default_factory=datetime.now)

    def to_dict(self) -> dict[str, Any]:
        """è¾æ›¸ã«å¤‰æ›."""
        return {
            "path": self.path,
            "total_files": self.total_files,
            "total_dirs": self.total_dirs,
            "total_size_mb": round(self.total_size_bytes / (1024 * 1024), 2),
            "by_category": self.by_category,
            "by_extension": self.by_extension,
            "old_files_count": len(self.old_files),
            "large_files_count": len(self.large_files),
            "empty_dirs_count": len(self.empty_dirs),
            "recommendations": self.recommendations,
            "analyzed_at": self.analyzed_at.isoformat(),
        }


@dataclass
class OrganizationResult:
    """æ•´ç†çµæœ.

    Attributes:
        files_moved: ç§»å‹•ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«æ•°
        files_renamed: ãƒªãƒãƒ¼ãƒ ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«æ•°
        dirs_created: ä½œæˆã—ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ•°
        errors: ã‚¨ãƒ©ãƒ¼ãƒªã‚¹ãƒˆ
        actions: å®Ÿè¡Œã—ãŸã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒªã‚¹ãƒˆ
        dry_run: ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ã‹
    """

    files_moved: int = 0
    files_renamed: int = 0
    dirs_created: int = 0
    errors: list[str] = field(default_factory=list)
    actions: list[dict[str, Any]] = field(default_factory=list)
    dry_run: bool = True

    def to_dict(self) -> dict[str, Any]:
        """è¾æ›¸ã«å¤‰æ›."""
        return {
            "files_moved": self.files_moved,
            "files_renamed": self.files_renamed,
            "dirs_created": self.dirs_created,
            "errors": self.errors,
            "actions": self.actions[:50],  # æœ€å¤§50ä»¶
            "total_actions": len(self.actions),
            "dry_run": self.dry_run,
        }


@dataclass
class DuplicateGroup:
    """é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—.

    Attributes:
        hash: ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚·ãƒ¥
        size: ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º
        files: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ãƒªã‚¹ãƒˆ
    """

    hash: str
    size: int
    files: list[str]

    def to_dict(self) -> dict[str, Any]:
        """è¾æ›¸ã«å¤‰æ›."""
        return {
            "hash": self.hash[:16] + "...",
            "size_mb": round(self.size / (1024 * 1024), 2),
            "files": self.files,
            "duplicate_count": len(self.files) - 1,
            "potential_savings_mb": round(self.size * (len(self.files) - 1) / (1024 * 1024), 2),
        }


class FileOrganizerAgent:
    """ãƒ•ã‚¡ã‚¤ãƒ«æ•´ç†ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ.

    SkillGatewayçµŒç”±ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‚’æ“ä½œã—ã€
    ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãªãƒ•ã‚¡ã‚¤ãƒ«æ•´ç†ã‚’è¡Œã†ã€‚
    """

    # ã‚«ãƒ†ã‚´ãƒªå®šç¾©
    CATEGORIES = {
        "documents": {
            "extensions": {
                ".pdf",
                ".doc",
                ".docx",
                ".xls",
                ".xlsx",
                ".ppt",
                ".pptx",
                ".txt",
                ".md",
                ".rtf",
                ".odt",
            },
            "emoji": "ğŸ“„",
        },
        "images": {
            "extensions": {
                ".jpg",
                ".jpeg",
                ".png",
                ".gif",
                ".bmp",
                ".svg",
                ".webp",
                ".ico",
                ".tiff",
                ".raw",
            },
            "emoji": "ğŸ–¼ï¸",
        },
        "videos": {
            "extensions": {".mp4", ".avi", ".mov", ".wmv", ".mkv", ".webm", ".flv", ".m4v"},
            "emoji": "ğŸ¬",
        },
        "audio": {
            "extensions": {".mp3", ".wav", ".flac", ".aac", ".ogg", ".wma", ".m4a"},
            "emoji": "ğŸµ",
        },
        "archives": {
            "extensions": {".zip", ".rar", ".7z", ".tar", ".gz", ".bz2", ".xz"},
            "emoji": "ğŸ“¦",
        },
        "code": {
            "extensions": {
                ".py",
                ".js",
                ".ts",
                ".java",
                ".cpp",
                ".c",
                ".h",
                ".css",
                ".html",
                ".json",
                ".yaml",
                ".yml",
            },
            "emoji": "ğŸ’»",
        },
        "executables": {
            "extensions": {".exe", ".msi", ".dmg", ".app", ".deb", ".rpm"},
            "emoji": "âš™ï¸",
        },
    }

    def __init__(
        self,
        gateway: SkillGateway | None = None,
        days_old_threshold: int = 30,
        large_file_mb: int = 100,
    ) -> None:
        """åˆæœŸåŒ–.

        Args:
            gateway: ã‚¹ã‚­ãƒ«ã‚²ãƒ¼ãƒˆã‚¦ã‚§ã‚¤
            days_old_threshold: å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã®é–¾å€¤ï¼ˆæ—¥ï¼‰
            large_file_mb: å¤§ãã„ãƒ•ã‚¡ã‚¤ãƒ«ã®é–¾å€¤ï¼ˆMBï¼‰
        """
        self._gateway = gateway
        self._days_old = days_old_threshold
        self._large_file_bytes = large_file_mb * 1024 * 1024
        self._logger = logging.getLogger(__name__)

    def _get_category(self, filename: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’å–å¾—."""
        ext = Path(filename).suffix.lower()
        for category, config in self.CATEGORIES.items():
            if ext in config["extensions"]:
                return category
        return "others"

    async def analyze_directory(
        self,
        path: str,
        recursive: bool = True,
    ) -> DirectoryAnalysis:
        """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åˆ†æ.

        Args:
            path: åˆ†æå¯¾è±¡ãƒ‘ã‚¹
            recursive: å†å¸°çš„ã«åˆ†æã™ã‚‹ã‹

        Returns:
            åˆ†æçµæœ
        """
        expanded_path = str(Path(path).expanduser())
        analysis = DirectoryAnalysis(path=expanded_path)

        # GatewayçµŒç”±ã§ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—
        if self._gateway:
            try:
                result = await self._gateway.call("list_dir", {"path": expanded_path})
                if not result.success:
                    self._logger.error("ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä¸€è¦§å–å¾—å¤±æ•—: %s", result.error)
                    return analysis

                files = result.result or []
            except Exception as e:
                self._logger.exception("Gatewayå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: %s", e)
                return analysis
        else:
            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‚’ç›´æ¥ä½¿ç”¨ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            try:
                p = Path(expanded_path)
                if not p.exists():
                    return analysis
                files = [
                    {
                        "name": f.name,
                        "size": f.stat().st_size if f.is_file() else 0,
                        "modified": f.stat().st_mtime,
                        "is_dir": f.is_dir(),
                    }
                    for f in p.iterdir()
                ]
            except Exception as e:
                self._logger.exception("ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼: %s", e)
                return analysis

        # åˆ†æ
        import time

        now = time.time()
        threshold_seconds = self._days_old * 24 * 60 * 60

        by_category: dict[str, dict[str, Any]] = defaultdict(lambda: {"count": 0, "size": 0, "files": []})
        by_extension: dict[str, int] = defaultdict(int)

        for file_info in files:
            name = file_info.get("name", "")
            size = file_info.get("size", 0)
            modified = file_info.get("modified", now)
            is_dir = file_info.get("is_dir", False)

            if is_dir:
                analysis.total_dirs += 1
                # ç©ºãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                continue

            analysis.total_files += 1
            analysis.total_size_bytes += size

            # ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
            category = self._get_category(name)
            by_category[category]["count"] += 1
            by_category[category]["size"] += size

            # æ‹¡å¼µå­çµ±è¨ˆ
            ext = Path(name).suffix.lower()
            if ext:
                by_extension[ext] += 1

            # å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«
            if isinstance(modified, (int, float)):
                age_seconds = now - modified
                if age_seconds > threshold_seconds:
                    analysis.old_files.append(
                        {
                            "name": name,
                            "size_mb": round(size / (1024 * 1024), 2),
                            "age_days": int(age_seconds / (24 * 60 * 60)),
                        }
                    )

            # å¤§ãã„ãƒ•ã‚¡ã‚¤ãƒ«
            if size > self._large_file_bytes:
                analysis.large_files.append(
                    {
                        "name": name,
                        "size_mb": round(size / (1024 * 1024), 2),
                    }
                )

        analysis.by_category = {k: dict(v) for k, v in by_category.items()}
        analysis.by_extension = dict(by_extension)

        # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ
        analysis.recommendations = self._generate_recommendations(analysis)

        self._logger.info(
            "ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåˆ†æå®Œäº†: path=%s, files=%d, size=%dMB",
            expanded_path,
            analysis.total_files,
            round(analysis.total_size_bytes / (1024 * 1024)),
        )

        return analysis

    def _generate_recommendations(self, analysis: DirectoryAnalysis) -> list[str]:
        """æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ."""
        recommendations = []

        if len(analysis.old_files) > 10:
            total_old_size = sum(f.get("size_mb", 0) for f in analysis.old_files)
            recommendations.append(
                f"å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«{len(analysis.old_files)}ä»¶ï¼ˆè¨ˆ{total_old_size:.1f}MBï¼‰ã®å‰Šé™¤ã‚’æ¤œè¨ã—ã¦ãã ã•ã„"
            )

        if len(analysis.large_files) > 5:
            recommendations.append(
                f"å¤§ãã„ãƒ•ã‚¡ã‚¤ãƒ«{len(analysis.large_files)}ä»¶ãŒã‚ã‚Šã¾ã™ã€‚å¤–éƒ¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã¸ã®ç§»å‹•ã‚’æ¤œè¨ã—ã¦ãã ã•ã„"
            )

        if analysis.total_files > 100:
            recommendations.append("ãƒ•ã‚¡ã‚¤ãƒ«æ•°ãŒå¤šã„ã§ã™ã€‚ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«ãƒ•ã‚©ãƒ«ãƒ€åˆ†ã‘ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™")

        if len(analysis.empty_dirs) > 0:
            recommendations.append(f"ç©ºã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª{len(analysis.empty_dirs)}ä»¶ã®å‰Šé™¤ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")

        return recommendations

    async def organize(
        self,
        path: str,
        rules: dict[str, Any] | None = None,
        dry_run: bool = True,
    ) -> OrganizationResult:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ•´ç†.

        Args:
            path: æ•´ç†å¯¾è±¡ãƒ‘ã‚¹
            rules: æ•´ç†ãƒ«ãƒ¼ãƒ«ï¼ˆã‚«ãƒ†ã‚´ãƒªåˆ¥ã®ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€åç­‰ï¼‰
            dry_run: True ã®å ´åˆã¯å®Ÿéš›ã®æ“ä½œã‚’è¡Œã‚ãªã„

        Returns:
            æ•´ç†çµæœ
        """
        expanded_path = str(Path(path).expanduser())
        result = OrganizationResult(dry_run=dry_run)

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ«ãƒ¼ãƒ«
        default_rules = {
            "create_category_folders": True,
            "category_names": {
                "documents": "Documents",
                "images": "Images",
                "videos": "Videos",
                "audio": "Audio",
                "archives": "Archives",
                "code": "Code",
                "executables": "Programs",
                "others": "Others",
            },
        }
        rules = {**default_rules, **(rules or {})}

        # åˆ†æ
        analysis = await self.analyze_directory(expanded_path, recursive=False)

        if analysis.total_files == 0:
            return result

        # LLMã§æ•´ç†è¨ˆç”»ã‚’ç”Ÿæˆ
        plan = await self._generate_organization_plan(analysis, rules)

        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ/è¨˜éŒ²
        for action in plan:
            action_type = action.get("type")
            action.get("source")
            target = action.get("target")

            if action_type == "create_dir":
                result.actions.append(action)
                if not dry_run and self._gateway:
                    # GatewayçµŒç”±ã§ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
                    try:
                        await self._gateway.call(
                            "write_file",
                            {
                                "path": f"{target}/.keep",
                                "content": "",
                            },
                        )
                        result.dirs_created += 1
                    except Exception as e:
                        result.errors.append(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆå¤±æ•—: {target} - {e}")
                else:
                    result.dirs_created += 1

            elif action_type == "move":
                result.actions.append(action)
                if not dry_run and self._gateway:
                    # GatewayçµŒç”±ã§ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•ï¼ˆèª­ã¿å–ã‚Šâ†’æ›¸ãè¾¼ã¿â†’å‰Šé™¤ï¼‰
                    # æ³¨: å®Ÿéš›ã®å®Ÿè£…ã§ã¯moveã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨
                    result.errors.append("moveæ“ä½œã¯æœªå®Ÿè£…ï¼ˆdry_runãƒ¢ãƒ¼ãƒ‰ã§ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼‰")
                else:
                    result.files_moved += 1

            elif action_type == "rename":
                result.actions.append(action)
                if not dry_run:
                    result.errors.append("renameæ“ä½œã¯æœªå®Ÿè£…ï¼ˆdry_runãƒ¢ãƒ¼ãƒ‰ã§ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼‰")
                else:
                    result.files_renamed += 1

        self._logger.info(
            "ãƒ•ã‚¡ã‚¤ãƒ«æ•´ç†å®Œäº†: path=%s, moved=%d, renamed=%d, dry_run=%s",
            expanded_path,
            result.files_moved,
            result.files_renamed,
            dry_run,
        )

        return result

    async def _generate_organization_plan(
        self,
        analysis: DirectoryAnalysis,
        rules: dict[str, Any],
    ) -> list[dict[str, Any]]:
        """æ•´ç†è¨ˆç”»ã‚’ç”Ÿæˆ."""
        actions: list[dict[str, Any]] = []
        category_names = rules.get("category_names", {})

        # ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚©ãƒ«ãƒ€ã®ä½œæˆ
        if rules.get("create_category_folders"):
            for category in analysis.by_category:
                folder_name = category_names.get(category, category.capitalize())
                target_path = str(Path(analysis.path) / folder_name)
                actions.append(
                    {
                        "type": "create_dir",
                        "target": target_path,
                    }
                )

        # TODO: å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•è¨ˆç”»ï¼ˆGatewayçµŒç”±ã§ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å†å–å¾—ã—ã¦è¨ˆç”»ï¼‰

        return actions

    async def smart_rename(
        self,
        path: str,
        pattern: str | None = None,
    ) -> OrganizationResult:
        """ã‚¹ãƒãƒ¼ãƒˆãƒªãƒãƒ¼ãƒ .

        Args:
            path: å¯¾è±¡ãƒ‘ã‚¹
            pattern: ãƒªãƒãƒ¼ãƒ ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆNoneã®å ´åˆã¯LLMã§ç”Ÿæˆï¼‰

        Returns:
            ãƒªãƒãƒ¼ãƒ çµæœ
        """
        return OrganizationResult(dry_run=True)

        # TODO: LLMã‚’ä½¿ç”¨ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«åã‚’åˆ†æã—ã€çµ±ä¸€çš„ãªãƒªãƒãƒ¼ãƒ ã‚’ææ¡ˆ

    async def find_duplicates(
        self,
        path: str,
        by_content: bool = True,
    ) -> list[DuplicateGroup]:
        """é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡º.

        Args:
            path: æ¤œç´¢ãƒ‘ã‚¹
            by_content: å†…å®¹ã§æ¯”è¼ƒï¼ˆFalseã®å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«åã¨ã‚µã‚¤ã‚ºï¼‰

        Returns:
            é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—ãƒªã‚¹ãƒˆ
        """
        expanded_path = str(Path(path).expanduser())
        duplicates: list[DuplicateGroup] = []

        # GatewayçµŒç”±ã§ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—
        if self._gateway:
            result = await self._gateway.call("list_dir", {"path": expanded_path})
            if not result.success:
                return duplicates
            files = result.result or []
        else:
            return duplicates

        # ã‚µã‚¤ã‚ºã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        size_groups: dict[int, list[dict[str, Any]]] = defaultdict(list)
        for f in files:
            if not f.get("is_dir", False):
                size_groups[f.get("size", 0)].append(f)

        # åŒã‚µã‚¤ã‚ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’è©³ç´°æ¯”è¼ƒ
        for size, file_list in size_groups.items():
            if len(file_list) < 2:
                continue

            if by_content:
                # å†…å®¹ãƒãƒƒã‚·ãƒ¥ã§æ¯”è¼ƒï¼ˆç°¡æ˜“ç‰ˆï¼šãƒ•ã‚¡ã‚¤ãƒ«å+ã‚µã‚¤ã‚ºï¼‰
                # å®Ÿéš›ã«ã¯ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—
                hash_groups: dict[str, list[str]] = defaultdict(list)
                for f in file_list:
                    # ç°¡æ˜“ãƒãƒƒã‚·ãƒ¥ï¼ˆå®Ÿéš›ã«ã¯ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’ãƒãƒƒã‚·ãƒ¥ï¼‰
                    simple_hash = hashlib.md5(f"{f.get('name', '')}_{size}".encode()).hexdigest()
                    hash_groups[simple_hash].append(str(Path(expanded_path) / f.get("name", "")))

                for file_hash, paths in hash_groups.items():
                    if len(paths) > 1:
                        duplicates.append(
                            DuplicateGroup(
                                hash=file_hash,
                                size=size,
                                files=paths,
                            )
                        )
            else:
                # åå‰ã¨ã‚µã‚¤ã‚ºã§æ¯”è¼ƒ
                duplicates.append(
                    DuplicateGroup(
                        hash="size_match",
                        size=size,
                        files=[str(Path(expanded_path) / f.get("name", "")) for f in file_list],
                    )
                )

        self._logger.info("é‡è¤‡æ¤œå‡ºå®Œäº†: path=%s, groups=%d", expanded_path, len(duplicates))

        return duplicates

    async def cleanup_old_files(
        self,
        path: str,
        days_old: int = 30,
        dry_run: bool = True,
    ) -> OrganizationResult:
        """å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—.

        Args:
            path: å¯¾è±¡ãƒ‘ã‚¹
            days_old: ä½•æ—¥ä»¥ä¸Šå¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¯¾è±¡ã«ã™ã‚‹ã‹
            dry_run: True ã®å ´åˆã¯å®Ÿéš›ã®å‰Šé™¤ã‚’è¡Œã‚ãªã„

        Returns:
            ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—çµæœ
        """
        result = OrganizationResult(dry_run=dry_run)

        # åˆ†æã§å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç‰¹å®š
        old_threshold = self._days_old
        self._days_old = days_old
        analysis = await self.analyze_directory(path)
        self._days_old = old_threshold

        for old_file in analysis.old_files:
            action = {
                "type": "delete",
                "target": str(Path(path).expanduser() / old_file["name"]),
                "reason": f"{old_file.get('age_days', 0)}æ—¥ä»¥ä¸Šå¤ã„",
            }
            result.actions.append(action)

            if not dry_run:
                result.errors.append("deleteæ“ä½œã¯æœªå®Ÿè£…ï¼ˆdry_runãƒ¢ãƒ¼ãƒ‰ã§ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼‰")

        self._logger.info(
            "ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†: path=%s, old_files=%d, dry_run=%s",
            path,
            len(analysis.old_files),
            dry_run,
        )

        return result
