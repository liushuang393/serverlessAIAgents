# ãƒ‡ãƒ—ãƒ­ã‚¤è¦ç´„

> **ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0.0
> **é©ç”¨ç¯„å›²**: AgentFlow Framework å…¨ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆãƒ—ãƒ­ã‚»ã‚¹
> **æœ€çµ‚æ›´æ–°**: 2025-01-19

## ğŸ“‹ ç›®æ¬¡

1. [ãƒ‡ãƒ—ãƒ­ã‚¤åŸå‰‡](#ãƒ‡ãƒ—ãƒ­ã‚¤åŸå‰‡)
2. [ç’°å¢ƒç®¡ç†](#ç’°å¢ƒç®¡ç†)
3. [ã‚³ãƒ³ãƒ†ãƒŠåŒ–](#ã‚³ãƒ³ãƒ†ãƒŠåŒ–)
4. [CI/CD ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³](#cicd-ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³)
5. [ãƒªãƒªãƒ¼ã‚¹ç®¡ç†](#ãƒªãƒªãƒ¼ã‚¹ç®¡ç†)
6. [ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥](#ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥)
7. [ç›£è¦–ã¨ãƒ­ã‚°](#ç›£è¦–ã¨ãƒ­ã‚°)

---

## ğŸ¯ ãƒ‡ãƒ—ãƒ­ã‚¤åŸå‰‡

### ç¶™ç¶šçš„ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ
```yaml
# âœ… æ­£ã—ã„: è‡ªå‹•åŒ–ã•ã‚ŒãŸãƒ‡ãƒ—ãƒ­ã‚¤ãƒ—ãƒ­ã‚»ã‚¹
# .github/workflows/deploy.yml
name: Deploy to Production
on:
  push:
    branches: [main]
  workflow_dispatch:

jobs:
  deploy:
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    environment: production

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Deploy to ECS
        run: |
          # è‡ªå‹•ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ
          ./scripts/deploy.sh production
```

### ã‚¤ãƒŸãƒ¥ãƒ¼ã‚¿ãƒ–ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ
```dockerfile
# âœ… æ­£ã—ã„: ã‚¤ãƒŸãƒ¥ãƒ¼ã‚¿ãƒ–ãƒ«ãªã‚³ãƒ³ãƒ†ãƒŠã‚¤ãƒ¡ãƒ¼ã‚¸
FROM python:3.13-slim

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ›´æ–°
RUN apt-get update && apt-get upgrade -y && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
WORKDIR /app

# ä¾å­˜é–¢ä¿‚ã®ã‚³ãƒ”ãƒ¼ã¨ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚³ãƒ¼ãƒ‰ã®ã‚³ãƒ”ãƒ¼
COPY agentflow/ ./agentflow/
COPY scripts/ ./scripts/

# érootãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ã®å®Ÿè¡Œ
RUN useradd --create-home --shell /bin/bash app
RUN chown -R app:app /app
USER app

# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:8000/health || exit 1

# èµ·å‹•ã‚³ãƒãƒ³ãƒ‰
CMD ["python", "-m", "agentflow.api"]
```

### ãƒ–ãƒ«ãƒ¼ã‚°ãƒªãƒ¼ãƒ³ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ
```bash
#!/bin/bash
# âœ… æ­£ã—ã„: ãƒ–ãƒ«ãƒ¼ã‚°ãƒªãƒ¼ãƒ³ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

set -e

ENVIRONMENT=$1
BLUE_SERVICE="agentflow-blue"
GREEN_SERVICE="agentflow-green"

# ç¾åœ¨ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒ¼ãƒ“ã‚¹ã‚’ç¢ºèª
ACTIVE_SERVICE=$(aws ecs describe-services \
    --cluster agentflow-cluster \
    --services $BLUE_SERVICE $GREEN_SERVICE \
    --query 'services[?runningCount>0].serviceName' \
    --output text)

if [ "$ACTIVE_SERVICE" = "$BLUE_SERVICE" ]; then
    TARGET_SERVICE=$GREEN_SERVICE
    STANDBY_SERVICE=$BLUE_SERVICE
else
    TARGET_SERVICE=$BLUE_SERVICE
    STANDBY_SERVICE=$GREEN_SERVICE
fi

echo "Deploying to $TARGET_SERVICE..."

# æ–°ã—ã„ã‚µãƒ¼ãƒ“ã‚¹ã‚’èµ·å‹•
aws ecs update-service \
    --cluster agentflow-cluster \
    --service $TARGET_SERVICE \
    --task-definition agentflow-$ENVIRONMENT \
    --desired-count 3

# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚’å¾…æ©Ÿ
echo "Waiting for health checks..."
aws ecs wait services-stable \
    --cluster agentflow-cluster \
    --services $TARGET_SERVICE

# ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚’åˆ‡ã‚Šæ›¿ãˆ
echo "Switching traffic..."
# Load Balancer ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚°ãƒ«ãƒ¼ãƒ—ã‚’æ›´æ–°
aws elbv2 modify-listener \
    --listener-arn $LISTENER_ARN \
    --default-actions Type=forward,TargetGroupArn=$TARGET_TG_ARN

# å¤ã„ã‚µãƒ¼ãƒ“ã‚¹ã‚’åœæ­¢
echo "Stopping old service..."
aws ecs update-service \
    --cluster agentflow-cluster \
    --service $STANDBY_SERVICE \
    --desired-count 0

echo "Deployment completed successfully!"
```

---

## ğŸŒ ç’°å¢ƒç®¡ç†

### ç’°å¢ƒè¨­å®šç®¡ç†
```python
# âœ… æ­£ã—ã„: ç’°å¢ƒåˆ¥è¨­å®šç®¡ç†
import os
from typing import Dict, Any
from pydantic import BaseSettings, Field

class BaseConfig(BaseSettings):
    """ãƒ™ãƒ¼ã‚¹è¨­å®šã‚¯ãƒ©ã‚¹ã€‚"""

    # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
    app_name: str = Field(default="AgentFlow")
    debug: bool = Field(default=False)
    log_level: str = Field(default="INFO")

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š
    database_url: str = Field(...)

    # å¤–éƒ¨ã‚µãƒ¼ãƒ“ã‚¹è¨­å®š
    redis_url: str = Field(default="redis://localhost:6379")
    openai_api_key: str = Field(...)

    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"

class DevelopmentConfig(BaseConfig):
    """é–‹ç™ºç’°å¢ƒè¨­å®šã€‚"""

    debug: bool = True
    log_level: str = "DEBUG"

    # é–‹ç™ºç’°å¢ƒã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    database_url: str = Field(default="postgresql://localhost/agentflow_dev")

class StagingConfig(BaseConfig):
    """ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ç’°å¢ƒè¨­å®šã€‚"""

    log_level: str = "WARNING"

    # ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ç’°å¢ƒã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    database_url: str = Field(default="postgresql://staging-db/agentflow_staging")

class ProductionConfig(BaseConfig):
    """æœ¬ç•ªç’°å¢ƒè¨­å®šã€‚"""

    debug: bool = False
    log_level: str = "ERROR"

    # æœ¬ç•ªç’°å¢ƒã§ã¯å¿…é ˆè¨­å®š
    database_url: str = Field(...)

def get_config() -> BaseConfig:
    """ç’°å¢ƒã«å¿œã˜ãŸè¨­å®šã‚’å–å¾—ã€‚"""
    env = os.getenv("ENVIRONMENT", "development").lower()

    config_map = {
        "development": DevelopmentConfig,
        "staging": StagingConfig,
        "production": ProductionConfig,
    }

    config_class = config_map.get(env, DevelopmentConfig)
    return config_class()

# ä½¿ç”¨ä¾‹
config = get_config()
print(f"Running in {config.app_name} mode")
```

### ç’°å¢ƒå¤‰æ•°ã®ç®¡ç†
```bash
# âœ… æ­£ã—ã„: .env ãƒ•ã‚¡ã‚¤ãƒ«ã®ç®¡ç†ï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†å¯¾è±¡å¤–ï¼‰
# .env.exampleï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼‰
APP_NAME=AgentFlow
DEBUG=false
LOG_LEVEL=INFO
DATABASE_URL=postgresql://user:password@localhost/dbname
REDIS_URL=redis://localhost:6379
OPENAI_API_KEY=your-api-key-here
JWT_SECRET=your-jwt-secret-here

# .env.localï¼ˆãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç”¨ã€ä¸Šæ›¸ãï¼‰
DEBUG=true
LOG_LEVEL=DEBUG
DATABASE_URL=postgresql://localhost/agentflow_dev

# æœ¬ç•ªç’°å¢ƒã§ã¯ç’°å¢ƒå¤‰æ•°ã‚’ç›´æ¥è¨­å®š
# AWS ECS ã‚¿ã‚¹ã‚¯å®šç¾©ä¾‹
{
    "family": "agentflow-production",
    "containerDefinitions": [
        {
            "name": "agentflow",
            "image": "agentflow:latest",
            "environment": [
                {"name": "ENVIRONMENT", "value": "production"},
                {"name": "DATABASE_URL", "valueFrom": "arn:aws:secretsmanager:..."},
                {"name": "OPENAI_API_KEY", "valueFrom": "arn:aws:secretsmanager:..."}
            ],
            "secrets": [
                {
                    "name": "JWT_SECRET",
                    "valueFrom": "arn:aws:secretsmanager:region:account:secret:name"
                }
            ]
        }
    ]
}
```

### ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆç®¡ç†
```python
# âœ… æ­£ã—ã„: AWS Secrets Manager çµ±åˆ
import boto3
from typing import Dict, Any
import json

class SecretsManager:
    """AWS Secrets Manager çµ±åˆã‚¯ãƒ©ã‚¹ã€‚"""

    def __init__(self, region_name: str = "us-east-1"):
        self.client = boto3.client("secretsmanager", region_name=region_name)

    async def get_secret(self, secret_name: str) -> Dict[str, Any]:
        """ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã‚’å–å¾—ã€‚"""
        try:
            response = await self.client.get_secret_value(SecretId=secret_name)
            secret_string = response["SecretString"]
            return json.loads(secret_string)
        except Exception as e:
            logger.error(f"Failed to retrieve secret {secret_name}: {e}")
            raise

    async def get_database_credentials(self) -> Dict[str, str]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èªè¨¼æƒ…å ±ã‚’å–å¾—ã€‚"""
        secrets = await self.get_secret("agentflow/database")
        return {
            "username": secrets["username"],
            "password": secrets["password"],
            "host": secrets["host"],
            "port": secrets["port"],
            "database": secrets["database"],
        }

class ConfigWithSecrets:
    """ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã‚’å«ã‚€è¨­å®šç®¡ç†ã€‚"""

    def __init__(self):
        self.secrets_manager = SecretsManager()

    async def load_config(self) -> Dict[str, Any]:
        """è¨­å®šã¨ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã‚’çµ±åˆã—ã¦èª­ã¿è¾¼ã¿ã€‚"""
        # åŸºæœ¬è¨­å®šã®èª­ã¿è¾¼ã¿
        config = get_config()

        # ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã®èª­ã¿è¾¼ã¿
        if config.environment == "production":
            db_creds = await self.secrets_manager.get_database_credentials()
            api_keys = await self.secrets_manager.get_secret("agentflow/api-keys")

            # è¨­å®šã«çµ±åˆ
            config.database_url = (
                f"postgresql://{db_creds['username']}:{db_creds['password']}"
                f"@{db_creds['host']}:{db_creds['port']}/{db_creds['database']}"
            )
            config.openai_api_key = api_keys["openai_api_key"]
            config.jwt_secret = api_keys["jwt_secret"]

        return config
```

---

## ğŸ³ ã‚³ãƒ³ãƒ†ãƒŠåŒ–

### Dockerfile ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
```dockerfile
# âœ… æ­£ã—ã„: ãƒãƒ«ãƒã‚¹ãƒ†ãƒ¼ã‚¸ãƒ“ãƒ«ãƒ‰
# Build stage
FROM python:3.13-slim as builder

WORKDIR /app

# ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
COPY requirements.txt .
RUN pip install --user --no-cache-dir -r requirements.txt

# Production stage
FROM python:3.13-slim as production

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ›´æ–°ã¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
RUN apt-get update && \
    apt-get upgrade -y && \
    apt-get install -y curl && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# érootãƒ¦ãƒ¼ã‚¶ãƒ¼ä½œæˆ
RUN groupadd -r appuser && useradd -r -g appuser appuser

WORKDIR /app

# ãƒ“ãƒ«ãƒ‰æ¸ˆã¿ä¾å­˜é–¢ä¿‚ã®ã‚³ãƒ”ãƒ¼
COPY --from=builder /root/.local /home/appuser/.local
ENV PATH=/home/appuser/.local/bin:$PATH

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚³ãƒ¼ãƒ‰ã®ã‚³ãƒ”ãƒ¼
COPY --chown=appuser:appuser agentflow/ ./agentflow/

# æ¨©é™è¨­å®š
RUN chown -R appuser:appuser /app
USER appuser

# ãƒãƒ¼ãƒˆå…¬é–‹
EXPOSE 8000

# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
HEALTHCHECK --interval=30s --timeout=10s --start-period=30s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# èµ·å‹•ã‚³ãƒãƒ³ãƒ‰
CMD ["python", "-m", "uvicorn", "agentflow.api:app", "--host", "0.0.0.0", "--port", "8000"]
```

### Docker Compose ã®è¨­å®š
```yaml
# âœ… æ­£ã—ã„: ãƒãƒ«ãƒã‚µãƒ¼ãƒ“ã‚¹æ§‹æˆ
version: '3.8'

services:
  api:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    ports:
      - "8000:8000"
    environment:
      - ENVIRONMENT=development
      - DATABASE_URL=postgresql://user:pass@db:5432/agentflow
      - REDIS_URL=redis://redis:6379
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    volumes:
      - ./logs:/app/logs
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  db:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=agentflow
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=pass
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U user -d agentflow"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  worker:
    build:
      context: .
      dockerfile: Dockerfile.worker
    environment:
      - ENVIRONMENT=development
      - DATABASE_URL=postgresql://user:pass@db:5432/agentflow
      - REDIS_URL=redis://redis:6379
    depends_on:
      - db
      - redis
    restart: unless-stopped

volumes:
  postgres_data:

networks:
  default:
    driver: bridge
```

---

## ğŸ”„ CI/CD ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

### GitHub Actions ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
```yaml
# .github/workflows/ci-cd.yml
name: CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.13'

      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt -r requirements-dev.txt

      - name: Run tests
        run: |
          pytest --cov=agentflow --cov-report=xml

      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml

  build-and-push:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    permissions:
      contents: read
      packages: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Log in to Container Registry
        uses: docker/login-action@v2
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v4
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push Docker image
        uses: docker/build-push-action@v4
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  deploy-staging:
    needs: build-and-push
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop'
    environment: staging

    steps:
      - name: Deploy to staging
        run: |
          echo "Deploying to staging environment"
          # ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ

  deploy-production:
    needs: build-and-push
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    environment: production

    steps:
      - name: Deploy to production
        run: |
          echo "Deploying to production environment"
          # æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ
```

### ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
```bash
#!/bin/bash
# scripts/deploy.sh

set -e

ENVIRONMENT=$1
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="/backups/$TIMESTAMP"

echo "ğŸš€ Deploying AgentFlow to $ENVIRONMENT environment..."

# äº‹å‰ãƒã‚§ãƒƒã‚¯
echo "ğŸ“‹ Running pre-deployment checks..."
./scripts/health-check.sh

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
echo "ğŸ’¾ Creating database backup..."
mkdir -p $BACKUP_DIR
pg_dump agentflow_db > $BACKUP_DIR/pre_deploy.sql

# ãƒ–ãƒ«ãƒ¼ã‚°ãƒªãƒ¼ãƒ³ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ
echo "ğŸ”„ Performing blue-green deployment..."
if [ "$ENVIRONMENT" = "production" ]; then
    ./scripts/blue-green-deploy.sh production
else
    ./scripts/rolling-deploy.sh $ENVIRONMENT
fi

# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
echo "ğŸ¥ Running post-deployment health checks..."
timeout 300 ./scripts/wait-for-health.sh || {
    echo "âŒ Health checks failed, initiating rollback..."
    ./scripts/rollback.sh $BACKUP_DIR
    exit 1
}

# ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
echo "ğŸ§¹ Cleaning up old deployments..."
./scripts/cleanup.sh

echo "âœ… Deployment completed successfully!"
```

---

## ğŸ“¦ ãƒªãƒªãƒ¼ã‚¹ç®¡ç†

### ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°
```python
# âœ… æ­£ã—ã„: ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
# agentflow/__init__.py
__version__ = "1.2.3"  # MAJOR.MINOR.PATCH

# ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ›´æ–°ãƒ«ãƒ¼ãƒ«:
# - MAJOR: ç ´å£Šçš„å¤‰æ›´ï¼ˆAPIã®éäº’æ›ï¼‰
# - MINOR: æ–°æ©Ÿèƒ½è¿½åŠ ï¼ˆå¾Œæ–¹äº’æ›ï¼‰
# - PATCH: ãƒã‚°ä¿®æ­£ï¼ˆå¾Œæ–¹äº’æ›ï¼‰

# ãƒªãƒªãƒ¼ã‚¹ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾‹
def update_version(version_type: str):
    """ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’æ›´æ–°ã™ã‚‹ã€‚"""
    current = parse_version(__version__)

    if version_type == "major":
        new_version = f"{current.major + 1}.0.0"
    elif version_type == "minor":
        new_version = f"{current.major}.{current.minor + 1}.0"
    else:  # patch
        new_version = f"{current.major}.{current.minor}.{current.patch + 1}"

    # __init__.py ã‚’æ›´æ–°
    update_file("__init__.py", __version__, new_version)

    # å¤‰æ›´å±¥æ­´æ›´æ–°
    update_changelog(new_version)

    # Git ã‚¿ã‚°ä½œæˆ
    run_command(f"git tag v{new_version}")
    run_command(f"git push origin v{new_version}")
```

### ãƒªãƒªãƒ¼ã‚¹ãƒãƒ¼ãƒˆç”Ÿæˆ
```python
# âœ… æ­£ã—ã„: è‡ªå‹•ãƒªãƒªãƒ¼ã‚¹ãƒãƒ¼ãƒˆç”Ÿæˆ
# scripts/generate-release-notes.py
import re
from pathlib import Path

def generate_release_notes(version: str) -> str:
    """ãƒªãƒªãƒ¼ã‚¹ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã€‚"""
    changelog_path = Path("CHANGELOG.md")
    content = changelog_path.read_text()

    # æŒ‡å®šãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®å¤‰æ›´å±¥æ­´ã‚’æŠ½å‡º
    pattern = rf"## \[{version}\].*?(?=## \[[0-9]+\.[0-9]+\.[0-9]+\]|$)"
    match = re.search(pattern, content, re.DOTALL)

    if not match:
        raise ValueError(f"Version {version} not found in changelog")

    notes = match.group(0).strip()

    # GitHub ãƒªãƒªãƒ¼ã‚¹å½¢å¼ã«å¤‰æ›
    return f"""# Release {version}

{notes}

## Installation

```bash
pip install agentflow=={version}
```

## Docker

```bash
docker pull ghcr.io/liushuang393/serverlessaiaagents:{version}
```
"""

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    notes = generate_release_notes("1.2.3")
    print(notes)
```

### ãƒªãƒªãƒ¼ã‚¹ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ
```markdown
# ãƒªãƒªãƒ¼ã‚¹ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ v1.2.3

## äº‹å‰ç¢ºèª
- [ ] ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒé€šé
- [ ] ã‚«ãƒãƒ¬ãƒƒã‚¸ 80% ä»¥ä¸Š
- [ ] ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚­ãƒ£ãƒ³é€šé
- [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆé€šé
- [ ] CHANGELOG.md æ›´æ–°æ¸ˆã¿
- [ ] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°æ¸ˆã¿
- [ ] ç§»è¡Œã‚¬ã‚¤ãƒ‰ä½œæˆæ¸ˆã¿ï¼ˆç ´å£Šçš„å¤‰æ›´ã®å ´åˆï¼‰

## ãƒªãƒªãƒ¼ã‚¹æº–å‚™
- [ ] ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·æ›´æ–°ï¼ˆagentflow/__init__.pyï¼‰
- [ ] Git ã‚¿ã‚°ä½œæˆï¼ˆv1.2.3ï¼‰
- [ ] Docker ã‚¤ãƒ¡ãƒ¼ã‚¸ãƒ“ãƒ«ãƒ‰
- [ ] PyPI ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ“ãƒ«ãƒ‰

## ãƒªãƒªãƒ¼ã‚¹å®Ÿè¡Œ
- [ ] GitHub Release ä½œæˆ
- [ ] PyPI ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å…¬é–‹
- [ ] Docker ã‚¤ãƒ¡ãƒ¼ã‚¸å…¬é–‹
- [ ] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚µã‚¤ãƒˆæ›´æ–°

## ãƒªãƒªãƒ¼ã‚¹å¾Œ
- [ ] ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤
- [ ] æœ¬ç•ªç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤ï¼ˆæ®µéšçš„ï¼‰
- [ ] ç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç¢ºèª
- [ ] ãƒ¦ãƒ¼ã‚¶ãƒ¼é€šçŸ¥ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰
- [ ] å•é¡Œç™ºç”Ÿæ™‚ã®ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æº–å‚™
```

---

## ğŸ”„ ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥

### è‡ªå‹•ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
```bash
#!/bin/bash
# scripts/rollback.sh

set -e

BACKUP_TIMESTAMP=$1
ENVIRONMENT=${2:-production}

echo "ğŸ”„ Rolling back to $BACKUP_TIMESTAMP in $ENVIRONMENT..."

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¾©å…ƒ
echo "ğŸ’¾ Restoring database..."
psql agentflow_db < /backups/$BACKUP_TIMESTAMP/pre_deploy.sql

# ä»¥å‰ã®ã‚³ãƒ³ãƒ†ãƒŠã‚¤ãƒ¡ãƒ¼ã‚¸ã«åˆ‡ã‚Šæˆ»ã—
echo "ğŸ³ Rolling back container image..."
PREVIOUS_IMAGE=$(get_previous_image_tag)
kubectl set image deployment/agentflow agentflow=$PREVIOUS_IMAGE

# è¨­å®šå¾©å…ƒ
echo "âš™ï¸ Restoring configuration..."
cp /backups/$BACKUP_TIMESTAMP/config.yaml /etc/agentflow/config.yaml

# ã‚µãƒ¼ãƒ“ã‚¹å†èµ·å‹•
echo "ğŸ”„ Restarting services..."
kubectl rollout restart deployment/agentflow

# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
echo "ğŸ¥ Waiting for health checks..."
timeout 300 ./scripts/wait-for-health.sh

echo "âœ… Rollback completed successfully!"
```

### æ®µéšçš„ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
```python
# âœ… æ­£ã—ã„: ã‚«ãƒŠãƒªã‚¢ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
class RollingRollback:
    """æ®µéšçš„ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¯ãƒ©ã‚¹ã€‚"""

    def __init__(self, k8s_client, deployment_name: str):
        self.k8s = k8s_client
        self.deployment_name = deployment_name

    async def gradual_rollback(self, steps: int = 5, delay: int = 60):
        """æ®µéšçš„ã«ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å®Ÿè¡Œã€‚"""
        # ç¾åœ¨ã®ãƒ¬ãƒ—ãƒªã‚«æ•°ã‚’å–å¾—
        current_replicas = await self._get_current_replicas()

        # æ®µéšçš„ã«å¤ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«æˆ»ã™
        old_image = await self._get_previous_image()

        for i in range(steps):
            # æ–°ã—ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨ã®æ¯”ç‡ã‚’è¨ˆç®—
            new_ratio = (steps - i - 1) / steps
            old_ratio = (i + 1) / steps

            # ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚’èª¿æ•´
            await self._update_traffic_split(new_ratio, old_ratio)

            # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
            if not await self._run_health_checks():
                # å•é¡ŒãŒç™ºç”Ÿã—ãŸã‚‰åœæ­¢
                await self._emergency_rollback()
                raise RollbackFailedError("Health checks failed during rollback")

            # å¾…æ©Ÿ
            await asyncio.sleep(delay)

        # å®Œå…¨ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
        await self._complete_rollback(old_image)

    async def _update_traffic_split(self, new_ratio: float, old_ratio: float):
        """ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯åˆ†å‰²ã‚’æ›´æ–°ã€‚"""
        # Istio ã¾ãŸã¯ã‚µãƒ¼ãƒ“ã‚¹ãƒ¡ãƒƒã‚·ãƒ¥çµŒç”±ã§ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯åˆ¶å¾¡
        pass

    async def _run_health_checks(self) -> bool:
        """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œã€‚"""
        # ã‚¨ãƒ©ãƒ¼ç‡ã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ ã€æˆåŠŸç‡ã‚’ãƒã‚§ãƒƒã‚¯
        pass
```

---

## ğŸ“Š ç›£è¦–ã¨ãƒ­ã‚°

### ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹
```python
# âœ… æ­£ã—ã„: Prometheus ãƒ¡ãƒˆãƒªã‚¯ã‚¹çµ±åˆ
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from fastapi import FastAPI, Response
from fastapi.middleware.base import BaseHTTPMiddleware

app = FastAPI()

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©
REQUEST_COUNT = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status_code']
)

REQUEST_LATENCY = Histogram(
    'http_request_duration_seconds',
    'HTTP request latency',
    ['method', 'endpoint']
)

ACTIVE_CONNECTIONS = Gauge(
    'active_connections',
    'Number of active connections'
)

class MetricsMiddleware(BaseHTTPMiddleware):
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢ã€‚"""

    async def dispatch(self, request, call_next):
        method = request.method
        endpoint = request.url.path

        with REQUEST_LATENCY.labels(method=method, endpoint=endpoint).time():
            ACTIVE_CONNECTIONS.inc()
            try:
                response = await call_next(request)
                REQUEST_COUNT.labels(
                    method=method,
                    endpoint=endpoint,
                    status_code=response.status_code
                ).inc()
                return response
            finally:
                ACTIVE_CONNECTIONS.dec()

@app.get("/metrics")
async def metrics():
    """Prometheus ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã€‚"""
    return Response(
        generate_latest(),
        media_type="text/plain; charset=utf-8"
    )
```

### æ§‹é€ åŒ–ãƒ­ã‚°
```python
# âœ… æ­£ã—ã„: JSON å½¢å¼ã®æ§‹é€ åŒ–ãƒ­ã‚°
import logging
import json
from typing import Dict, Any
from datetime import datetime

class StructuredLogger:
    """æ§‹é€ åŒ–ãƒ­ã‚¬ãƒ¼ã€‚"""

    def __init__(self, name: str):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(logging.INFO)

        # JSON ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿
        formatter = logging.Formatter(
            '{"timestamp": "%(asctime)s", "level": "%(levelname)s", '
            '"logger": "%(name)s", "message": "%(message)s"}'
        )

        # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¨­å®š
        handler = logging.StreamHandler()
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)

    def log_request(self, request_id: str, method: str, url: str, status: int, duration: float):
        """HTTP ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ­ã‚°ã€‚"""
        self.logger.info(
            "Request processed",
            extra={
                "request_id": request_id,
                "method": method,
                "url": url,
                "status": status,
                "duration": duration,
                "event_type": "http_request"
            }
        )

    def log_error(self, error: Exception, context: Dict[str, Any]):
        """ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã€‚"""
        self.logger.error(
            f"Error occurred: {error}",
            extra={
                "error_type": type(error).__name__,
                "error_message": str(error),
                "context": context,
                "event_type": "error"
            }
        )

    def log_workflow_execution(self, workflow_id: str, status: str, metrics: Dict[str, Any]):
        """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œãƒ­ã‚°ã€‚"""
        self.logger.info(
            f"Workflow {workflow_id} {status}",
            extra={
                "workflow_id": workflow_id,
                "status": status,
                "metrics": metrics,
                "event_type": "workflow_execution"
            }
        )

# ä½¿ç”¨ä¾‹
logger = StructuredLogger("agentflow.api")

# ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ­ã‚°
logger.log_request(
    request_id="req-123",
    method="POST",
    url="/api/workflows",
    status=200,
    duration=0.145
)

# ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°
try:
    # ä½•ã‚‰ã‹ã®å‡¦ç†
    pass
except Exception as e:
    logger.log_error(e, {"user_id": "user123", "action": "create_workflow"})
```

### ãƒ­ã‚°é›†ç´„ã¨åˆ†æ
```yaml
# âœ… æ­£ã—ã„: ELK Stack è¨­å®šä¾‹
# docker-compose.logging.yml
version: '3.8'

services:
  elasticsearch:
    image: elasticsearch:8.5.0
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data

  logstash:
    image: logstash:8.5.0
    ports:
      - "5044:5044"
    volumes:
      - ./config/logstash.conf:/usr/share/logstash/pipeline/logstash.conf
    depends_on:
      - elasticsearch

  kibana:
    image: kibana:8.5.0
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch

volumes:
  elasticsearch_data:
```

---

## âœ… ãƒ‡ãƒ—ãƒ­ã‚¤ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### ãƒ‡ãƒ—ãƒ­ã‚¤å‰ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ
- [ ] ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒé€šé
- [ ] ã‚«ãƒãƒ¬ãƒƒã‚¸ 80% ä»¥ä¸Š
- [ ] ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚­ãƒ£ãƒ³é€šé
- [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆé€šé
- [ ] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°æ¸ˆã¿
- [ ] ç§»è¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆæ¸ˆã¿
- [ ] ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ—ãƒ©ãƒ³ç­–å®šæ¸ˆã¿

### ãƒ‡ãƒ—ãƒ­ã‚¤ä¸­ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ
- [ ] ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å–å¾—
- [ ] ãƒ–ãƒ«ãƒ¼ã‚°ãƒªãƒ¼ãƒ³ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå®Ÿè¡Œ
- [ ] ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯é€šé
- [ ] ç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç¢ºèª
- [ ] ãƒ­ã‚°é›†ç´„æ­£å¸¸å‹•ä½œ

### ãƒ‡ãƒ—ãƒ­ã‚¤å¾Œãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ
- [ ] æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
- [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
- [ ] ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒ¼ãƒˆç¢ºèª
- [ ] ãƒ¦ãƒ¼ã‚¶ãƒ¼å½±éŸ¿è©•ä¾¡
- [ ] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°

---

## ğŸ“š é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- [**CI/CD ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**](ci-cd-pipeline.md) - ç¶™ç¶šçš„ã‚¤ãƒ³ãƒ†ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®è©³ç´°
- [**ç›£è¦–ã‚¢ãƒ©ãƒ¼ãƒˆ**](monitoring-alerting.md) - é‹ç”¨ç›£è¦–ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
- [**ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¦ç´„**](security-standards.md) - å®‰å…¨ãªãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ
- [**ãƒªãƒªãƒ¼ã‚¹ç®¡ç†**](release-management.md) - ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã¨ãƒªãƒªãƒ¼ã‚¹ãƒ—ãƒ­ã‚»ã‚¹

---

**ä¿¡é ¼æ€§ãŒé«˜ãã€å®‰å…¨ãªãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆãŒæˆåŠŸã®éµã§ã™ã€‚** ğŸš€

*æœ€çµ‚æ›´æ–°: 2025-01-19 | ãƒãƒ¼ã‚¸ãƒ§ãƒ³: 1.0.0*