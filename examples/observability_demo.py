"""
é«˜åº¦ãªè¦³æ¸¬å¯èƒ½æ€§æ©Ÿèƒ½ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

ã“ã®ã‚µãƒ³ãƒ—ãƒ«ã¯ã€åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–æ©Ÿèƒ½ã‚’ç¤ºã—ã¾ã™ã€‚
OpenTelemetryæº–æ‹ ã®ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã€Prometheuså½¢å¼ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€æ§‹é€ åŒ–ãƒ­ã‚°ã‚’å«ã¿ã¾ã™ã€‚
"""

import asyncio
import random
import time
from typing import Any, Dict, List, Tuple

from ai_blocks.core.memory import VectorMemory
from ai_blocks.core.tool import ToolManager
from ai_blocks.utils.observability import (
    SpanStatus,
    get_observability_manager,
    record_counter,
    record_gauge,
    record_histogram,
    trace,
)


@trace("simulate_llm_request")
async def simulate_llm_request(prompt: str, model: str = "gpt-3.5-turbo") -> str:
    """LLMãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹"""
    manager = await get_observability_manager()

    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã‚’å¢—åŠ 
    await record_counter("llm_requests_total", labels={"model": model})

    # å‡¦ç†æ™‚é–“ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    processing_time = random.uniform(0.5, 3.0)

    with manager.trace(
        "llm_processing", {"model": model, "prompt_length": len(prompt)}
    ):
        await asyncio.sleep(processing_time)

        # å¿œç­”æ™‚é–“ã‚’ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã«è¨˜éŒ²
        await record_histogram(
            "llm_response_time_seconds", processing_time, {"model": model}
        )

        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚²ãƒ¼ã‚¸ã«è¨˜éŒ²
        token_count = len(prompt.split()) * 1.3  # æ¦‚ç®—
        await record_gauge("llm_tokens_processed", token_count, {"model": model})

        # ã‚¨ãƒ©ãƒ¼ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        if random.random() < 0.1:  # 10%ã®ç¢ºç‡ã§ã‚¨ãƒ©ãƒ¼
            await record_counter(
                "llm_errors_total", labels={"model": model, "error_type": "timeout"}
            )
            raise Exception("LLMã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼")

        return f"LLMå¿œç­”: {prompt[:50]}... (å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’)"


@trace("memory_operations")
async def simulate_memory_operations() -> None:
    """ãƒ¡ãƒ¢ãƒªæ“ä½œã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹"""
    manager = await get_observability_manager()

    memory = VectorMemory(max_items=100)

    # ãƒ‡ãƒ¼ã‚¿ä¿å­˜æ“ä½œ
    with manager.trace("memory_store_batch"):
        documents = [
            "äººå·¥çŸ¥èƒ½ã¯ç¾ä»£æŠ€è¡“ã®é‡è¦ãªåˆ†é‡ã§ã™",
            "æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯æ—¥ã€…é€²æ­©ã—ã¦ã„ã¾ã™",
            "æ·±å±¤å­¦ç¿’ã¯ç”»åƒèªè­˜ã«é©å‘½ã‚’ã‚‚ãŸã‚‰ã—ã¾ã—ãŸ",
            "è‡ªç„¶è¨€èªå‡¦ç†ã¯äººé–“ã¨ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã®æ©‹æ¸¡ã—ã§ã™",
            "å¼·åŒ–å­¦ç¿’ã¯ã‚²ãƒ¼ãƒ AIã§å¤§ããªæˆæœã‚’ä¸Šã’ã¦ã„ã¾ã™",
        ]

        for i, doc in enumerate(documents):
            store_time = time.time()
            await memory.store(doc, {"batch_id": "demo", "index": i})
            store_duration = time.time() - store_time

            await record_histogram("memory_store_duration_seconds", store_duration)
            await record_counter(
                "memory_operations_total", labels={"operation": "store"}
            )

    # æ¤œç´¢æ“ä½œ
    with manager.trace("memory_search_batch"):
        queries = ["äººå·¥çŸ¥èƒ½", "æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’"]

        for query in queries:
            search_time = time.time()
            results = await memory.search(query, limit=3)
            search_duration = time.time() - search_time

            await record_histogram("memory_search_duration_seconds", search_duration)
            await record_gauge(
                "memory_search_results_count",
                len(results),
                {"query_type": "similarity"},
            )
            await record_counter(
                "memory_operations_total", labels={"operation": "search"}
            )

    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¨˜éŒ²
    total_items = await memory.count()
    await record_gauge("memory_items_total", total_items)


@trace("tool_execution_simulation")
async def simulate_tool_execution() -> None:
    """ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹"""
    manager = await get_observability_manager()

    tool_manager = ToolManager()

    # è¤‡æ•°ã®ãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œ
    tools_to_test: List[Tuple[str, Dict[str, Any]]] = [
        ("echo", {"text": "Hello, World!"}),
        ("add", {"a": 10, "b": 20}),
        ("multiply", {"a": 5, "b": 7}),
        ("get_current_time", {}),
    ]

    for tool_name, params in tools_to_test:
        with manager.trace(f"tool_execution_{tool_name}", {"tool": tool_name}):
            execution_time = time.time()

            try:
                result = await tool_manager.execute(tool_name, params)
                execution_duration = time.time() - execution_time

                # æˆåŠŸãƒ¡ãƒˆãƒªã‚¯ã‚¹
                await record_histogram(
                    "tool_execution_duration_seconds",
                    execution_duration,
                    {"tool": tool_name},
                )
                await record_counter(
                    "tool_executions_total",
                    labels={"tool": tool_name, "status": "success"},
                )

                if result.success:
                    await record_gauge(
                        "tool_last_execution_success", 1, {"tool": tool_name}
                    )
                else:
                    await record_gauge(
                        "tool_last_execution_success", 0, {"tool": tool_name}
                    )
                    await record_counter(
                        "tool_errors_total",
                        labels={"tool": tool_name, "error_type": "execution_failed"},
                    )

            except Exception:
                execution_duration = time.time() - execution_time

                # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹
                await record_histogram(
                    "tool_execution_duration_seconds",
                    execution_duration,
                    {"tool": tool_name},
                )
                await record_counter(
                    "tool_executions_total",
                    labels={"tool": tool_name, "status": "error"},
                )
                await record_counter(
                    "tool_errors_total",
                    labels={"tool": tool_name, "error_type": "exception"},
                )
                await record_gauge(
                    "tool_last_execution_success", 0, {"tool": tool_name}
                )


async def demo_distributed_tracing():
    """åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã®ãƒ‡ãƒ¢"""
    print("\n" + "=" * 60)
    print("ğŸ” åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ãƒ‡ãƒ¢")
    print("=" * 60)

    manager = await get_observability_manager()

    # è¤‡é›‘ãªå‡¦ç†ãƒ•ãƒ­ãƒ¼ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹
    trace = manager.tracer.start_trace("complex_ai_workflow")

    try:
        # ä¸¦åˆ—å‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        tasks = [
            simulate_llm_request("äººå·¥çŸ¥èƒ½ã«ã¤ã„ã¦æ•™ãˆã¦", "gpt-3.5-turbo"),
            simulate_llm_request("æ©Ÿæ¢°å­¦ç¿’ã®åŸºç¤ã‚’èª¬æ˜ã—ã¦", "gpt-4"),
            simulate_memory_operations(),
            simulate_tool_execution(),
        ]

        results = await asyncio.gather(*tasks, return_exceptions=True)

        # çµæœã‚’å‡¦ç†
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"   ã‚¿ã‚¹ã‚¯ {i+1} ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {result}")
            else:
                print(f"   ã‚¿ã‚¹ã‚¯ {i+1} å®Œäº†")

    finally:
        manager.tracer.finish_trace(trace)

    # ãƒˆãƒ¬ãƒ¼ã‚¹çµæœã‚’è¡¨ç¤º
    print("\nğŸ“Š ãƒˆãƒ¬ãƒ¼ã‚¹çµæœ:")
    print(f"   ãƒˆãƒ¬ãƒ¼ã‚¹ID: {trace.trace_id}")
    print(f"   ç·å®Ÿè¡Œæ™‚é–“: {trace.duration:.3f}ç§’")
    print(f"   ã‚¹ãƒ‘ãƒ³æ•°: {len(trace.spans)}")

    # ã‚¹ãƒ‘ãƒ³ã®è©³ç´°
    print("\nğŸ“‹ ã‚¹ãƒ‘ãƒ³è©³ç´°:")
    for span in trace.spans.values():
        status_icon = "âœ…" if span.status == SpanStatus.OK else "âŒ"
        print(f"   {status_icon} {span.operation_name}: {span.duration:.3f}ç§’")
        if span.tags:
            for key, value in list(span.tags.items())[:3]:  # æœ€åˆã®3ã¤ã®ã‚¿ã‚°ã‚’è¡¨ç¤º
                print(f"      {key}: {value}")


async def demo_metrics_collection():
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã®ãƒ‡ãƒ¢"""
    print("\n" + "=" * 60)
    print("ğŸ“Š ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ãƒ‡ãƒ¢")
    print("=" * 60)

    manager = await get_observability_manager()

    # æ§˜ã€…ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ç”Ÿæˆ
    print("\n1ï¸âƒ£ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç”Ÿæˆä¸­...")

    # ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    for i in range(10):
        await record_counter(
            "demo_requests_total", labels={"endpoint": f"/api/v{i%3+1}"}
        )
        await record_counter("demo_errors_total", labels={"error_type": "validation"})

    # ã‚²ãƒ¼ã‚¸ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    await record_gauge("demo_active_users", random.randint(50, 200))
    await record_gauge("demo_queue_size", random.randint(0, 50))
    await record_gauge("demo_cache_hit_ratio", random.uniform(0.7, 0.95))

    # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    for _ in range(20):
        await record_histogram(
            "demo_request_duration_seconds", random.uniform(0.1, 2.0)
        )
        await record_histogram("demo_payload_size_bytes", random.uniform(100, 10000))

    # åé›†ã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¡¨ç¤º
    metrics = manager.metrics_collector.get_metrics()
    print(f"\n2ï¸âƒ£ åé›†ã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ ({len(metrics)}ä»¶):")

    for metric in metrics[:10]:  # æœ€åˆã®10ä»¶ã‚’è¡¨ç¤º
        labels_str = (
            ", ".join(f"{k}={v}" for k, v in metric.labels.items())
            if metric.labels
            else "ãªã—"
        )
        print(
            f"   {metric.name} ({metric.type}): {metric.value:.3f} [ãƒ©ãƒ™ãƒ«: {labels_str}]"
        )

    # Prometheuså½¢å¼ã§ã®å‡ºåŠ›
    print("\n3ï¸âƒ£ Prometheuså½¢å¼å‡ºåŠ›ï¼ˆæŠœç²‹ï¼‰:")
    prometheus_output = manager.metrics_collector.get_prometheus_format()
    lines = prometheus_output.split("\n")[:15]  # æœ€åˆã®15è¡Œã‚’è¡¨ç¤º
    for line in lines:
        print(f"   {line}")
    if len(prometheus_output.split("\n")) > 15:
        print("   ...")


async def demo_performance_monitoring():
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã®ãƒ‡ãƒ¢"""
    print("\n" + "=" * 60)
    print("ğŸ¥ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒ‡ãƒ¢")
    print("=" * 60)

    manager = await get_observability_manager()

    # ç¾åœ¨ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—
    current_metrics = manager.performance_monitor.get_current_metrics()

    if current_metrics:
        print("\nğŸ“ˆ ç¾åœ¨ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:")
        print(f"   CPUä½¿ç”¨ç‡: {current_metrics.cpu_usage:.1f}%")
        print(f"   ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {current_metrics.memory_usage:.1f}%")
        print(f"   ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°: {current_metrics.request_count}")
        print(f"   ã‚¨ãƒ©ãƒ¼æ•°: {current_metrics.error_count}")
        print(f"   å¹³å‡å¿œç­”æ™‚é–“: {current_metrics.avg_response_time:.3f}ç§’")
        print(f"   ã‚¢ã‚¯ãƒ†ã‚£ãƒ–æ¥ç¶šæ•°: {current_metrics.active_connections}")
    else:
        print("   ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¯ã¾ã åˆ©ç”¨ã§ãã¾ã›ã‚“")

    # å±¥æ­´ã‚’å–å¾—
    history = manager.performance_monitor.get_metrics_history(limit=5)

    if history:
        print(f"\nğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å±¥æ­´ (æœ€æ–°{len(history)}ä»¶):")
        for i, metrics in enumerate(history, 1):
            timestamp = time.strftime("%H:%M:%S", time.localtime(metrics.timestamp))
            print(
                f"   {i}. {timestamp} - CPU: {metrics.cpu_usage:.1f}%, ãƒ¡ãƒ¢ãƒª: {metrics.memory_usage:.1f}%"
            )

    # è² è·ãƒ†ã‚¹ãƒˆã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    print("\nâš¡ è² è·ãƒ†ã‚¹ãƒˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³:")

    # é«˜è² è·å‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    tasks = []
    for i in range(5):
        task = asyncio.create_task(simulate_llm_request(f"è² è·ãƒ†ã‚¹ãƒˆ {i}", "gpt-3.5-turbo"))
        tasks.append(task)

    start_time = time.time()
    results = await asyncio.gather(*tasks, return_exceptions=True)
    end_time = time.time()

    successful_requests = sum(1 for r in results if not isinstance(r, Exception))
    failed_requests = len(results) - successful_requests

    print(f"   ç·å®Ÿè¡Œæ™‚é–“: {end_time - start_time:.2f}ç§’")
    print(f"   æˆåŠŸãƒªã‚¯ã‚¨ã‚¹ãƒˆ: {successful_requests}")
    print(f"   å¤±æ•—ãƒªã‚¯ã‚¨ã‚¹ãƒˆ: {failed_requests}")


async def demo_observability_summary():
    """è¦³æ¸¬å¯èƒ½æ€§ã®æ¦‚è¦ãƒ‡ãƒ¢"""
    print("\n" + "=" * 60)
    print("ğŸ“‹ è¦³æ¸¬å¯èƒ½æ€§æ¦‚è¦ãƒ‡ãƒ¢")
    print("=" * 60)

    manager = await get_observability_manager()

    # æ¦‚è¦ã‚’å–å¾—
    summary = manager.get_observability_summary()

    print("\nğŸ¯ ã‚µãƒ¼ãƒ“ã‚¹æ¦‚è¦:")
    print(f"   ã‚µãƒ¼ãƒ“ã‚¹å: {summary['service_name']}")
    print(
        f"   ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(summary['timestamp']))}"
    )

    print("\nğŸ” ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°:")
    print(f"   ç·ãƒˆãƒ¬ãƒ¼ã‚¹æ•°: {summary['traces']['total_count']}")
    print(f"   ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒˆãƒ¬ãƒ¼ã‚¹æ•°: {summary['traces']['active_count']}")

    print("\nğŸ“Š ãƒ¡ãƒˆãƒªã‚¯ã‚¹:")
    print(f"   ç·ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ•°: {summary['metrics']['total_count']}")
    for metric_type, count in summary["metrics"]["types"].items():
        print(f"   {metric_type}: {count}ä»¶")

    if summary["performance"]:
        print("\nğŸ¥ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:")
        perf = summary["performance"]
        print(f"   CPUä½¿ç”¨ç‡: {perf['cpu_usage']:.1f}%")
        print(f"   ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {perf['memory_usage']:.1f}%")
        print(f"   å¹³å‡å¿œç­”æ™‚é–“: {perf['avg_response_time']:.3f}ç§’")


async def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¢å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ¯ AI Blocks é«˜åº¦ãªè¦³æ¸¬å¯èƒ½æ€§æ©Ÿèƒ½ãƒ‡ãƒ¢")
    print("=" * 80)

    try:
        # å„ãƒ‡ãƒ¢ã‚’é †æ¬¡å®Ÿè¡Œ
        await demo_distributed_tracing()
        await demo_metrics_collection()
        await demo_performance_monitoring()
        await demo_observability_summary()

        print("\n" + "=" * 80)
        print("ğŸ‰ å…¨ã¦ã®è¦³æ¸¬å¯èƒ½æ€§ãƒ‡ãƒ¢ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("=" * 80)

    except Exception as e:
        print(f"\nâŒ ãƒ‡ãƒ¢å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback

        traceback.print_exc()

    finally:
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        manager = await get_observability_manager()
        await manager.cleanup()


if __name__ == "__main__":
    asyncio.run(main())
