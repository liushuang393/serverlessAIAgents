#!/usr/bin/env bash

# åˆ›å»ºæ–‡ä»¶å¤¹
mkdir -p agent_codes

echo "ğŸš€ å¼€å§‹ç”Ÿæˆ 19 ä¸ª Python æ–‡ä»¶..."

# ========== æ–‡ä»¶ 1 ==========
cat << 'EOF' > agent_codes/1_OpenAI_agents.py
# OpenAI Agents SDK + MCP ç¤ºä¾‹
import asyncio
from openai_agents import Agent, create_agent
from mcp_client import MCPClient

async def setup_openai_agent_with_mcp():
    mcp = MCPClient()
    await mcp.connect_stdio(command="npx", args=["@tavily/mcp-server"])
    tools = await mcp.get_tools()
    agent = create_agent(name="æ¤œç´¢ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ", instructions="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æœç´¢åŠ©æ‰‹", tools=tools, model="gpt-4")
    return agent, mcp

async def main():
    agent, mcp = await setup_openai_agent_with_mcp()
    response = await agent.run("2025å¹´ã®AIåˆ†é‡ã®é‡è¦ãªé€²å±•ã‚’æ¤œç´¢ã—ã¦")
    print(response.content)
    await mcp.disconnect()

if __name__ == "__main__":
    asyncio.run(main())
EOF

# ========== æ–‡ä»¶ 2 ==========
cat << 'EOF' > agent_codes/2_LangGraph_agent.py
# LangGraph + MCP åŸºæœ¬ç”¨æ³•
import asyncio
from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI
from mcp_client import load_mcp_tools

async def create_langgraph_agent():
    tools = await load_mcp_tools(server_config={"command":"npx","args":["@tavily/mcp-server"]})
    llm = ChatOpenAI(model="gpt-4")
    agent = create_react_agent(llm=llm, tools=tools, state_modifier="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹")
    return agent

async def run():
    agent = await create_langgraph_agent()
    result = await agent.ainvoke({"messages":[("user","Python 3.12 ã®æ–°æ©Ÿèƒ½ã‚’ã¾ã¨ã‚ã¦")]}, config={"configurable":{"thread_id":"1"}})
    print(result["messages"][-1].content)

if __name__=="__main__":
    asyncio.run(run())
EOF

# ========== æ–‡ä»¶ 3 ==========
cat << 'EOF' > agent_codes/3_LangGraph_multi_server.py
# LangGraph å¤šMCP Server ç¤ºä¾‹
import asyncio
from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI
from langgraph.mcp import MultiServerMCPClient

async def create_multi():
    mcp = MultiServerMCPClient()
    await mcp.add_server("search", command="npx", args=["@tavily/mcp-server"])
    await mcp.add_server("db", command="npx", args=["@example/database-mcp-server"])
    tools = await mcp.get_all_tools()
    llm = ChatOpenAI(model="gpt-4")
    return create_react_agent(llm=llm, tools=tools), mcp

if __name__=="__main__":
    async def run():
        agent, mcp = await create_multi()
        res = await agent.ainvoke({"messages":[("user","Python async ç‰¹æ€§ã¾ã¨ã‚")]}, config={})
        print(res["messages"][-1].content)
        await mcp.disconnect()
    asyncio.run(run())
EOF

# ========== æ–‡ä»¶ 4 ==========
cat << 'EOF' > agent_codes/4_LangGraph_complex_workflow.py
# LangGraph å¤æ‚å·¥ä½œæµç¤ºä¾‹
import asyncio
from langgraph.graph import StateGraph, START, END
from typing import TypedDict
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent
from mcp_client import load_mcp_tools

class AgentState(TypedDict):
    messages: list
    research_data: dict
    final_report: str

async def create_research_workflow():
    tools = await load_mcp_tools(server_config={"command": "npx", "args": ["@tavily/mcp-server"]})
    llm = ChatOpenAI(model="gpt-4")

    async def research_node(state: AgentState):
        agent = create_react_agent(llm=llm, tools=tools, state_modifier="ä½ æ˜¯ç ”ç©¶å‘˜")
        res = await agent.ainvoke(
            {"messages": state["messages"] + [("user", f"æ·±å…¥ç ”ç©¶ï¼š{state['messages'][-1][1]}")]},
            config={"configurable": {"thread_id": "research"}}
        )
        return {"messages": res["messages"], "research_data": {"raw_data": res["messages"][-1].content}, "final_report": ""}

    async def analysis_node(state: AgentState):
        agent = create_react_agent(llm=llm, tools=[], state_modifier="ä½ æ˜¯åˆ†æå¸ˆ")
        prompt = f"åŸºäºï¼š{state['research_data']['raw_data']}ï¼Œè¯·åˆ†æå…³é”®å‘ç°å’Œæ½œåœ¨å½±å“"
        res = await agent.ainvoke({"messages": [("user", prompt)]}, config={"configurable": {"thread_id": "analysis"}})
        state["research_data"]["analysis"] = res["messages"][-1].content
        state["messages"] = res["messages"]
        return state

    async def report_node(state: AgentState):
        agent = create_react_agent(llm=llm, tools=[], state_modifier="ä½ æ˜¯æŠ¥å‘Šä¸“å®¶")
        prompt = f"è¯·ç”ŸæˆæŠ¥å‘Šï¼šåŸå§‹ç ”ç©¶ï¼š{state['research_data']['raw_data']}ï¼Œåˆ†æç»“æœï¼š{state['research_data']['analysis']}"
        res = await agent.ainvoke({"messages": [("user", prompt)]}, config={"configurable": {"thread_id": "report"}})
        state["final_report"] = res["messages"][-1].content
        return state

    wf = StateGraph(AgentState)
    wf.add_node("research", research_node)
    wf.add_node("analysis", analysis_node)
    wf.add_node("report", report_node)
    wf.add_edge(START, "research")
    wf.add_edge("research", "analysis")
    wf.add_edge("analysis", "report")
    wf.add_edge("report", END)
    return wf.compile()

async def run():
    app = await create_research_workflow()
    result = await app.ainvoke({"messages":[("user","AI åŒ»ç–—é¢†åŸŸåº”ç”¨å‰æ™¯")],"research_data":{},"final_report":""})
    print("æœ€ç»ˆæŠ¥å‘Šï¼š", result["final_report"])

if __name__=="__main__":
    asyncio.run(run())
EOF

# ========== æ–‡ä»¶ 5 ==========
cat << 'EOF' > agent_codes/5_LlamaIndex_basic.py
# LlamaIndex åŸºç¡€ Agent + MCP ç¤ºä¾‹
import asyncio
from llama_index.core import Settings
from llama_index.llms.openai import OpenAI
from llama_index.agent.openai import OpenAIAgent
from llama_index.tools.mcp import McpToolSpec
from mcp_client import BasicMCPClient

async def create():
    Settings.llm = OpenAI(model="gpt-4")
    mcp = BasicMCPClient()
    await mcp.connect_stdio(command="npx", args=["@tavily/mcp-server"])
    tools = McpToolSpec(mcp).to_tool_list()
    agent = OpenAIAgent.from_tools(
        tools=tools,
        system_prompt="ä½ æ˜¯ä¸“ä¸šAIåŠ©æ‰‹ï¼Œå…·å¤‡æœç´¢åˆ†æèƒ½åŠ›",
        verbose=True
    )
    return agent, mcp

async def run():
    agent, mcp = await create()
    resp = await agent.achat("2024å¹´å¤§è¯­è¨€æ¨¡å‹å‘å±•è¶‹åŠ¿æ€»ç»“")
    print(resp.response)
    await mcp.disconnect()

if __name__=="__main__":
    asyncio.run(run())
EOF

# ========== æ–‡ä»¶ 6 ==========
cat << 'EOF' > agent_codes/6_LlamaIndex_rag_agent.py
# LlamaIndex RAG + MCP ç¤ºä¾‹
import asyncio
from llama_index.core import Settings
from llama_index.llms.openai import OpenAI
from llama_index.core import VectorStoreIndex, Document
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.agent.openai import OpenAIAgent
from llama_index.tools.mcp import McpToolSpec
from llama_index.core.tools import QueryEngineTool
from mcp_client import BasicMCPClient

async def create_rag_agent():
    Settings.llm = OpenAI(model="gpt-4")
    Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-large")
    documents = [Document(text=t) for t in ["AI ç®€ä»‹", "æœºå™¨å­¦ä¹ æ¦‚è¿°", "æ·±åº¦å­¦ä¹ æŠ€æœ¯"]]
    index = VectorStoreIndex.from_documents(documents)
    query_engine = index.as_query_engine()
    mcp = BasicMCPClient()
    await mcp.connect_stdio(command="npx", args=["@tavily/mcp-server"])
    tools = [QueryEngineTool.from_defaults(query_engine=query_engine, name="kb", description="æŸ¥è¯¢KB")] + McpToolSpec(mcp).to_tool_list()
    agent = OpenAIAgent.from_tools(
        tools=tools,
        system_prompt="ä½ æ˜¯AIçŸ¥è¯†åŠ©æ‰‹ï¼Œç»“åˆå†…éƒ¨KBå’Œæœç´¢å·¥å…·å›ç­”",
        verbose=True
    )
    return agent, mcp

async def run():
    agent, mcp = await create_rag_agent()
    resp = await agent.achat("ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿå®ƒåœ¨2025å¹´æœ‰å“ªäº›æœ€æ–°è¿›å±•ï¼Ÿ")
    print(resp.response)
    await mcp.disconnect()

if __name__=="__main__":
    asyncio.run(run())
EOF

# ========== æ–‡ä»¶ 7 ==========
cat << 'EOF' > agent_codes/7_AutoGen_basic.py
# AutoGen å¤š Agent åä½œåŸºç¡€
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from mcp_client.integrations.autogen import AutoGenMCPClient
from mcp_client import StdioServerParams
from llama_index.llms.openai import OpenAI

async def create_autogen_agents():
    mcp = AutoGenMCPClient()
    await mcp.connect_server("search_tools", StdioServerParams(command="npx", args=["@tavily/mcp-server"]))
    tools = await mcp.get_tools("search_tools")
    researcher = AssistantAgent(name="ç ”ç©¶å‘˜", model_client=OpenAI(model="gpt-4"), tools=tools, system_message="ä½ æ˜¯ç ”ç©¶å‘˜")
    analyst = AssistantAgent(name="åˆ†æå¸ˆ", model_client=OpenAI(model="gpt-4"), system_message="ä½ æ˜¯åˆ†æå¸ˆ")
    reporter = AssistantAgent(name="æŠ¥å‘Šå‘˜", model_client=OpenAI(model="gpt-4"), system_message="ä½ æ˜¯æŠ¥å‘Šå‘˜")
    return researcher, analyst, reporter, mcp

async def run():
    r, a, p, mcp = await create_autogen_agents()
    team = RoundRobinGroupChat([r, a, p])
    result = await Console(team.run_stream(task="è¯·ç ”ç©¶åˆ†æ2024å¹´åŒºå—é“¾å‘å±•è¶‹åŠ¿"))
    print("åä½œç»“æœ:", result.messages[-1].content)
    await mcp.disconnect_all()

if __name__=="__main__":
    asyncio.run(run())
EOF
# ========== æ–‡ä»¶ 8 ==========
cat << 'EOF' > agent_codes/8_AutoGen_distributed.py
# AutoGen åˆ†å¸ƒå¼ MCP ç¤ºä¾‹
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from llama_index.llms.openai import OpenAI
from mcp_client import AutoGenMCPClient, SseServerParams

async def run():
    mcp_search = AutoGenMCPClient()
    await mcp_search.connect_server("remote_search", SseServerParams(url="http://localhost:8000/mcp", headers={"Authorization":"Bearer token"}))
    searcher = AssistantAgent(name="æœç´¢ä¸“å®¶", model_client=OpenAI(model="gpt-4"), tools=await mcp_search.get_tools("remote_search"), system_message="ä½ è´Ÿè´£æœç´¢")
    analyst = AssistantAgent(name="æœ¬åœ°åˆ†æå¸ˆ", model_client=OpenAI(model="gpt-4"), system_message="ä½ è´Ÿè´£åˆ†æ")
    team = RoundRobinGroupChat([searcher, analyst])
    result = await team.run(task="åˆ†æäº‘è®¡ç®—å¸‚åœºå‘å±•")
    print("è¾“å‡º:", result.messages[-1].content)
    await mcp_search.disconnect_all()

if __name__=="__main__":
    asyncio.run(run())
EOF

# ========== æ–‡ä»¶ 9 ==========
cat << 'EOF' > agent_codes/9_AutoGen_custom_tool.py
# AutoGen è‡ªå®šä¹‰ MCP å·¥å…·
import asyncio
from autogen_agentchat.base import Tool
from llama_index.llms.openai import OpenAI
from autogen_agentchat.agents import AssistantAgent
from mcp_client import AutoGenMCPClient, StdioServerParams

class CustomSearchTool(Tool):
    name = "tavily_search"
    description = "è‡ªå®šä¹‰æœç´¢å·¥å…·"
    def __init__(self, mcp): self.mcp = mcp; super().__init__()
    async def run(self, query:str): return await self.mcp.call_tool("tavily_search", {"query":query})
    @property
    def schema(self):
        return {"type":"function","function":{"name":self.name,"description":self.description,"parameters":{"type":"object","properties":{"query":{"type":"string"}},"required":["query"]}}}

async def run():
    mcp = AutoGenMCPClient()
    await mcp.connect_server("search", StdioServerParams(command="npx", args=["@tavily/mcp-server"]))
    tool = CustomSearchTool(mcp)
    agent = AssistantAgent(name="å¤šå·¥å…·ä¸“å®¶", model_client=OpenAI(model="gpt-4"), tools=[tool], system_message="ä½ å¯ç”¨å¤šå·¥å…·")
    from autogen_agentchat.teams import Swarm
    team = Swarm([agent])
    result = await team.run("æŸ¥æ‰¾Python asyncioæœ€ä½³å®è·µ")
    print("ç»“æœ:", result.messages[-1].content)
    await mcp.disconnect_all()

if __name__=="__main__":
    asyncio.run(run())
EOF

# ========== æ–‡ä»¶ 10 ==========
cat << 'EOF' > agent_codes/10_Pydantic_basic.py
# Pydantic AIï¼šç»“æ„åŒ–è¾“å‡ºåŸºç¡€
import asyncio
from pydantic_ai import Agent
from pydantic import BaseModel, Field
from mcp_client import MCPServerStdio

class SearchResult(BaseModel):
    title: str
    content: str
    url: str
    relevance_score: float = Field(ge=0.0, le=1.0)

class ResearchReport(BaseModel):
    topic: str
    executive_summary: str
    key_findings: list[SearchResult]
    conclusion: str
    confidence_level: float = Field(ge=0.0, le=1.0)

async def main():
    mcp = MCPServerStdio(command="npx", args=["@tavily/mcp-server"])
    tools = await mcp.get_tools()
    agent = Agent(model="openai:gpt-4", tools=tools, system_prompt="ä½ æ˜¯ç ”ç©¶åŠ©æ‰‹ï¼Œè¾“å‡ºç»“æ„åŒ–æŠ¥å‘Š", result_type=ResearchReport)
    result = await agent.run("ç ”ç©¶AIåœ¨æ•™è‚²é¢†åŸŸçš„åº”ç”¨ç°çŠ¶")
    print(result.data)
    await mcp.disconnect()

if __name__=="__main__":
    asyncio.run(main())
EOF

# ========== æ–‡ä»¶ 11 ==========
cat << 'EOF' > agent_codes/11_Pydantic_advanced.py
# Pydantic AIï¼šå¤šæ­¥éª¤åˆ†æ
import asyncio
from pydantic_ai import Agent
from pydantic import BaseModel, Field
from mcp_client import MCPServerHTTP

class AnalysisStep(BaseModel):
    step_name: str
    description: str
    tools_used: list[str]
    findings: str
    confidence: float = Field(ge=0.0, le=1.0)

class MultiStepAnalysis(BaseModel):
    query: str
    analysis_steps: list[AnalysisStep]
    final_answer: str
    overall_confidence: float = Field(ge=0.0, le=1.0)
    sources_cited: int

async def main():
    mcp = MCPServerHTTP(base_url="http://localhost:8000/mcp", headers={"Authorization":"Bearer token"})
    tools = await mcp.get_tools()
    agent = Agent(model="openai:gpt-4", tools=tools, system_prompt="å¤šæ­¥éª¤åˆ†æä¸“å®¶", result_type=MultiStepAnalysis)
    result = await agent.run("åˆ†æåŒºå—é“¾åœ¨ä¾›åº”é“¾ä¸­çš„å‰æ™¯å’ŒæŒ‘æˆ˜")
    print(result.data)
    await mcp.disconnect()

if __name__=="__main__":
    asyncio.run(main())
EOF

# ========== æ–‡ä»¶ 12 ==========
cat << 'EOF' > agent_codes/12_Smolagents_basic.py
# SmolAgents åŸºç¡€ç¤ºä¾‹
import asyncio
from smolagents import CodeAgent, ToolCollection
from mcp_client import MCPClient

async def main():
    mcp = MCPClient()
    await mcp.connect_stdio(command="npx", args=["@tavily/mcp-server"])
    tools = ToolCollection.from_mcp(mcp)
    agent = CodeAgent(tools=tools, model="gpt-4", system_prompt="ä½ æ˜¯ä»£ç ç”ŸæˆåŠ©æ‰‹")
    result = await agent.run("æœç´¢ Python å¼‚æ­¥æœ€ä½³å®è·µï¼Œå¹¶æå–è¦ç‚¹")
    print("ä»£ç :", result.code)
    print("è¾“å‡º:", result.output)
    await mcp.disconnect()

if __name__=="__main__":
    asyncio.run(main())
EOF

# ========== æ–‡ä»¶ 13 ==========
cat << 'EOF' > agent_codes/13_Smolagents_custom.py
# SmolAgents è‡ªå®šä¹‰å·¥å…·
import asyncio
from smolagents import CodeAgent, Tool
from mcp_client import MCPClient

class CustomTool(Tool):
    name = "custom_search"
    description = "è‡ªå®šä¹‰æœç´¢"
    def __init__(self, mcp): self.mcp = mcp; super().__init__()
    async def forward(self, query: str): return await self.mcp.call_tool("tavily_search", {"query": query})

async def main():
    mcp = MCPClient()
    await mcp.connect_stdio(command="npx", args=["@tavily/mcp-server"])
    tool = CustomTool(mcp)
    agent = CodeAgent(tools=[tool], model="gpt-4", system_prompt="è‡ªå®šä¹‰æœç´¢åˆ†æå¸ˆ")
    result = await agent.run("ä½¿ç”¨è‡ªå®šä¹‰å·¥å…·æœç´¢æœºå™¨å­¦ä¹ æ¨¡å‹éƒ¨ç½²")
    print("ä»£ç :", result.code)
    print("è¾“å‡º:", result.output)
    await mcp.disconnect()

if __name__=="__main__":
    asyncio.run(main())
EOF

# ========== æ–‡ä»¶ 14 ==========
cat << 'EOF' > agent_codes/14_Smolagents_batch.py
# SmolAgents æ‰¹é‡ä»»åŠ¡
import asyncio
from smolagents import CodeAgent, ToolCollection
from mcp_client import MCPClient

async def main():
    mcp = MCPClient()
    await mcp.connect_stdio(command="npx", args=["@tavily/mcp-server"])
    tools = ToolCollection.from_mcp(mcp)
    agent = CodeAgent(tools=tools, model="gpt-4", system_prompt="æ‰¹é‡ä»»åŠ¡å¤„ç†ä¸“å®¶")
    queries = ["AI in finance", "åŒºå—é“¾å‘å±•", "äº‘è®¡ç®—å®‰å…¨", "ç‰©è”ç½‘è¶‹åŠ¿"]
    result = await agent.run(f"æ‰¹é‡æœç´¢å¹¶æ€»ç»“ï¼š{queries}")
    print("ä»£ç :", result.code)
    print("è¾“å‡º:", result.output)
    await mcp.disconnect()

if __name__=="__main__":
    asyncio.run(main())
EOF

# ========== æ–‡ä»¶ 15 ==========
cat << 'EOF' > agent_codes/15_Camel_basic_collab.py
# Camel åä½œåŸºç¡€
import asyncio
from camel.agents import ChatAgent
from camel.messages import BaseMessage
from camel.types import ModelType
from camel.toolkits import MCPToolkit
from mcp_client import MCPClient

async def main():
    mcp = MCPClient()
    await mcp.connect_stdio(command="npx", args=["@tavily/mcp-server"])
    tools = MCPToolkit(mcp).get_tools()
    researcher = ChatAgent(system_message=BaseMessage.make_assistant_message("ç ”ç©¶å‘˜", "è´Ÿè´£ç ”ç©¶"), model_type=ModelType.GPT_4, tools=tools)
    strategist = ChatAgent(system_message=BaseMessage.make_assistant_message("ç­–ç•¥å¸ˆ", "è´Ÿè´£ç­–ç•¥"), model_type=ModelType.GPT_4)
    executor = ChatAgent(system_message=BaseMessage.make_assistant_message("æ‰§è¡Œå®˜", "è´Ÿè´£æ‰§è¡Œ"), model_type=ModelType.GPT_4)
    prompt = BaseMessage.make_user_message("é¡¹ç›®ç»ç†", "AI åœ¨é›¶å”®ä¸šçš„å•†ä¸šç­–ç•¥")
    res1 = await researcher.step(prompt)
    res2 = await strategist.step(BaseMessage.make_user_message("é¡¹ç›®ç»ç†", res1.msg.content))
    res3 = await executor.step(BaseMessage.make_user_message("é¡¹ç›®ç»ç†", res2.msg.content))
    print("ç ”ç©¶:", res1.msg.content)
    print("ç­–ç•¥:", res2.msg.content)
    print("æ‰§è¡Œ:", res3.msg.content)
    await mcp.disconnect()

if __name__=="__main__":
    asyncio.run(main())
EOF

# ========== æ–‡ä»¶ 16 ==========
cat << 'EOF' > agent_codes/16_Camel_dynamic_roles.py
# Camel åŠ¨æ€è§’è‰²æ‰®æ¼”
import asyncio
from camel.societies import RolePlaying
from camel.toolkits import MCPToolkit
from mcp_client import MCPClient
from camel.types import ModelType
from camel.messages import BaseMessage

async def main():
    mcp = MCPClient()
    await mcp.connect_stdio(command="npx", args=["@tavily/mcp-server"])
    tools = MCPToolkit(mcp).get_tools()
    role_play = RolePlaying(assistant_role_name="AIä¸“å®¶", user_role_name="é¡¾é—®", assistant_agent_kwargs={"model_type":ModelType.GPT_4, "tools":tools}, user_agent_kwargs={"model_type":ModelType.GPT_4})
    prompt = BaseMessage.make_user_message("é¡¹ç›®ç»ç†", "åˆ¶å®šåˆ¶é€ ä¸šAIè½¬å‹æˆ˜ç•¥")
    for i in range(3):
        a_msg, u_msg = await role_play.step(prompt)
        print(f"ç¬¬{i+1}è½® AIä¸“å®¶: {a_msg.content}")
        print(f"ç¬¬{i+1}è½® é¡¾é—®: {u_msg.content}")
        prompt = u_msg
    await mcp.disconnect()

if __name__=="__main__":
    asyncio.run(main())
EOF

# ========== æ–‡ä»¶ 17 ==========
cat << 'EOF' > agent_codes/17_Camel_professional_team.py
# Camel ä¸“ä¸šå›¢é˜Ÿ
import asyncio
from camel.agents import ChatAgent
from camel.messages import BaseMessage
from camel.types import ModelType
from camel.toolkits import MCPToolkit
from mcp_client import MCPClient

async def main():
    mcp = MCPClient()
    await mcp.connect_stdio(command="npx", args=["@tavily/mcp-server"])
    tools = MCPToolkit(mcp).get_tools()
    roles = {"æ•°æ®ç§‘å­¦å®¶": "æ•°æ®åˆ†æ", "äº§å“ç»ç†": "äº§å“ç­–ç•¥", "æ¶æ„å¸ˆ": "æŠ€æœ¯æ¶æ„", "åˆ†æå¸ˆ": "ä¸šåŠ¡åˆ†æ"}
    agents = {r: ChatAgent(system_message=BaseMessage.make_assistant_message(r, f"{desc}ä¸“å®¶"), model_type=ModelType.GPT_4, tools=tools if r != "æ¶æ„å¸ˆ" else []) for r, desc in roles.items()}
    for role, agent in agents.items():
        prompt = BaseMessage.make_user_message("é¡¹ç›®ç»ç†", f"å°± AI å®¢æœé¡¹ç›®ï¼Œ{role} æä¾›å»ºè®®")
        res = await agent.step(prompt)
        print(f"{role}: {res.msg.content}")
    await mcp.disconnect()

if __name__=="__main__":
    asyncio.run(main())
EOF

# ========== æ–‡ä»¶ 18 ==========
cat << 'EOF' > agent_codes/18_CrewAI_basic.py
# CrewAI åŸºç¡€
import asyncio
from crewai import Agent, Task, Crew, Process
from crewai.tools import BaseTool
from mcp_client import MCPClient

class MCPSearchTool(BaseTool):
    name = "MCPæœç´¢"
    description = "æœç´¢å·¥å…·"
    def __init__(self, mcp): self.mcp = mcp; super().__init__()
    def _run(self, query: str): return asyncio.run(self.mcp.call_tool("tavily_search", {"query": query}))

async def run():
    mcp = MCPClient()
    await mcp.connect_stdio(command="npx", args=["@tavily/mcp-server"])
    tool = MCPSearchTool(mcp)
    researcher = Agent(role="ç ”ç©¶å‘˜", goal="ç ”ç©¶AIåŒ»ç–—", tools=[tool], verbose=True)
    analyst = Agent(role="åˆ†æå¸ˆ", goal="åˆ†æ", verbose=True)
    writer = Agent(role="æ’°ç¨¿äºº", goal="å†™æŠ¥å‘Š", verbose=True)
    tasks = [Task("ç ”ç©¶", "ç ”ç©¶æŠ¥å‘Š", researcher), Task("åˆ†æ", "åˆ†ææŠ¥å‘Š", analyst), Task("å†™ä½œ", "æ–‡ç« ", writer)]
    crew = Crew(agents=[researcher, analyst, writer], tasks=tasks, process=Process.sequential, verbose=2)
    result = crew.kickoff()
    print(result)
    await mcp.disconnect()

if __name__=="__main__":
    asyncio.run(run())
EOF

# ========== æ–‡ä»¶ 19 ==========
cat << 'EOF' > agent_codes/19_CrewAI_advanced.py
# CrewAI é«˜çº§
import asyncio
from crewai import Agent, Task, Crew, Process
from crewai.tools import BaseTool
from mcp_client import MCPClient

class MCPSearchTool(BaseTool):
    name = "MCPæœç´¢"
    description = "é«˜çº§æœç´¢"
    def __init__(self, mcp): self.mcp = mcp; super().__init__()
    def _run(self, query: str): return asyncio.run(self.mcp.call_tool("tavily_search", {"query": query}))

async def run():
    mcp = MCPClient()
    await mcp.connect_stdio(command="npx", args=["@tavily/mcp-server"])
    tool = MCPSearchTool(mcp)
    researcher = Agent(role="å¸‚åœºç ”ç©¶", goal="å¸‚åœºç ”ç©¶", tools=[tool], verbose=True, max_iter=3, memory=True)
    analyst = Agent(role="æŠ€æœ¯åˆ†æ", goal="æŠ€æœ¯åˆ†æ", tools=[tool], verbose=True, max_iter=3, memory=True)
    strategist = Agent(role="æˆ˜ç•¥", goal="æˆ˜ç•¥å»ºè®®", verbose=True, max_iter=3, memory=True)
    tasks = [Task("å¸‚åœºè°ƒç ”", "å¸‚åœºæŠ¥å‘Š", researcher), Task("æŠ€æœ¯åˆ†æ", "æŠ€æœ¯æŠ¥å‘Š", analyst), Task("æˆ˜ç•¥åˆ¶å®š", "æˆ˜ç•¥æ–¹æ¡ˆ", strategist)]
    crew = Crew(agents=[researcher, analyst, strategist], tasks=tasks, process=Process.sequential, verbose=2)
    result = crew.kickoff()
    print(result)
    await mcp.disconnect()

if __name__=="__main__":
    asyncio.run(run())
EOF

echo "âœ… æ–‡ä»¶ 1 ~ 19 å…¨éƒ¨ç”Ÿæˆå®Œæˆï¼"


