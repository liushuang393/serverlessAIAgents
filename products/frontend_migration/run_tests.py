#!/usr/bin/env python3
"""
ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç§»è¡Œã‚·ã‚¹ãƒ†ãƒ  ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€å˜ä½“ãƒ†ã‚¹ãƒˆã€çµ±åˆãƒ†ã‚¹ãƒˆã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã€
åŒ…æ‹¬çš„ãªãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚

ä½¿ç”¨æ–¹æ³•:
    python run_tests.py [--unit] [--integration] [--performance] [--all]

ã‚ªãƒ—ã‚·ãƒ§ãƒ³:
    --unit: å˜ä½“ãƒ†ã‚¹ãƒˆã®ã¿å®Ÿè¡Œ
    --integration: çµ±åˆãƒ†ã‚¹ãƒˆã®ã¿å®Ÿè¡Œ
    --performance: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã®ã¿å®Ÿè¡Œ
    --all: å…¨ã¦ã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
    --report: HTMLãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    --coverage: ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
"""

import argparse
import json
import subprocess
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))


class TestRunner:
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self, output_dir: str = "test_reports"):
        """
        ãƒ†ã‚¹ãƒˆãƒ©ãƒ³ãƒŠãƒ¼ã‚’åˆæœŸåŒ–

        Args:
            output_dir: ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.test_results: Dict[str, Any] = {}
        self.start_time: Optional[datetime] = None
        self.end_time: Optional[datetime] = None

    def run_unit_tests(self) -> Dict[str, Any]:
        """
        å˜ä½“ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ

        Returns:
            ãƒ†ã‚¹ãƒˆçµæœã®è¾æ›¸
        """
        print("ğŸ§ª å˜ä½“ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")

        cmd = [
            "conda",
            "run",
            "-n",
            "agent_ragenv",
            "python",
            "-m",
            "pytest",
            "products/frontend_migration/tests/unit/",
            "-v",
            "--tb=short",
            "--json-report",
            f"--json-report-file=products/frontend_migration/{self.output_dir}/unit_test_results.json",
        ]

        try:
            result = subprocess.run(
                cmd, capture_output=True, text=True, cwd=project_root
            )

            # JSONçµæœã‚’èª­ã¿è¾¼ã¿
            json_file = (
                Path("products/frontend_migration")
                / self.output_dir
                / "unit_test_results.json"
            )
            if json_file.exists():
                with open(json_file, "r", encoding="utf-8") as f:
                    test_data = json.load(f)
                # summaryã®æ§‹é€ ã‚’ç¢ºèªã—ã¦é©åˆ‡ã«å–å¾—
                summary = test_data.get("summary", {})
                if "total" not in summary and "collected" in summary:
                    # pytest-json-reportã®æ–°ã—ã„å½¢å¼ã«å¯¾å¿œ
                    summary = {
                        "total": summary.get("collected", 0),
                        "passed": summary.get("passed", 0),
                        "failed": summary.get("failed", 0),
                    }
                elif "total" in summary:
                    # æ—¢å­˜ã®å½¢å¼ã‚’ãã®ã¾ã¾ä½¿ç”¨
                    pass
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    summary = {"total": 0, "passed": 0, "failed": 0}
                test_data["summary"] = summary
            else:
                test_data = {"summary": {"total": 0, "passed": 0, "failed": 0}}

            return {
                "type": "unit",
                "status": "passed" if result.returncode == 0 else "failed",
                "return_code": result.returncode,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "summary": test_data.get("summary", {}),
                "duration": test_data.get("duration", 0),
            }

        except Exception as e:
            return {
                "type": "unit",
                "status": "error",
                "error": str(e),
                "return_code": -1,
            }

    def run_integration_tests(self) -> Dict[str, Any]:
        """
        çµ±åˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ

        Returns:
            ãƒ†ã‚¹ãƒˆçµæœã®è¾æ›¸
        """
        print("ğŸ”— çµ±åˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")

        cmd = [
            "conda",
            "run",
            "-n",
            "agent_ragenv",
            "python",
            "-m",
            "pytest",
            "products/frontend_migration/tests/integration/",
            "-v",
            "--tb=short",
            "--json-report",
            f"--json-report-file=products/frontend_migration/{self.output_dir}/integration_test_results.json",
        ]

        try:
            result = subprocess.run(
                cmd, capture_output=True, text=True, cwd=project_root
            )

            # JSONçµæœã‚’èª­ã¿è¾¼ã¿
            json_file = (
                Path("products/frontend_migration")
                / self.output_dir
                / "integration_test_results.json"
            )
            if json_file.exists():
                with open(json_file, "r", encoding="utf-8") as f:
                    test_data = json.load(f)
                # summaryã®æ§‹é€ ã‚’ç¢ºèªã—ã¦é©åˆ‡ã«å–å¾—
                summary = test_data.get("summary", {})
                if "total" not in summary and "collected" in summary:
                    # pytest-json-reportã®æ–°ã—ã„å½¢å¼ã«å¯¾å¿œ
                    summary = {
                        "total": summary.get("collected", 0),
                        "passed": summary.get("passed", 0),
                        "failed": summary.get("failed", 0),
                    }
                elif "total" in summary:
                    # æ—¢å­˜ã®å½¢å¼ã‚’ãã®ã¾ã¾ä½¿ç”¨
                    pass
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    summary = {"total": 0, "passed": 0, "failed": 0}
                test_data["summary"] = summary
            else:
                test_data = {"summary": {"total": 0, "passed": 0, "failed": 0}}

            return {
                "type": "integration",
                "status": "passed" if result.returncode == 0 else "failed",
                "return_code": result.returncode,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "summary": test_data.get("summary", {}),
                "duration": test_data.get("duration", 0),
            }

        except Exception as e:
            return {
                "type": "integration",
                "status": "error",
                "error": str(e),
                "return_code": -1,
            }

    def run_performance_tests(self) -> Dict[str, Any]:
        """
        ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ

        Returns:
            ãƒ†ã‚¹ãƒˆçµæœã®è¾æ›¸
        """
        print("âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")

        cmd = [
            "conda",
            "run",
            "-n",
            "agent_ragenv",
            "python",
            "-m",
            "pytest",
            "products/frontend_migration/tests/",
            "-v",
            "-k",
            "performance",
            "--tb=short",
            "--json-report",
            f"--json-report-file=products/frontend_migration/{self.output_dir}/performance_test_results.json",
        ]

        try:
            result = subprocess.run(
                cmd, capture_output=True, text=True, cwd=project_root
            )

            # JSONçµæœã‚’èª­ã¿è¾¼ã¿
            json_file = (
                Path("products/frontend_migration")
                / self.output_dir
                / "performance_test_results.json"
            )
            if json_file.exists():
                with open(json_file, "r", encoding="utf-8") as f:
                    test_data = json.load(f)
                # summaryã®æ§‹é€ ã‚’ç¢ºèªã—ã¦é©åˆ‡ã«å–å¾—
                summary = test_data.get("summary", {})
                if "total" not in summary and "collected" in summary:
                    # pytest-json-reportã®æ–°ã—ã„å½¢å¼ã«å¯¾å¿œ
                    summary = {
                        "total": summary.get("collected", 0),
                        "passed": summary.get("passed", 0),
                        "failed": summary.get("failed", 0),
                    }
                elif "total" in summary:
                    # æ—¢å­˜ã®å½¢å¼ã‚’ãã®ã¾ã¾ä½¿ç”¨
                    pass
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    summary = {"total": 0, "passed": 0, "failed": 0}
                test_data["summary"] = summary
            else:
                test_data = {"summary": {"total": 0, "passed": 0, "failed": 0}}

            return {
                "type": "performance",
                "status": "passed" if result.returncode == 0 else "failed",
                "return_code": result.returncode,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "summary": test_data.get("summary", {}),
                "duration": test_data.get("duration", 0),
            }

        except Exception as e:
            return {
                "type": "performance",
                "status": "error",
                "error": str(e),
                "return_code": -1,
            }

    def run_coverage_analysis(self) -> Dict[str, Any]:
        """
        ã‚«ãƒãƒ¬ãƒƒã‚¸è§£æã‚’å®Ÿè¡Œ

        Returns:
            ã‚«ãƒãƒ¬ãƒƒã‚¸çµæœã®è¾æ›¸
        """
        print("ğŸ“Š ã‚«ãƒãƒ¬ãƒƒã‚¸è§£æã‚’å®Ÿè¡Œä¸­...")

        cmd = [
            "conda",
            "run",
            "-n",
            "agent_ragenv",
            "python",
            "-m",
            "pytest",
            "products/frontend_migration/tests/",
            "--cov=products.frontend_migration",
            "--cov-report=html:products/frontend_migration/"
            + str(self.output_dir / "coverage_html"),
            "--cov-report=json:products/frontend_migration/"
            + str(self.output_dir / "coverage.json"),
            "--cov-report=term",
        ]

        try:
            _ = subprocess.run(cmd, capture_output=True, text=True, cwd=project_root)

            # ã‚«ãƒãƒ¬ãƒƒã‚¸JSONã‚’èª­ã¿è¾¼ã¿
            coverage_file = (
                Path("products/frontend_migration") / self.output_dir / "coverage.json"
            )
            if coverage_file.exists():
                with open(coverage_file, "r", encoding="utf-8") as f:
                    coverage_data = json.load(f)

                total_coverage = coverage_data.get("totals", {}).get(
                    "percent_covered", 0
                )
            else:
                total_coverage = 0

            return {
                "type": "coverage",
                "status": "completed",
                "total_coverage": total_coverage,
                "html_report": str(self.output_dir / "coverage_html" / "index.html"),
                "json_report": str(coverage_file),
            }

        except Exception as e:
            return {"type": "coverage", "status": "error", "error": str(e)}

    def generate_summary_report(self) -> None:
        """ç·åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        print("ğŸ“‹ ç·åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")

        # ç·åˆçµ±è¨ˆã‚’è¨ˆç®—
        total_tests = 0
        total_passed = 0
        total_failed = 0
        total_duration = 0

        for result in self.test_results.values():
            if "summary" in result:
                summary = result["summary"]
                total_tests += summary.get("total", 0)
                total_passed += summary.get("passed", 0)
                total_failed += summary.get("failed", 0)
                total_duration += result.get("duration", 0)

        # ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        report_data = {
            "test_run_info": {
                "start_time": self.start_time.isoformat() if self.start_time else None,
                "end_time": self.end_time.isoformat() if self.end_time else None,
                "total_duration": (self.end_time - self.start_time).total_seconds()
                if self.start_time and self.end_time
                else 0,
                "timestamp": datetime.now().isoformat(),
            },
            "summary": {
                "total_tests": total_tests,
                "passed_tests": total_passed,
                "failed_tests": total_failed,
                "success_rate": (total_passed / max(total_tests, 1)) * 100,
                "total_test_duration": total_duration,
            },
            "test_results": self.test_results,
        }

        # JSONãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜
        json_report_path = self.output_dir / "test_summary.json"
        with open(json_report_path, "w", encoding="utf-8") as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)

        # Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
        markdown_report = self._generate_markdown_report(report_data)
        markdown_report_path = self.output_dir / "test_summary.md"
        with open(markdown_report_path, "w", encoding="utf-8") as f:
            f.write(markdown_report)

        print("âœ… ç·åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ:")
        print(f"   JSON: {json_report_path}")
        print(f"   Markdown: {markdown_report_path}")

    def _generate_markdown_report(self, report_data: Dict[str, Any]) -> str:
        """Markdownå½¢å¼ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        summary = report_data["summary"]
        test_info = report_data["test_run_info"]

        report = f"""# ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç§»è¡Œã‚·ã‚¹ãƒ†ãƒ  ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆ

## ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæƒ…å ±
- **å®Ÿè¡Œé–‹å§‹æ™‚åˆ»**: {test_info.get('start_time', 'N/A')}
- **å®Ÿè¡Œçµ‚äº†æ™‚åˆ»**: {test_info.get('end_time', 'N/A')}
- **ç·å®Ÿè¡Œæ™‚é–“**: {test_info.get('total_duration', 0):.2f}ç§’

## ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼
- **ç·ãƒ†ã‚¹ãƒˆæ•°**: {summary['total_tests']}
- **æˆåŠŸ**: {summary['passed_tests']}
- **å¤±æ•—**: {summary['failed_tests']}
- **æˆåŠŸç‡**: {summary['success_rate']:.1f}%
- **ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚é–“**: {summary['total_test_duration']:.2f}ç§’

## è©³ç´°çµæœ

"""

        for test_type, result in report_data["test_results"].items():
            status_emoji = (
                "âœ…"
                if result["status"] == "passed"
                else "âŒ"
                if result["status"] == "failed"
                else "âš ï¸"
            )

            report += f"""### {test_type.title()}ãƒ†ã‚¹ãƒˆ {status_emoji}
- **ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: {result["status"]}
- **å®Ÿè¡Œæ™‚é–“**: {result.get("duration", 0):.2f}ç§’
"""

            if "summary" in result:
                test_summary = result["summary"]
                report += f"""- **ç·ãƒ†ã‚¹ãƒˆæ•°**: {test_summary.get("total", 0)}
- **æˆåŠŸ**: {test_summary.get("passed", 0)}
- **å¤±æ•—**: {test_summary.get("failed", 0)}
"""

            if result["status"] == "failed" and "stderr" in result:
                report += f"""
**ã‚¨ãƒ©ãƒ¼è©³ç´°**:
```
{result["stderr"][:500]}...
```
"""

            report += "\n"

        # ã‚«ãƒãƒ¬ãƒƒã‚¸æƒ…å ±ã‚’è¿½åŠ 
        if "coverage" in report_data["test_results"]:
            coverage_result = report_data["test_results"]["coverage"]
            if coverage_result["status"] == "completed":
                report += f"""## ã‚«ãƒãƒ¬ãƒƒã‚¸æƒ…å ±
- **ç·ã‚«ãƒãƒ¬ãƒƒã‚¸**: {coverage_result.get('total_coverage', 0):.1f}%
- **HTMLãƒ¬ãƒãƒ¼ãƒˆ**: {coverage_result.get('html_report', 'N/A')}

"""

        report += """---
*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯ AI Blocks ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç§»è¡Œã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆãƒ©ãƒ³ãƒŠãƒ¼ã«ã‚ˆã£ã¦è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚*
"""

        return report

    def run_tests(self, test_types: List[str], include_coverage: bool = False) -> None:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ—ã‚’å®Ÿè¡Œ

        Args:
            test_types: å®Ÿè¡Œã™ã‚‹ãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ—ã®ãƒªã‚¹ãƒˆ
            include_coverage: ã‚«ãƒãƒ¬ãƒƒã‚¸è§£æã‚’å«ã‚ã‚‹ã‹ã©ã†ã‹
        """
        self.start_time = datetime.now()

        print(f"ğŸš€ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚’é–‹å§‹: {', '.join(test_types)}")
        print(f"ğŸ“ ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›å…ˆ: {self.output_dir}")

        # å„ãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ—ã‚’å®Ÿè¡Œ
        if "unit" in test_types:
            self.test_results["unit"] = self.run_unit_tests()

        if "integration" in test_types:
            self.test_results["integration"] = self.run_integration_tests()

        if "performance" in test_types:
            self.test_results["performance"] = self.run_performance_tests()

        # ã‚«ãƒãƒ¬ãƒƒã‚¸è§£æã‚’å®Ÿè¡Œ
        if include_coverage:
            self.test_results["coverage"] = self.run_coverage_analysis()

        self.end_time = datetime.now()

        # ç·åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
        self.generate_summary_report()

        # çµæœã‚’è¡¨ç¤º
        self._print_summary()

    def _print_summary(self) -> None:
        """ãƒ†ã‚¹ãƒˆçµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        print("\n" + "=" * 60)
        print("ğŸ“Š ãƒ†ã‚¹ãƒˆå®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼")
        print("=" * 60)

        total_tests = 0
        total_passed = 0
        total_failed = 0

        for test_type, result in self.test_results.items():
            if test_type == "coverage":
                continue

            status_emoji = (
                "âœ…"
                if result["status"] == "passed"
                else "âŒ"
                if result["status"] == "failed"
                else "âš ï¸"
            )
            print(f"{status_emoji} {test_type.title()}ãƒ†ã‚¹ãƒˆ: {result['status']}")

            if "summary" in result:
                summary = result["summary"]
                total_tests += summary.get("total", 0)
                total_passed += summary.get("passed", 0)
                total_failed += summary.get("failed", 0)
                print(
                    f"   ãƒ†ã‚¹ãƒˆæ•°: {summary.get('total', 0)}, "
                    f"æˆåŠŸ: {summary.get('passed', 0)}, "
                    f"å¤±æ•—: {summary.get('failed', 0)}"
                )

        print("-" * 60)
        success_rate = (total_passed / max(total_tests, 1)) * 100
        print(f"ğŸ“ˆ ç·åˆçµæœ: {total_tests}ãƒ†ã‚¹ãƒˆä¸­ {total_passed}æˆåŠŸ ({success_rate:.1f}%)")

        if "coverage" in self.test_results:
            coverage_result = self.test_results["coverage"]
            if coverage_result["status"] == "completed":
                print(f"ğŸ“Š ã‚³ãƒ¼ãƒ‰ã‚«ãƒãƒ¬ãƒƒã‚¸: {coverage_result.get('total_coverage', 0):.1f}%")

        if self.end_time and self.start_time:
            duration = (self.end_time - self.start_time).total_seconds()
            print(f"â±ï¸  å®Ÿè¡Œæ™‚é–“: {duration:.2f}ç§’")
        else:
            print("â±ï¸  å®Ÿè¡Œæ™‚é–“: ä¸æ˜")
        print("=" * 60)


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç§»è¡Œã‚·ã‚¹ãƒ†ãƒ  ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
    parser.add_argument("--unit", action="store_true", help="å˜ä½“ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ")
    parser.add_argument("--integration", action="store_true", help="çµ±åˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ")
    parser.add_argument("--performance", action="store_true", help="ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ")
    parser.add_argument("--all", action="store_true", help="å…¨ã¦ã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ")
    parser.add_argument("--coverage", action="store_true", help="ã‚«ãƒãƒ¬ãƒƒã‚¸è§£æã‚’å®Ÿè¡Œ")
    parser.add_argument("--output", default="test_reports", help="ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")

    args = parser.parse_args()

    # ãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ—ã‚’æ±ºå®š
    test_types = []
    if args.unit:
        test_types.append("unit")
    if args.integration:
        test_types.append("integration")
    if args.performance:
        test_types.append("performance")

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¾ãŸã¯--allãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã¯å…¨ã¦ã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
    if args.all or not test_types:
        test_types = ["unit", "integration", "performance"]

    # ãƒ†ã‚¹ãƒˆãƒ©ãƒ³ãƒŠãƒ¼ã‚’ä½œæˆã—ã¦å®Ÿè¡Œ
    runner = TestRunner(args.output)
    runner.run_tests(test_types, include_coverage=args.coverage)


if __name__ == "__main__":
    main()
