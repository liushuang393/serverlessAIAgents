{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbe02d59",
   "metadata": {},
   "source": [
    "### ä¿®æ­£ç‰ˆ LLMçµ±åˆç®¡ç† (`LLMProvider.py`)\n",
    "è¤‡æ•°ã®LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆOpenAIã€Anthropicã€Googleã€HuggingFaceï¼‰ã‚’çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§ç®¡ç†ã€‚\n",
    "\n",
    "**ä¿®æ­£ã•ã‚ŒãŸå•é¡Œ:**\n",
    "- OpenAIåˆæœŸåŒ–ã®proxiesãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¨ãƒ©ãƒ¼ã‚’è§£æ±º\n",
    "- HuggingFace vLLMãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®äº’æ›æ€§å•é¡Œã‚’è§£æ±º\n",
    "- ãƒ‡ãƒã‚¤ã‚¹ä¸ä¸€è‡´ã‚¨ãƒ©ãƒ¼ã‚’è§£æ±º\n",
    "- ã‚ˆã‚Šå …ç‰¢ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’è¿½åŠ \n",
    "\n",
    "**ä¸»è¦æ©Ÿèƒ½:**\n",
    "- çµ±ä¸€ã•ã‚ŒãŸAPIå‘¼ã³å‡ºã—ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹\n",
    "- ãƒ¡ãƒ¢ãƒªå†…ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ï¼ˆ@lru_cacheä½¿ç”¨ï¼‰\n",
    "- ãƒãƒ£ãƒƒãƒˆå±¥æ­´å‡¦ç†æ©Ÿèƒ½\n",
    "- è©³ç´°ãƒ­ã‚°æ©Ÿèƒ½\n",
    "- å†è©¦è¡Œå¯¾å¿œ\n",
    "- è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "setup_cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… generate_simple_transformers ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ\n",
      "âœ… LLMProvideré–¢æ•°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ \n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# ã‚«ãƒ¼ãƒãƒ«ã‚’å†èµ·å‹•ã™ã‚‹ã‹ã€ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã§ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å¼·åˆ¶ãƒªãƒ­ãƒ¼ãƒ‰\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# # utilsãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ãƒªãƒ­ãƒ¼ãƒ‰\n",
    "if 'utils' in sys.modules:\n",
    "    importlib.reload(sys.modules['utils'])\n",
    "if 'utils.LLMProvider' in sys.modules:\n",
    "    importlib.reload(sys.modules['utils.LLMProvider'])\n",
    "\n",
    "# å†åº¦ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è©¦è¡Œ\n",
    "from utils import generate_simple_transformers\n",
    "print(\"âœ… generate_simple_transformers ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ\")\n",
    "# utilsãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰é–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "from utils import generate, generate_anthropic, generate_google, generate_huggingface, generate_simple_transformers\n",
    "\n",
    "print(\"âœ… LLMProvideré–¢æ•°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "openai_test",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.LLMProvider:=== LLMå‘¼ã³å‡ºã—é–‹å§‹ ===\n",
      "INFO:utils.LLMProvider:ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: ã“ã‚“ã«ã¡ã¯ã€ä¸–ç•Œã«ã¤ã„ã¦æ•™ãˆã¦\n",
      "INFO:utils.LLMProvider:ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: 15 æ–‡å­—\n",
      "INFO:utils.LLMProvider:base_url: http://localhost:11434/v1\n",
      "INFO:utils.LLMProvider:model_name: gemma3n:latest\n",
      "INFO:utils.LLMProvider:api_key: ***n2kA\n",
      "INFO:utils.LLMProvider:use_cache: False\n",
      "INFO:utils.LLMProvider:cur_retry: 0\n",
      "INFO:utils.LLMProvider:ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã›ãšLLMå‘¼ã³å‡ºã—\n",
      "INFO:utils.LLMProvider:ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸå‘¼ã³å‡ºã—: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ=ã“ã‚“ã«ã¡ã¯ã€ä¸–ç•Œã«ã¤ã„ã¦æ•™ãˆã¦...\n",
      "INFO:utils.LLMProvider:OpenAIåˆæœŸåŒ–æˆåŠŸ\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:utils.LLMProvider:å¿œç­”: ã“ã‚“ã«ã¡ã¯ï¼ä¸–ç•Œã«ã¤ã„ã¦ã§ã™ã­ã€‚åºƒå¤§ã§è¤‡é›‘ãªãƒ†ãƒ¼ãƒãªã®ã§ã€ã©ã“ã‹ã‚‰ãŠä¼ãˆã—ã‚ˆã†ã‹è¿·ã„ã¾ã™ãŒã€ã„ãã¤ã‹ãƒã‚¤ãƒ³ãƒˆã«åˆ†ã‘ã¦ã”ç´¹ä»‹ã—ã¾ã™ã­ã€‚\n",
      "\n",
      "**1. åœ°ç†ã¨è‡ªç„¶**\n",
      "\n",
      "*   **åœ°çƒã®å½¢:** åœ°çƒã¯å®Œå…¨ãªçƒä½“ã§ã¯ãªãã€èµ¤é“ä»˜è¿‘ãŒè†¨ã‚‰ã‚“ã å›è»¢æ¥•å††ä½“ã§ã™ã€‚\n",
      "*   **å¤§é™¸ã¨æµ·æ´‹:** åœ°çƒã®è¡¨é¢ã¯ã€ã‚¢ã‚¸ã‚¢ã€ã‚¢ãƒ•ãƒªã‚«ã€ãƒ¨ãƒ¼ãƒ­ãƒƒãƒ‘ã€åŒ—ã‚¢ãƒ¡ãƒªã‚«ã€å—ã‚¢ãƒ¡ãƒªã‚«ã€ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢ã€å—æ¥µã¨ã„ã†7ã¤ã®å¤§é™¸ã¨ã€å¤ªå¹³æ´‹ã€...\n",
      "INFO:utils.LLMProvider:å¿œç­”é•·: 1438 æ–‡å­—\n",
      "INFO:utils.LLMProvider:=== LLMå‘¼ã³å‡ºã—å®Œäº† ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenAI APIæˆåŠŸ: ã“ã‚“ã«ã¡ã¯ï¼ä¸–ç•Œã«ã¤ã„ã¦ã§ã™ã­ã€‚åºƒå¤§ã§è¤‡é›‘ãªãƒ†ãƒ¼ãƒãªã®ã§ã€ã©ã“ã‹ã‚‰ãŠä¼ãˆã—ã‚ˆã†ã‹è¿·ã„ã¾ã™ãŒã€ã„ãã¤ã‹ãƒã‚¤ãƒ³ãƒˆã«åˆ†ã‘ã¦ã”ç´¹ä»‹ã—ã¾ã™ã­ã€‚\n",
      "\n",
      "**1. åœ°ç†ã¨è‡ªç„¶**\n",
      "\n",
      "*   **åœ°çƒã®å½¢:** åœ°çƒã¯å®Œ...\n"
     ]
    }
   ],
   "source": [
    "# OpenAI APIä½¿ç”¨ï¼ˆä¿®æ­£ç‰ˆ - HTTPãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯¾å¿œï¼‰\n",
    "try:\n",
    "    response = generate(\"ã“ã‚“ã«ã¡ã¯ã€ä¸–ç•Œã«ã¤ã„ã¦æ•™ãˆã¦\", use_cache=False)\n",
    "    print(f\"âœ… OpenAI APIæˆåŠŸ: {response[:100]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ OpenAI APIã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    print(\"ğŸ’¡ ãƒ­ãƒ¼ã‚«ãƒ«ã®Ollamaã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anthropic_test",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.LLMProvider:=== Anthropic LLMå‘¼ã³å‡ºã—é–‹å§‹ ===\n",
      "INFO:utils.LLMProvider:ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã«ã¤ã„ã¦èª¬æ˜ã—ã¦\n",
      "INFO:utils.LLMProvider:ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: 19 æ–‡å­—\n",
      "INFO:utils.LLMProvider:model_name: claude-3-7-sonnet-20250219\n",
      "INFO:utils.LLMProvider:api_key: ***oAAA\n",
      "INFO:utils.LLMProvider:use_cache: True\n",
      "INFO:utils.LLMProvider:cur_retry: 0\n",
      "INFO:utils.LLMProvider:Anthropic APIå‘¼ã³å‡ºã—ä¸­: claude-3-7-sonnet-20250219\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:utils.LLMProvider:å¿œç­”: # é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®èª¬æ˜\n",
      "\n",
      "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¨ã¯ã€é‡å­åŠ›å­¦ã®åŸç†ã‚’åˆ©ç”¨ã—ãŸæ–°ã—ã„è¨ˆç®—ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã§ã™ã€‚å¾“æ¥ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ãŒã€Œãƒ“ãƒƒãƒˆã€ï¼ˆ0ã‹1ï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã®ã«å¯¾ã—ã€é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã¯ã€Œé‡å­ãƒ“ãƒƒãƒˆã€ã¾ãŸã¯ã€Œã‚­ãƒ¥ãƒ¼ãƒ“ãƒƒãƒˆã€ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n",
      "\n",
      "## é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®åŸºæœ¬æ¦‚å¿µ\n",
      "\n",
      "1. **é‡å­ãƒ“ãƒƒãƒˆï¼ˆã‚­ãƒ¥ãƒ¼ãƒ“ãƒƒãƒˆï¼‰**: 0ã¨1ã®çŠ¶æ…‹ãŒåŒæ™‚ã«å­˜åœ¨ã§ãã‚‹ã€Œé‡ã­åˆã‚ã›ã€ã®çŠ¶æ…‹ã‚’ã¨ã‚‹ã“ã¨ãŒã§...\n",
      "INFO:utils.LLMProvider:å¿œç­”é•·: 608 æ–‡å­—\n",
      "INFO:utils.LLMProvider:=== Anthropic LLMå‘¼ã³å‡ºã—å®Œäº† ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Anthropic APIæˆåŠŸ: # é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®èª¬æ˜\n",
      "\n",
      "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¨ã¯ã€é‡å­åŠ›å­¦ã®åŸç†ã‚’åˆ©ç”¨ã—ãŸæ–°ã—ã„è¨ˆç®—ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã§ã™ã€‚å¾“æ¥ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ãŒã€Œãƒ“ãƒƒãƒˆã€ï¼ˆ0ã‹1ï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã®ã«å¯¾ã—ã€é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã¯ã€Œé‡...\n"
     ]
    }
   ],
   "source": [
    "# Anthropic APIä½¿ç”¨\n",
    "#os.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-key\"\n",
    "\n",
    "try:\n",
    "    response = generate_anthropic(\"é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã«ã¤ã„ã¦èª¬æ˜ã—ã¦\")\n",
    "    print(f\"âœ… Anthropic APIæˆåŠŸ: {response[:100]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Anthropic APIã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    print(\"ğŸ’¡ ANTHROPIC_API_KEYç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "google_test",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.LLMProvider:=== Google LLMå‘¼ã³å‡ºã—é–‹å§‹ ===\n",
      "INFO:utils.LLMProvider:ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: AIã®æœªæ¥ã«ã¤ã„ã¦æ•™ãˆã¦\n",
      "INFO:utils.LLMProvider:ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: 12 æ–‡å­—\n",
      "INFO:utils.LLMProvider:model_name: gemini-2.0-flash-001\n",
      "INFO:utils.LLMProvider:api_key: ***_0Kk\n",
      "INFO:utils.LLMProvider:use_cache: True\n",
      "INFO:utils.LLMProvider:cur_retry: 0\n",
      "INFO:utils.LLMProvider:ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã—ã¦Google LLMå‘¼ã³å‡ºã—\n",
      "INFO:utils.LLMProvider:Google ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸå‘¼ã³å‡ºã—: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ=AIã®æœªæ¥ã«ã¤ã„ã¦æ•™ãˆã¦...\n",
      "/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/pydantic/_internal/_fields.py:184: UserWarning: Field name \"name\" shadows an attribute in parent \"Operation\"; \n",
      "  warnings.warn(\n",
      "/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/pydantic/_internal/_fields.py:184: UserWarning: Field name \"metadata\" shadows an attribute in parent \"Operation\"; \n",
      "  warnings.warn(\n",
      "/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/pydantic/_internal/_fields.py:184: UserWarning: Field name \"done\" shadows an attribute in parent \"Operation\"; \n",
      "  warnings.warn(\n",
      "/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/pydantic/_internal/_fields.py:184: UserWarning: Field name \"error\" shadows an attribute in parent \"Operation\"; \n",
      "  warnings.warn(\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-001:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:utils.LLMProvider:å¿œç­”: AIã®æœªæ¥ã¯éå¸¸ã«æ˜ã‚‹ãã€æ§˜ã€…ãªåˆ†é‡ã§å¤§ããªå¤‰é©ã‚’ã‚‚ãŸã‚‰ã™ã¨æœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ã€‚ãŸã ã—ã€åŒæ™‚ã«èª²é¡Œã‚‚å­˜åœ¨ã™ã‚‹ãŸã‚ã€ãã®ä¸¡é¢ã‚’ç†è§£ã—ã¦ãŠãã“ã¨ãŒé‡è¦ã§ã™ã€‚\n",
      "\n",
      "**AIã®æœªæ¥ã®å¯èƒ½æ€§**\n",
      "\n",
      "*   **ç”£æ¥­ãƒ»çµŒæ¸ˆã®å¤‰é©:**\n",
      "    *   è‡ªå‹•åŒ–ã®é€²åŒ–: è£½é€ ã€ç‰©æµã€ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒ¼ãƒ“ã‚¹ãªã©ã€æ§˜ã€…ãªæ¥­å‹™ãŒè‡ªå‹•åŒ–ã•ã‚Œã€ç”Ÿç”£æ€§å‘ä¸Šã‚„ã‚³ã‚¹ãƒˆå‰Šæ¸›ã«ã¤ãªãŒã‚Šã¾ã™ã€‚\n",
      "    *   æ–°ãŸãªç”£æ¥­ã®å‰µå‡º: AIã‚’æ´»ç”¨ã—...\n",
      "INFO:utils.LLMProvider:å¿œç­”é•·: 1965 æ–‡å­—\n",
      "INFO:utils.LLMProvider:=== Google LLMå‘¼ã³å‡ºã—å®Œäº† ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Google APIæˆåŠŸ: AIã®æœªæ¥ã¯éå¸¸ã«æ˜ã‚‹ãã€æ§˜ã€…ãªåˆ†é‡ã§å¤§ããªå¤‰é©ã‚’ã‚‚ãŸã‚‰ã™ã¨æœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ã€‚ãŸã ã—ã€åŒæ™‚ã«èª²é¡Œã‚‚å­˜åœ¨ã™ã‚‹ãŸã‚ã€ãã®ä¸¡é¢ã‚’ç†è§£ã—ã¦ãŠãã“ã¨ãŒé‡è¦ã§ã™ã€‚\n",
      "\n",
      "**AIã®æœªæ¥ã®å¯èƒ½æ€§**\n",
      "\n",
      "*   **ç”£æ¥­...\n"
     ]
    }
   ],
   "source": [
    "# Google APIä½¿ç”¨\n",
    "#os.environ[\"GOOGLE_API_KEY\"] = \"your-google-key\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyClYSVeRKdYwlan_hNiB3f0M28tb7y_0Kk\"\n",
    "try:\n",
    "    response = generate_google(\"AIã®æœªæ¥ã«ã¤ã„ã¦æ•™ãˆã¦\")\n",
    "    print(f\"âœ… Google APIæˆåŠŸ: {response[:100]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Google APIã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    print(\"ğŸ’¡ GOOGLE_API_KEYç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "huggingface_test",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.LLMProvider:=== HuggingFace LLMå‘¼ã³å‡ºã—é–‹å§‹ ===\n",
      "INFO:utils.LLMProvider:ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: User: æ©Ÿæ¢°å­¦ç¿’ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ\n",
      "Bot:\n",
      "INFO:utils.LLMProvider:ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: 22 æ–‡å­—\n",
      "INFO:utils.LLMProvider:model_name: microsoft/DialoGPT-small\n",
      "INFO:utils.LLMProvider:use_cache: True\n",
      "INFO:utils.LLMProvider:cur_retry: 0\n",
      "INFO:utils.LLMProvider:ãƒ¢ãƒ‡ãƒ«è¨­å®šç¢ºèªä¸­: microsoft/DialoGPT-small\n",
      "INFO:utils.LLMProvider:Model: microsoft/DialoGPT-small\n",
      "INFO:utils.LLMProvider:  max_position_embeddings: 1024\n",
      "INFO:utils.LLMProvider:  model_max_length: 20\n",
      "INFO:utils.LLMProvider:  vocab_size: 50257\n",
      "INFO:utils.LLMProvider:max_model_len: 1024\n",
      "INFO:utils.LLMProvider:ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã—ã¦HuggingFace LLMå‘¼ã³å‡ºã—\n",
      "INFO:utils.LLMProvider:HuggingFace ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸå‘¼ã³å‡ºã—: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ=User: æ©Ÿæ¢°å­¦ç¿’ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ\n",
      "Bot:...\n",
      "INFO:utils.LLMProvider:HuggingFace Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "INFO:utils.LLMProvider:ãƒ¢ãƒ‡ãƒ« microsoft/DialoGPT-small ã®åˆæœŸåŒ–æˆåŠŸ\n",
      "INFO:utils.LLMProvider:HuggingFaceç”Ÿæˆå®Œäº†: 13æ–‡å­—\n",
      "INFO:utils.LLMProvider:å¿œç­”: u Karmaliels\n",
      "INFO:utils.LLMProvider:å¿œç­”é•·: 12 æ–‡å­—\n",
      "INFO:utils.LLMProvider:=== HuggingFace LLMå‘¼ã³å‡ºã—å®Œäº† ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… HuggingFace APIæˆåŠŸ: u Karmaliels...\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace APIä½¿ç”¨ï¼ˆä¿®æ­£ç‰ˆ - vLLMå•é¡Œè§£æ±ºæ¸ˆã¿ï¼‰\n",
    "try:\n",
    "    response = generate_huggingface(\"æ©Ÿæ¢°å­¦ç¿’ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ\")\n",
    "    print(f\"âœ… HuggingFace APIæˆåŠŸ: {response[:100]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ HuggingFace APIã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    print(\"ğŸ’¡ transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "simple_transformers_test",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.LLMProvider:=== Simple Transformers LLMå‘¼ã³å‡ºã—é–‹å§‹ ===\n",
      "INFO:utils.LLMProvider:ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: æ©Ÿæ¢°å­¦ç¿’ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ\n",
      "INFO:utils.LLMProvider:ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: 11 æ–‡å­—\n",
      "INFO:utils.LLMProvider:model_name: distilgpt2\n",
      "INFO:utils.LLMProvider:use_cache: True\n",
      "INFO:utils.LLMProvider:cur_retry: 0\n",
      "INFO:utils.LLMProvider:ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã—ã¦Simple Transformers LLMå‘¼ã³å‡ºã—\n",
      "INFO:utils.LLMProvider:[HF] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…ˆé ­: æ©Ÿæ¢°å­¦ç¿’ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ... | model=distilgpt2\n",
      "INFO:utils.LLMProvider:[HF] ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ cuda:0 ã§åˆæœŸåŒ–...\n",
      "ERROR:utils.LLMProvider:[HF] ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "WARNING:utils.LLMProvider:[HF] GPU åˆæœŸåŒ–å¤±æ•— â†’ CPU ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/d/pythonPJ/serverlessAIAgents/products/template_project/utils/LLMProvider.py\", line 632, in cached_call_hf\n",
      "    gen_pipe = _load_pipeline(device_str)\n",
      "RuntimeError: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "INFO:utils.LLMProvider:[HF] ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ cpu ã§åˆæœŸåŒ–...\n",
      "Device set to use cpu\n",
      "INFO:utils.LLMProvider:å¿œç­”: ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¿½\n",
      "INFO:utils.LLMProvider:å¿œç­”é•·: 17 æ–‡å­—\n",
      "INFO:utils.LLMProvider:=== Simple Transformers LLMå‘¼ã³å‡ºã—å®Œäº† ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ã‚·ãƒ³ãƒ—ãƒ«ãªTransformersãƒ†ã‚¹ãƒˆæˆåŠŸ: ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¿½\n"
     ]
    }
   ],
   "source": [
    "# ã‚·ãƒ³ãƒ—ãƒ«ãªTransformersãƒ†ã‚¹ãƒˆï¼ˆè»½é‡ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ï¼‰\n",
    "try:\n",
    "    # from transformers import pipeline\n",
    "    \n",
    "    # print(\"ğŸ”„ è»½é‡ãƒ¢ãƒ‡ãƒ«ï¼ˆdistilgpt2ï¼‰ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
    "    # generator = pipeline('text-generation', \n",
    "    #                    model='distilgpt2',\n",
    "    #                    max_length=50,\n",
    "    #                    num_return_sequences=1)\n",
    "    \n",
    "    result = generate_simple_transformers(\"æ©Ÿæ¢°å­¦ç¿’ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ\")\n",
    "    print(f\"âœ… ã‚·ãƒ³ãƒ—ãƒ«ãªTransformersãƒ†ã‚¹ãƒˆæˆåŠŸ: {result}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ã‚·ãƒ³ãƒ—ãƒ«ãªTransformersãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e60325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers >= 4.53\n",
    "from functools import lru_cache\n",
    "from transformers import (pipeline, AutoTokenizer,\n",
    "                          AutoModelForCausalLM)\n",
    "import torch, logging, os\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "MODEL_NAME = os.getenv(\"HF_MODEL\", \"rinna/japanese-gpt2-small\")\n",
    "\n",
    "@lru_cache(maxsize=1) \n",
    "def get_pipe():\n",
    "    tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    if tok.pad_token_id is None:\n",
    "        tok.pad_token_id = tok.eos_token_id\n",
    "\n",
    "    try:  # GPU ä¼˜å…ˆ\n",
    "        logger.info(\"try GPU with device_map='auto'\")\n",
    "        return pipeline(\"text-generation\",\n",
    "                        model=AutoModelForCausalLM.from_pretrained(\n",
    "                            MODEL_NAME,\n",
    "                            torch_dtype=torch.float16,\n",
    "                        ),\n",
    "                        tokenizer=tok,\n",
    "                        device_map=\"auto\",           # å¤š GPU è‡ªåŠ¨åˆ‡ç‰‡\n",
    "                        max_new_tokens=512)\n",
    "    except RuntimeError as e:\n",
    "        logger.warning(f\"GPU fail: {e}; fallback CPU\")\n",
    "        return pipeline(\"text-generation\",\n",
    "                        model=MODEL_NAME,\n",
    "                        tokenizer=tok,\n",
    "                        device=-1,                  # CPU\n",
    "                        max_new_tokens=256)\n",
    "\n",
    "def generate(text: str):\n",
    "    pipe = get_pipe()\n",
    "    try:\n",
    "        out = pipe(text, truncation=True)[0][\"generated_text\"]\n",
    "        return out[len(text):].strip()\n",
    "    except Exception as ex:\n",
    "        logger.error(f\"Inference error: {ex}\")\n",
    "        return \"ã€ç”Ÿæˆå¤±è´¥ã€‘\"\n",
    "\n",
    "# æœåŠ¡åŒ–ç¤ºä¾‹ï¼ˆFastAPIï¼‰\n",
    "# from fastapi import FastAPI\n",
    "# app = FastAPI()\n",
    "# @app.post(\"/generate\")\n",
    "# async def gen(req: dict): return {\"answer\": generate(req[\"prompt\"])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:try GPU with device_map='auto'\n",
      "Device set to use cuda:0\n",
      "WARNING:__main__:GPU fail: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "; fallback CPU\n",
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ã¯ã„ã€ä¸–ç•Œã«ã¤ã„ã¦ã§ã™ã€‚ ç§ã¯ã€ç¾åœ¨ã€ã‚¢ãƒ¡ãƒªã‚«ã®å¤§å­¦ã«ç•™å­¦ä¸­ã§ã€ãƒ‹ãƒ¥ãƒ¼ãƒ¨ãƒ¼ã‚¯ã«ä½ã‚“ã§ã„ã¾ã™ã€‚ ç§ãŒç•™å­¦ã—ãŸå¤§å­¦ã¯ã€ä¸–ç•ŒçµŒæ¸ˆå¤§å­¦ã§ã™ã€‚ ç§ã¯ã€å¤§å­¦ã§çµŒæ¸ˆå­¦ã‚’å­¦ã³ã€ä¿®å£«å·ã‚’å–å¾—ã—ã¦ã„ã¾ã—ãŸã€‚ å¤§å­¦ã‚’å’æ¥­ã™ã‚‹ã¨ã€ã‚¢ãƒ¡ãƒªã‚«ã®å¤§å­¦é™¢ã«é€²å­¦ã—ã€ä¿®å£«å·ã‚’å–å¾—ã—ã¾ã—ãŸã€‚ ã‚¢ãƒ¡ãƒªã‚«ã«ç•™å­¦ã™ã‚‹éš›ã«ã€ç§ã¯ã€ã‚¢ãƒ¡ãƒªã‚«ã§çµŒæ¸ˆå­¦ã‚’å­¦ã‚“ã ã“ã¨ã‚’å¾Œæ‚”ã—ã¦ã„ã¾ã™ã€‚ãªãœãªã‚‰ã€å¤§å­¦ã‚’å’æ¥­ã™ã‚‹ã¨ã€ã‚¢ãƒ¡ãƒªã‚«ã®å¤§å­¦é™¢ã«é€²å­¦ã—ã€ä¿®å£«å·ã‚’å–å¾—ã—ã¦ã„ã¾ã—ãŸã€‚ ç§ã¯ã€å¤§å­¦ã§çµŒæ¸ˆå­¦ã‚’å­¦ã³ã€ä¿®å£«å·ã‚’å–å¾—ã—ã¾ã—ãŸã€‚ ã‚¢ãƒ¡ãƒªã‚«ã«ç•™å­¦ã™ã‚‹éš›ã«ã€ç§ã¯ã€ã‚¢ãƒ¡ãƒªã‚«ã§çµŒæ¸ˆå­¦ã‚’å­¦ã‚“ã ã“ã¨ã‚’å¾Œæ‚”ã—ã¦ã„ã¾ã™ã€‚ãªãœãªã‚‰ã€å¤§å­¦ã‚’å’æ¥­ã™ã‚‹ã¨ã€ã‚¢ãƒ¡ãƒªã‚«ã®å¤§å­¦é™¢ã«é€²å­¦ã—ã€ä¿®å£«å·ã‚’å–å¾—ã—ã¾ã—ãŸã€‚ æ—¥æœ¬ã«ã„ã‚‹æ™‚ã«ã¯ã€æ—¥æœ¬ã®å¤§å­¦é™¢ã«è¡Œãã€ä¿®å£«å·ã‚’å–å¾—ã—ã¦ã„ã¾ã—ãŸã€‚ ç§ã¯ã€å¤§å­¦ã§çµŒæ¸ˆå­¦ã‚’å­¦ã³ã€ä¿®å£«å·ã‚’å–å¾—ã—ã¾ã—ãŸã€‚ æ—¥æœ¬ã®å¤§å­¦é™¢ã«ç•™å­¦ã™ã‚‹éš›ã«ã€ç§ã¯ã€å¤§å­¦ã§çµŒæ¸ˆå­¦ã‚’å­¦ã‚“ã ã“ã¨ã‚’å¾Œæ‚”ã—ã¦ã„ã¾ã™ã€‚ãªãœãªã‚‰ã€å¤§å­¦ã‚’å’æ¥­ã™ã‚‹ã¨ã€ã‚¢ãƒ¡ãƒªã‚«ã®å¤§å­¦é™¢ã«é€²å­¦ã—ã€ä¿®å£«å·ã‚’å–å¾—ã—ã¦ã„ã¾ã—ãŸã€‚ ã‚¢ãƒ¡ãƒªã‚«ã®å¤§å­¦é™¢ã«ç•™å­¦ã™ã‚‹éš›ã«ã€ç§ã¯ã€æ—¥æœ¬ã§çµŒæ¸ˆå­¦ã‚’å­¦ã‚“ã ã“ã¨ã‚’å¾Œæ‚”ã—ã¦ã„ã¾ã™ã€‚ãªãœãªã‚‰ã€å¤§å­¦ã‚’å’æ¥­ã™ã‚‹ã¨ã€ã‚¢ãƒ¡ãƒªã‚«ã®å¤§å­¦é™¢ã«é€²å­¦ã—ã€ä¿®å£«å·ã‚’å–å¾—ã—ã¦ã„ã¾ã—ãŸã€‚ æ—¥æœ¬ã«ã„ã‚‹æ™‚ã«ã¯ã€æ—¥æœ¬ã®å¤§å­¦é™¢ã«è¡Œãã€ä¿®å£«å·ã‚’å–å¾—ã—ã¦ã„ã¾ã—ãŸã€‚ ç§ã¯ã€å¤§å­¦ã§çµŒæ¸ˆå­¦ã‚’å­¦ã³ã€ä¿®å£«å·ã‚’å–å¾—ã—ã¾ã—ãŸã€‚ ç§ã¯ã€å¤§å­¦ã§çµŒæ¸ˆå­¦ã‚’å­¦ã³ã€ä¿®å£«'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"ã“ã‚“ã«ã¡ã¯ã€ä¸–ç•Œã«ã¤ã„ã¦æ•™ãˆã¦\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "troubleshooting",
   "metadata": {},
   "source": [
    "### ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\n",
    "\n",
    "**ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºç­–:**\n",
    "\n",
    "1. **OpenAI APIã‚¨ãƒ©ãƒ¼ (`proxies` parameter)**\n",
    "   - ä¿®æ­£æ¸ˆã¿: HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã«ã‚ˆã‚‹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ã‚’è¿½åŠ \n",
    "   - ãƒ­ãƒ¼ã‚«ãƒ«Ollamaã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•ã‚’ç¢ºèª\n",
    "\n",
    "2. **HuggingFace vLLMã‚¨ãƒ©ãƒ¼ (`SchemaError`)**\n",
    "   - ä¿®æ­£æ¸ˆã¿: ç›´æ¥Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨\n",
    "   - ãƒ‡ãƒã‚¤ã‚¹è¨­å®šã‚’çµ±ä¸€ï¼ˆCUDA/CPUï¼‰\n",
    "\n",
    "3. **ãƒ‡ãƒã‚¤ã‚¹ä¸ä¸€è‡´ã‚¨ãƒ©ãƒ¼**\n",
    "   - ä¿®æ­£æ¸ˆã¿: ãƒ†ãƒ³ã‚½ãƒ«ã®ãƒ‡ãƒã‚¤ã‚¹é…ç½®ã‚’çµ±ä¸€\n",
    "   - attention_maskã‚’æ˜ç¤ºçš„ã«è¨­å®š\n",
    "\n",
    "4. **ç’°å¢ƒå¤‰æ•°è¨­å®š**\n",
    "   ```bash\n",
    "   export OPENAI_API_KEY=\"your-api-key\"\n",
    "   export ANTHROPIC_API_KEY=\"your-api-key\"\n",
    "   export GOOGLE_API_KEY=\"your-api-key\"\n",
    "   ```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_ragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
