#!/usr/bin/env python3
"""
åŠŸèƒ½æ‰§è¡Œæµ‹è¯•è„šæœ¬

è¿™ä¸ªè„šæœ¬æµ‹è¯•å„ä¸ªå·¥å…·æ¨¡å—çš„å®é™…åŠŸèƒ½æ‰§è¡Œï¼Œè€Œä¸ä»…ä»…æ˜¯å¯¼å…¥ã€‚
"""

import sys
import os
import time
import tempfile
from pathlib import Path

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

def test_llm_functionality():
    """æµ‹è¯•LLMåŠŸèƒ½æ‰§è¡Œï¼ˆä½¿ç”¨ç°æœ‰çš„LLMProviderï¼‰"""
    print("\n" + "="*50)
    print("ğŸ¤– LLMåŠŸèƒ½æ‰§è¡Œæµ‹è¯•")
    print("="*50)

    try:
        from LLMProvider import generate, generate_anthropic, generate_google

        # æµ‹è¯•1: åŸºæœ¬æ–‡æœ¬ç”Ÿæˆ
        print("æµ‹è¯•1: åŸºæœ¬æ–‡æœ¬ç”Ÿæˆ")
        response = generate("ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿè¯·ç”¨ä¸€å¥è¯å›ç­”ã€‚")
        print(f"âœ… ç”ŸæˆæˆåŠŸ: {response[:100]}...")
        assert len(response) > 10, "å“åº”å¤ªçŸ­"

        # æµ‹è¯•2: Anthropicç”Ÿæˆ
        print("æµ‹è¯•2: Anthropicç”Ÿæˆ")
        try:
            anthropic_response = generate_anthropic("ç®€å•ä»‹ç»æœºå™¨å­¦ä¹ ")
            print(f"âœ… Anthropicç”Ÿæˆ: {anthropic_response[:100]}...")
            assert len(anthropic_response) > 5, "Anthropicå“åº”å¤ªçŸ­"
        except Exception as e:
            print(f"âš ï¸  Anthropicæµ‹è¯•è·³è¿‡: {e}")

        # æµ‹è¯•3: Googleç”Ÿæˆ
        print("æµ‹è¯•3: Googleç”Ÿæˆ")
        try:
            google_response = generate_google("ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ")
            print(f"âœ… Googleç”Ÿæˆ: {google_response[:100]}...")
            assert len(google_response) > 5, "Googleå“åº”å¤ªçŸ­"
        except Exception as e:
            print(f"âš ï¸  Googleæµ‹è¯•è·³è¿‡: {e}")

        print("ğŸ‰ LLMåŠŸèƒ½æµ‹è¯•å…¨éƒ¨é€šè¿‡!")
        return True

    except Exception as e:
        print(f"âŒ LLMåŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_text_processing_functionality():
    """æµ‹è¯•æ–‡æœ¬å¤„ç†åŠŸèƒ½æ‰§è¡Œ"""
    print("\n" + "="*50)
    print("ğŸ“ æ–‡æœ¬å¤„ç†åŠŸèƒ½æ‰§è¡Œæµ‹è¯•")
    print("="*50)
    
    try:
        from text_utils import chunk_text, preprocess_text, merge_chunks, TextChunk
        
        # æµ‹è¯•1: æ–‡æœ¬é¢„å¤„ç†
        print("æµ‹è¯•1: æ–‡æœ¬é¢„å¤„ç†")
        dirty_text = "<p>è¿™æ˜¯ä¸€ä¸ª   åŒ…å«HTMLæ ‡ç­¾çš„   æ–‡æœ¬</p>"
        clean_text = preprocess_text(dirty_text)
        print(f"âœ… é¢„å¤„ç†: '{dirty_text}' -> '{clean_text}'")
        assert "<p>" not in clean_text, "HTMLæ ‡ç­¾æœªæ¸…ç†"
        assert "  " not in clean_text, "å¤šä½™ç©ºæ ¼æœªæ¸…ç†"
        
        # æµ‹è¯•2: å›ºå®šå¤§å°åˆ†å—
        print("æµ‹è¯•2: å›ºå®šå¤§å°åˆ†å—")
        long_text = "è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„æ–‡æœ¬ã€‚" * 20  # 200å­—ç¬¦å·¦å³
        chunks = chunk_text(long_text, chunker_type="fixed", chunk_size=50, overlap=10)
        print(f"âœ… åˆ†å—æˆåŠŸ: {len(long_text)}å­—ç¬¦ -> {len(chunks)}ä¸ªå—")
        assert len(chunks) > 1, "åº”è¯¥ç”Ÿæˆå¤šä¸ªå—"
        assert all(isinstance(chunk, TextChunk) for chunk in chunks), "å—ç±»å‹é”™è¯¯"
        
        # æµ‹è¯•3: æ–‡æœ¬å—åˆå¹¶
        print("æµ‹è¯•3: æ–‡æœ¬å—åˆå¹¶")
        merged_text = merge_chunks(chunks, separator=" | ")
        print(f"âœ… åˆå¹¶æˆåŠŸ: {len(merged_text)}å­—ç¬¦")
        assert " | " in merged_text, "åˆ†éš”ç¬¦æœªæ­£ç¡®æ·»åŠ "
        
        print("ğŸ‰ æ–‡æœ¬å¤„ç†åŠŸèƒ½æµ‹è¯•å…¨éƒ¨é€šè¿‡!")
        return True
        
    except Exception as e:
        print(f"âŒ æ–‡æœ¬å¤„ç†åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_vector_functionality():
    """æµ‹è¯•å‘é‡æ•°æ®åº“åŠŸèƒ½æ‰§è¡Œ"""
    print("\n" + "="*50)
    print("ğŸ” å‘é‡æ•°æ®åº“åŠŸèƒ½æ‰§è¡Œæµ‹è¯•")
    print("="*50)
    
    try:
        from vector_utils import VectorDocument, get_vector_db_manager
        import numpy as np
        
        # æµ‹è¯•1: å‘é‡æ–‡æ¡£åˆ›å»º
        print("æµ‹è¯•1: å‘é‡æ–‡æ¡£åˆ›å»º")
        doc1 = VectorDocument(
            id="doc1",
            vector=[0.1, 0.2, 0.3, 0.4, 0.5],
            metadata={"title": "æµ‹è¯•æ–‡æ¡£1", "category": "test"},
            text="è¿™æ˜¯ç¬¬ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£"
        )
        print(f"âœ… æ–‡æ¡£åˆ›å»º: ID={doc1.id}, å‘é‡ç»´åº¦={len(doc1.vector)}")
        assert doc1.id == "doc1", "æ–‡æ¡£IDé”™è¯¯"
        assert len(doc1.vector) == 5, "å‘é‡ç»´åº¦é”™è¯¯"
        assert isinstance(doc1.vector, np.ndarray), "å‘é‡ç±»å‹é”™è¯¯"
        
        # æµ‹è¯•2: å¤šä¸ªæ–‡æ¡£åˆ›å»º
        print("æµ‹è¯•2: å¤šä¸ªæ–‡æ¡£åˆ›å»º")
        docs = []
        for i in range(3):
            doc = VectorDocument(
                id=f"doc{i+2}",
                vector=np.random.rand(5).tolist(),
                metadata={"title": f"æ–‡æ¡£{i+2}", "index": i+2},
                text=f"è¿™æ˜¯ç¬¬{i+2}ä¸ªæµ‹è¯•æ–‡æ¡£"
            )
            docs.append(doc)
        print(f"âœ… æ‰¹é‡åˆ›å»º: {len(docs)}ä¸ªæ–‡æ¡£")
        assert len(docs) == 3, "æ–‡æ¡£æ•°é‡é”™è¯¯"
        
        # æµ‹è¯•3: ç®¡ç†å™¨åŠŸèƒ½
        print("æµ‹è¯•3: ç®¡ç†å™¨åŠŸèƒ½")
        manager = get_vector_db_manager()
        print(f"âœ… ç®¡ç†å™¨: é»˜è®¤æä¾›å•†={manager.default_provider}")
        assert manager.default_provider == "faiss", "é»˜è®¤æä¾›å•†é”™è¯¯"
        
        print("ğŸ‰ å‘é‡æ•°æ®åº“åŠŸèƒ½æµ‹è¯•å…¨éƒ¨é€šè¿‡!")
        return True
        
    except Exception as e:
        print(f"âŒ å‘é‡æ•°æ®åº“åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_websearch_functionality():
    """æµ‹è¯•Webæœç´¢åŠŸèƒ½æ‰§è¡Œ"""
    print("\n" + "="*50)
    print("ğŸŒ Webæœç´¢åŠŸèƒ½æ‰§è¡Œæµ‹è¯•")
    print("="*50)
    
    try:
        from websearch_utils import SearchResult, get_web_search_manager, search_and_summarize
        
        # æµ‹è¯•1: æœç´¢ç»“æœåˆ›å»º
        print("æµ‹è¯•1: æœç´¢ç»“æœåˆ›å»º")
        result = SearchResult(
            title="Pythonç¼–ç¨‹æ•™ç¨‹",
            url="https://python.org/tutorial",
            snippet="Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€...",
            source="google",
            metadata={"rank": 1, "date": "2024-01-01"}
        )
        print(f"âœ… æœç´¢ç»“æœ: {result.title} - {result.url}")
        assert result.title == "Pythonç¼–ç¨‹æ•™ç¨‹", "æ ‡é¢˜é”™è¯¯"
        assert result.source == "google", "æ¥æºé”™è¯¯"
        
        # æµ‹è¯•2: æœç´¢ç»“æœè½¬æ¢
        print("æµ‹è¯•2: æœç´¢ç»“æœè½¬æ¢")
        if hasattr(result, 'to_dict'):
            result_dict = result.to_dict()
            print(f"âœ… å­—å…¸è½¬æ¢: {len(result_dict)}ä¸ªå­—æ®µ")
            assert "title" in result_dict, "å­—å…¸ç¼ºå°‘titleå­—æ®µ"
            assert "url" in result_dict, "å­—å…¸ç¼ºå°‘urlå­—æ®µ"
        
        # æµ‹è¯•3: ç®¡ç†å™¨åŠŸèƒ½
        print("æµ‹è¯•3: ç®¡ç†å™¨åŠŸèƒ½")
        manager = get_web_search_manager()
        print(f"âœ… ç®¡ç†å™¨: é»˜è®¤æä¾›å•†={manager.default_provider}")
        assert manager.default_provider == "duckduckgo", "é»˜è®¤æä¾›å•†é”™è¯¯"
        
        # æµ‹è¯•4: æœç´¢æ‘˜è¦åŠŸèƒ½
        print("æµ‹è¯•4: æœç´¢æ‘˜è¦åŠŸèƒ½")
        # åˆ›å»ºæ¨¡æ‹Ÿæœç´¢ç»“æœ
        mock_results = [result]
        summary = {
            "query": "test query",
            "total_results": len(mock_results),
            "results": [r.to_dict() if hasattr(r, 'to_dict') else {"title": r.title, "url": r.url} for r in mock_results],
            "sources": list(set(r.source for r in mock_results))
        }
        print(f"âœ… æœç´¢æ‘˜è¦: {summary['total_results']}ä¸ªç»“æœ")
        assert summary["total_results"] == 1, "ç»“æœæ•°é‡é”™è¯¯"
        
        print("ğŸ‰ Webæœç´¢åŠŸèƒ½æµ‹è¯•å…¨éƒ¨é€šè¿‡!")
        return True
        
    except Exception as e:
        print(f"âŒ Webæœç´¢åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_audio_functionality():
    """æµ‹è¯•éŸ³é¢‘å¤„ç†åŠŸèƒ½æ‰§è¡Œ"""
    print("\n" + "="*50)
    print("ğŸ”Š éŸ³é¢‘å¤„ç†åŠŸèƒ½æ‰§è¡Œæµ‹è¯•")
    print("="*50)
    
    try:
        from audio_utils import get_tts_manager, AudioUtils
        
        # æµ‹è¯•1: TTSç®¡ç†å™¨
        print("æµ‹è¯•1: TTSç®¡ç†å™¨")
        manager = get_tts_manager()
        print(f"âœ… TTSç®¡ç†å™¨: é»˜è®¤æä¾›å•†={manager.default_provider}")
        assert manager.default_provider == "amazon_polly", "é»˜è®¤æä¾›å•†é”™è¯¯"
        
        # æµ‹è¯•2: æ–‡æœ¬åˆ†å‰²åŠŸèƒ½
        print("æµ‹è¯•2: æ–‡æœ¬åˆ†å‰²åŠŸèƒ½")
        long_text = "è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„æ–‡æœ¬ï¼Œéœ€è¦åˆ†å‰²æˆå¤šä¸ªéƒ¨åˆ†è¿›è¡ŒTTSå¤„ç†ã€‚" * 50
        chunks = AudioUtils.split_text_for_tts(long_text, max_length=100)
        print(f"âœ… æ–‡æœ¬åˆ†å‰²: {len(long_text)}å­—ç¬¦ -> {len(chunks)}ä¸ªå—")
        assert len(chunks) > 1, "åº”è¯¥ç”Ÿæˆå¤šä¸ªå—"
        assert all(len(chunk) <= 100 for chunk in chunks), "å—é•¿åº¦è¶…é™"
        
        # æµ‹è¯•3: éŸ³é¢‘æ—¶é•¿è·å–ï¼ˆæ¨¡æ‹Ÿï¼‰
        print("æµ‹è¯•3: éŸ³é¢‘æ—¶é•¿è·å–")
        # ç”±äºæ²¡æœ‰çœŸå®éŸ³é¢‘æ–‡ä»¶ï¼Œæµ‹è¯•ä¼šè¿”å›0.0ï¼ˆæ— librosaæ—¶çš„é»˜è®¤å€¼ï¼‰
        duration = AudioUtils.get_audio_duration("nonexistent.mp3")
        print(f"âœ… éŸ³é¢‘æ—¶é•¿: {duration}ç§’ (æ¨¡æ‹Ÿ)")
        assert isinstance(duration, (int, float)), "æ—¶é•¿ç±»å‹é”™è¯¯"
        
        print("ğŸ‰ éŸ³é¢‘å¤„ç†åŠŸèƒ½æµ‹è¯•å…¨éƒ¨é€šè¿‡!")
        return True
        
    except Exception as e:
        print(f"âŒ éŸ³é¢‘å¤„ç†åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_visualization_functionality():
    """æµ‹è¯•å¯è§†åŒ–åŠŸèƒ½æ‰§è¡Œ"""
    print("\n" + "="*50)
    print("ğŸ“Š å¯è§†åŒ–åŠŸèƒ½æ‰§è¡Œæµ‹è¯•")
    print("="*50)
    
    try:
        from viz_utils import get_performance_profiler, get_debug_logger, profile_execution
        
        # æµ‹è¯•1: æ€§èƒ½åˆ†æå™¨
        print("æµ‹è¯•1: æ€§èƒ½åˆ†æå™¨")
        profiler = get_performance_profiler()
        
        @profiler.profile_function("test_function")
        def test_func():
            time.sleep(0.01)  # 10ms
            return "test_result"
        
        result = test_func()
        report = profiler.get_performance_report()
        print(f"âœ… æ€§èƒ½åˆ†æ: å‡½æ•°æ‰§è¡Œå®Œæˆï¼Œç»“æœ={result}")
        print(f"âœ… æ€§èƒ½æŠ¥å‘Š: {len(report)}ä¸ªå‡½æ•°è¢«ç›‘æ§")
        assert result == "test_result", "å‡½æ•°ç»“æœé”™è¯¯"
        assert "test_function" in report, "æ€§èƒ½æŠ¥å‘Šç¼ºå°‘å‡½æ•°"
        
        # æµ‹è¯•2: è°ƒè¯•æ—¥å¿—
        print("æµ‹è¯•2: è°ƒè¯•æ—¥å¿—")
        debug_logger = get_debug_logger()
        debug_logger.log_variable("test_var", "test_value", "test_context")
        debug_logger.log_variable("another_var", 42, "number_context")
        
        debug_data = debug_logger.get_debug_data()
        print(f"âœ… è°ƒè¯•æ—¥å¿—: è®°å½•äº†{len(debug_data)}ä¸ªå˜é‡")
        assert "test_var" in debug_data, "ç¼ºå°‘test_var"
        assert "another_var" in debug_data, "ç¼ºå°‘another_var"
        assert debug_data["test_var"][0]["value"] == "test_value", "å˜é‡å€¼é”™è¯¯"
        
        # æµ‹è¯•3: å…¨å±€è£…é¥°å™¨
        print("æµ‹è¯•3: å…¨å±€è£…é¥°å™¨")
        @profile_execution("global_test_func")
        def global_test():
            time.sleep(0.005)  # 5ms
            return "global_result"
        
        global_result = global_test()
        global_profiler = get_performance_profiler()
        global_report = global_profiler.get_performance_report()
        print(f"âœ… å…¨å±€è£…é¥°å™¨: ç»“æœ={global_result}")
        assert global_result == "global_result", "å…¨å±€å‡½æ•°ç»“æœé”™è¯¯"
        assert "global_test_func" in global_report, "å…¨å±€æ€§èƒ½æŠ¥å‘Šç¼ºå°‘å‡½æ•°"
        
        print("ğŸ‰ å¯è§†åŒ–åŠŸèƒ½æµ‹è¯•å…¨éƒ¨é€šè¿‡!")
        return True
        
    except Exception as e:
        print(f"âŒ å¯è§†åŒ–åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_integration_scenarios():
    """æµ‹è¯•é›†æˆåœºæ™¯"""
    print("\n" + "="*50)
    print("ğŸ”— é›†æˆåœºæ™¯æµ‹è¯•")
    print("="*50)
    
    try:
        # åœºæ™¯1: LLM + æ–‡æœ¬å¤„ç† + å‘é‡åŒ–
        print("åœºæ™¯1: LLM + æ–‡æœ¬å¤„ç† + å‘é‡åŒ–")
        from LLMProvider import generate
        from text_utils import chunk_text
        from vector_utils import VectorDocument
        import numpy as np
        
        # ç”Ÿæˆæ–‡æœ¬
        generated_text = generate("å†™ä¸€ç¯‡å…³äºæœºå™¨å­¦ä¹ çš„ç®€çŸ­ä»‹ç»")
        print(f"âœ… æ–‡æœ¬ç”Ÿæˆ: {len(generated_text)}å­—ç¬¦")
        
        # æ–‡æœ¬åˆ†å—
        chunks = chunk_text(generated_text, chunk_size=100)
        print(f"âœ… æ–‡æœ¬åˆ†å—: {len(chunks)}ä¸ªå—")
        
        # åˆ›å»ºå‘é‡æ–‡æ¡£
        vector_docs = []
        for i, chunk in enumerate(chunks[:3]):  # åªå¤„ç†å‰3ä¸ªå—
            # æ¨¡æ‹Ÿå‘é‡ï¼ˆå®é™…åº”ç”¨ä¸­ä¼šä½¿ç”¨çœŸå®çš„åµŒå…¥ï¼‰
            mock_vector = np.random.rand(128).tolist()
            doc = VectorDocument(
                id=f"chunk_{i}",
                vector=mock_vector,
                metadata={"chunk_index": i, "source": "llm_generated"},
                text=chunk.text
            )
            vector_docs.append(doc)
        
        print(f"âœ… å‘é‡æ–‡æ¡£åˆ›å»º: {len(vector_docs)}ä¸ªæ–‡æ¡£")
        assert len(vector_docs) <= 3, "æ–‡æ¡£æ•°é‡é”™è¯¯"
        
        # åœºæ™¯2: æ€§èƒ½ç›‘æ§ + è°ƒè¯•æ—¥å¿—
        print("åœºæ™¯2: æ€§èƒ½ç›‘æ§ + è°ƒè¯•æ—¥å¿—")
        from viz_utils import get_performance_profiler, get_debug_logger
        
        profiler = get_performance_profiler()
        debug_logger = get_debug_logger()
        
        @profiler.profile_function("integration_test")
        def integration_function(data):
            debug_logger.log_variable("input_data", data, "integration_test")
            time.sleep(0.01)
            result = f"processed_{data}"
            debug_logger.log_variable("output_result", result, "integration_test")
            return result
        
        test_result = integration_function("test_data")
        
        # æ£€æŸ¥ç»“æœ
        perf_report = profiler.get_performance_report()
        debug_data = debug_logger.get_debug_data()
        
        print(f"âœ… é›†æˆå‡½æ•°æ‰§è¡Œ: {test_result}")
        print(f"âœ… æ€§èƒ½ç›‘æ§: {len(perf_report)}ä¸ªå‡½æ•°")
        print(f"âœ… è°ƒè¯•æ—¥å¿—: {len(debug_data)}ä¸ªå˜é‡")
        
        assert "integration_test" in perf_report, "æ€§èƒ½æŠ¥å‘Šç¼ºå°‘é›†æˆå‡½æ•°"
        assert "input_data" in debug_data, "è°ƒè¯•æ—¥å¿—ç¼ºå°‘è¾“å…¥æ•°æ®"
        assert "output_result" in debug_data, "è°ƒè¯•æ—¥å¿—ç¼ºå°‘è¾“å‡ºç»“æœ"
        
        print("ğŸ‰ é›†æˆåœºæ™¯æµ‹è¯•å…¨éƒ¨é€šè¿‡!")
        return True
        
    except Exception as e:
        print(f"âŒ é›†æˆåœºæ™¯æµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ AIå·¥å…·åŒ…åŠŸèƒ½æ‰§è¡Œæµ‹è¯•")
    print("æµ‹è¯•æ—¶é—´:", time.strftime("%Y-%m-%d %H:%M:%S"))
    
    # è¿è¡Œæ‰€æœ‰åŠŸèƒ½æµ‹è¯•
    test_results = {
        "LLMåŠŸèƒ½": test_llm_functionality(),
        "æ–‡æœ¬å¤„ç†": test_text_processing_functionality(),
        "å‘é‡æ•°æ®åº“": test_vector_functionality(),
        "Webæœç´¢": test_websearch_functionality(),
        "éŸ³é¢‘å¤„ç†": test_audio_functionality(),
        "å¯è§†åŒ–": test_visualization_functionality(),
        "é›†æˆåœºæ™¯": test_integration_scenarios()
    }
    
    # ç»Ÿè®¡ç»“æœ
    print("\n" + "="*60)
    print("ğŸ“‹ æµ‹è¯•ç»“æœæ€»ç»“")
    print("="*60)
    
    passed = sum(test_results.values())
    total = len(test_results)
    
    for test_name, result in test_results.items():
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name:12} : {status}")
    
    print(f"\næ€»ä½“ç»“æœ: {passed}/{total} ({passed/total*100:.1f}%) é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰åŠŸèƒ½æµ‹è¯•éƒ½é€šè¿‡äº†ï¼AIå·¥å…·åŒ…å·²å‡†å¤‡å°±ç»ªï¼")
        return True
    else:
        print(f"âš ï¸  æœ‰ {total-passed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³åŠŸèƒ½ã€‚")
        return False


if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)
