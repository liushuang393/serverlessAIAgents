{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f3dc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 12.0 GB\n",
      "=== vLLM段階的テスト開始 ===\n",
      "\n",
      "=== Testing microsoft/DialoGPT-medium ===\n",
      "Model: microsoft/DialoGPT-medium\n",
      "  max_position_embeddings: 1024\n",
      "  model_max_length: 20\n",
      "INFO 07-23 17:38:15 [config.py:841] This model supports multiple tasks: {'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.\n",
      "ERROR 07-23 17:38:16 [config.py:130] Error retrieving safetensors: 'microsoft/DialoGPT-medium' is not a safetensors repo. Couldn't find 'model.safetensors.index.json' or 'model.safetensors' files., retrying 1 of 2\n",
      "ERROR 07-23 17:38:18 [config.py:128] Error retrieving safetensors: 'microsoft/DialoGPT-medium' is not a safetensors repo. Couldn't find 'model.safetensors.index.json' or 'model.safetensors' files.\n",
      "INFO 07-23 17:38:18 [config.py:3368] Downcasting torch.float32 to torch.bfloat16.\n",
      "INFO 07-23 17:38:18 [config.py:1472] Using max model len 1024\n",
      "INFO 07-23 17:38:18 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 07-23 17:38:18 [config.py:2321] max_num_batched_tokens (8192) exceeds max_num_seqs * max_model_len (1024). This may lead to unexpected behavior.\n",
      "WARNING 07-23 17:38:18 [config.py:2321] max_num_batched_tokens (8192) exceeds max_num_seqs * max_model_len (1024). This may lead to unexpected behavior.\n",
      "WARNING 07-23 17:38:18 [cuda.py:102] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 07-23 17:38:19 [__init__.py:2662] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 07-23 17:38:23 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 07-23 17:38:25 [core.py:526] Waiting for init message from front-end.\n",
      "INFO 07-23 17:38:25 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='microsoft/DialoGPT-medium', speculative_config=None, tokenizer='microsoft/DialoGPT-medium', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=microsoft/DialoGPT-medium, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}\n",
      "INFO 07-23 17:38:26 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 07-23 17:38:26 [interface.py:382] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "WARNING 07-23 17:38:26 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 07-23 17:38:26 [gpu_model_runner.py:1770] Starting to load model microsoft/DialoGPT-medium...\n",
      "INFO 07-23 17:38:26 [gpu_model_runner.py:1775] Loading model from scratch...\n",
      "INFO 07-23 17:38:26 [cuda.py:284] Using Flash Attention backend on V1 engine.\n",
      "INFO 07-23 17:38:26 [weight_utils.py:292] Using model weights format ['*.bin']\n",
      "INFO 07-23 17:38:58 [weight_utils.py:308] Time spent downloading weights for microsoft/DialoGPT-medium: 31.462548 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading pt checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.02s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.02s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-23 17:39:00 [default_loader.py:272] Loading weights took 2.03 seconds\n",
      "INFO 07-23 17:39:00 [gpu_model_runner.py:1801] Model loading took 0.6611 GiB and 33.998652 seconds\n",
      "INFO 07-23 17:39:01 [gpu_worker.py:232] Available KV cache memory: 7.48 GiB\n",
      "INFO 07-23 17:39:02 [kv_cache_utils.py:716] GPU KV cache size: 81,696 tokens\n",
      "INFO 07-23 17:39:02 [kv_cache_utils.py:720] Maximum concurrency for 1,024 tokens per request: 79.78x\n",
      "INFO 07-23 17:39:02 [core.py:172] init engine (profile, create kv cache, warmup model) took 1.79 seconds\n",
      "✅ microsoft/DialoGPT-medium ロード成功\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0d1082b0c14f2e85883f1b58309be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b367e136bf63412da2c96ee786007877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ テキスト生成成功: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W723 17:39:03.781756885 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== テスト結果: 1/1 成功 ===\n"
     ]
    }
   ],
   "source": [
    "# vLLM基本機能テスト - 修正版\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoConfig\n",
    "\n",
    "def check_model_config(model_name):\n",
    "    \"\"\"モデル設定を事前確認\"\"\"\n",
    "    try:\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        max_pos = getattr(config, 'max_position_embeddings', None)\n",
    "        model_max_len = getattr(config, 'max_length', None)\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(f\"  max_position_embeddings: {max_pos}\")\n",
    "        print(f\"  model_max_length: {model_max_len}\")\n",
    "        return max_pos\n",
    "    except Exception as e:\n",
    "        print(f\"設定確認エラー: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_vllm_with_model(model_name, max_len=512, gpu_util=0.7):\n",
    "    \"\"\"指定されたモデルでvLLMテスト\"\"\"\n",
    "    print(f\"\\n=== Testing {model_name} ===\")\n",
    "    \n",
    "    # モデル設定確認\n",
    "    max_pos = check_model_config(model_name)\n",
    "    if max_pos and max_len > max_pos:\n",
    "        max_len = min(max_len, max_pos)\n",
    "        print(f\"max_model_lenを{max_len}に調整\")\n",
    "    \n",
    "    try:\n",
    "        # vLLMモデル初期化\n",
    "        llm = LLM(\n",
    "            model=model_name,\n",
    "            trust_remote_code=True,\n",
    "            max_model_len=max_len,\n",
    "            max_num_seqs=1,\n",
    "            gpu_memory_utilization=gpu_util,  # GPU使用率を下げる\n",
    "            enforce_eager=True,  # メモリ効率を改善\n",
    "        )\n",
    "        print(f\"✅ {model_name} ロード成功\")\n",
    "        \n",
    "        # テキスト生成テスト\n",
    "        prompts = [\"Hello, how are you?\"]\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0.2,\n",
    "            max_tokens=32,\n",
    "            top_p=0.9\n",
    "        )\n",
    "        \n",
    "        outputs = llm.generate(prompts, sampling_params)\n",
    "        result = outputs[0].outputs[0].text\n",
    "        print(f\"✅ テキスト生成成功: {result.strip()}\")\n",
    "        \n",
    "        # メモリクリーンアップ\n",
    "        del llm\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ {model_name} テスト失敗: {e}\")\n",
    "        # メモリクリーンアップ\n",
    "        torch.cuda.empty_cache()\n",
    "        return False\n",
    "\n",
    "# GPU情報確認\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"GPU Memory: {gpu_memory:.1f} GB\")\n",
    "else:\n",
    "    print(\"CUDA not available\")\n",
    "\n",
    "# 段階的テスト\n",
    "models_to_test = [\n",
    "    # (\"gpt2\", 512, 0.6),  # 最軽量\n",
    "    # (\"microsoft/DialoGPT-small\", 512, 0.7),  # 小サイズ\n",
    "    (\"microsoft/DialoGPT-medium\", 1024, 0.7),  # 中サイズ（修正版）\n",
    "]\n",
    "\n",
    "print(\"=== vLLM段階的テスト開始 ===\")\n",
    "success_count = 0\n",
    "\n",
    "for model_name, max_len, gpu_util in models_to_test:\n",
    "    if test_vllm_with_model(model_name, max_len, gpu_util):\n",
    "        success_count += 1\n",
    "    else:\n",
    "        print(f\"⚠️ {model_name}をスキップして次のモデルをテスト\")\n",
    "\n",
    "print(f\"\\n=== テスト結果: {success_count}/{len(models_to_test)} 成功 ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8efe983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face モデルキャッシュテスト\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "\n",
    "def test_model_cache():\n",
    "    \"\"\"モデルキャッシュ機能をテスト\"\"\"\n",
    "    try:\n",
    "        print(\"=== Hugging Face モデルキャッシュテスト ===\")\n",
    "        \n",
    "        # 軽量モデルでテスト（BERT-base-uncased）\n",
    "        model_name = \"bert-base-uncased\"\n",
    "        print(f\"モデル '{model_name}' をダウンロード中...\")\n",
    "        \n",
    "        # モデルとトークナイザーをロード（自動キャッシュ）\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # config.jsonファイルのローカルキャッシュパスを表示\n",
    "        config_path = hf_hub_download(repo_id=model_name, filename=\"config.json\")\n",
    "        print(f\"✅ モデルキャッシュパス: {config_path}\")\n",
    "        \n",
    "        # 簡単な推論テスト\n",
    "        text = \"Hello, this is a test.\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        print(f\"✅ 推論テスト成功: 出力形状 {outputs.last_hidden_state.shape}\")\n",
    "        \n",
    "        # メモリクリーンアップ\n",
    "        del model, tokenizer\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ モデルキャッシュテスト失敗: {e}\")\n",
    "        return False\n",
    "\n",
    "# テスト実行\n",
    "test_model_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508b82fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ImageAsset...\n",
      "Image loaded successfully: (1770, 1180)\n",
      "Loading model...\n",
      "INFO 07-23 17:21:43 [config.py:240] Replacing legacy 'type' key with 'rope_type'\n",
      "WARNING 07-23 17:21:43 [config.py:247] Replacing legacy rope_type 'su' with 'longrope'\n",
      "INFO 07-23 17:21:43 [config.py:841] This model supports multiple tasks: {'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 07-23 17:21:43 [config.py:1472] Using max model len 2048\n",
      "INFO 07-23 17:21:43 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 07-23 17:21:43 [config.py:2321] max_num_batched_tokens (8192) exceeds max_num_seqs * max_model_len (2048). This may lead to unexpected behavior.\n",
      "WARNING 07-23 17:21:43 [config.py:2321] max_num_batched_tokens (8192) exceeds max_num_seqs * max_model_len (2048). This may lead to unexpected behavior.\n",
      "WARNING 07-23 17:21:44 [__init__.py:2662] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 07-23 17:21:51 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 07-23 17:21:54 [core.py:526] Waiting for init message from front-end.\n",
      "INFO 07-23 17:21:54 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='microsoft/Phi-3.5-vision-instruct', speculative_config=None, tokenizer='microsoft/Phi-3.5-vision-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=microsoft/Phi-3.5-vision-instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "ERROR 07-23 17:21:55 [core.py:586] EngineCore failed to start.\n",
      "ERROR 07-23 17:21:55 [core.py:586] Traceback (most recent call last):\n",
      "ERROR 07-23 17:21:55 [core.py:586]   File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 577, in run_engine_core\n",
      "ERROR 07-23 17:21:55 [core.py:586]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "ERROR 07-23 17:21:55 [core.py:586]   File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 404, in __init__\n",
      "ERROR 07-23 17:21:55 [core.py:586]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "ERROR 07-23 17:21:55 [core.py:586]   File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 75, in __init__\n",
      "ERROR 07-23 17:21:55 [core.py:586]     self.model_executor = executor_class(vllm_config)\n",
      "ERROR 07-23 17:21:55 [core.py:586]   File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 53, in __init__\n",
      "ERROR 07-23 17:21:55 [core.py:586]     self._init_executor()\n",
      "ERROR 07-23 17:21:55 [core.py:586]   File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 47, in _init_executor\n",
      "ERROR 07-23 17:21:55 [core.py:586]     self.collective_rpc(\"init_device\")\n",
      "ERROR 07-23 17:21:55 [core.py:586]   File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 57, in collective_rpc\n",
      "ERROR 07-23 17:21:55 [core.py:586]     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "ERROR 07-23 17:21:55 [core.py:586]   File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/utils/__init__.py\", line 2736, in run_method\n",
      "ERROR 07-23 17:21:55 [core.py:586]     return func(*args, **kwargs)\n",
      "ERROR 07-23 17:21:55 [core.py:586]   File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 606, in init_device\n",
      "ERROR 07-23 17:21:55 [core.py:586]     self.worker.init_device()  # type: ignore\n",
      "ERROR 07-23 17:21:55 [core.py:586]   File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 145, in init_device\n",
      "ERROR 07-23 17:21:55 [core.py:586]     raise ValueError(\n",
      "ERROR 07-23 17:21:55 [core.py:586] ValueError: Free memory on device (10.53/12.0 GiB) on startup is less than desired GPU memory utilization (0.9, 10.8 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process EngineCore_0:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 590, in run_engine_core\n",
      "    raise e\n",
      "  File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 577, in run_engine_core\n",
      "    engine_core = EngineCoreProc(*args, **kwargs)\n",
      "  File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 404, in __init__\n",
      "    super().__init__(vllm_config, executor_class, log_stats,\n",
      "  File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 75, in __init__\n",
      "    self.model_executor = executor_class(vllm_config)\n",
      "  File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 53, in __init__\n",
      "    self._init_executor()\n",
      "  File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 47, in _init_executor\n",
      "    self.collective_rpc(\"init_device\")\n",
      "  File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 57, in collective_rpc\n",
      "    answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "  File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/utils/__init__.py\", line 2736, in run_method\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 606, in init_device\n",
      "    self.worker.init_device()  # type: ignore\n",
      "  File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 145, in init_device\n",
      "    raise ValueError(\n",
      "ValueError: Free memory on device (10.53/12.0 GiB) on startup is less than desired GPU memory utilization (0.9, 10.8 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loading failed: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "Trying alternative approach...\n",
      "Testing basic vLLM functionality...\n",
      "INFO 07-23 17:22:03 [config.py:841] This model supports multiple tasks: {'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "ERROR 07-23 17:22:03 [config.py:130] Error retrieving safetensors: 'microsoft/DialoGPT-medium' is not a safetensors repo. Couldn't find 'model.safetensors.index.json' or 'model.safetensors' files., retrying 1 of 2\n",
      "ERROR 07-23 17:22:05 [config.py:128] Error retrieving safetensors: 'microsoft/DialoGPT-medium' is not a safetensors repo. Couldn't find 'model.safetensors.index.json' or 'model.safetensors' files.\n",
      "INFO 07-23 17:22:05 [config.py:3368] Downcasting torch.float32 to torch.bfloat16.\n",
      "INFO 07-23 17:22:05 [config.py:1472] Using max model len 512\n",
      "INFO 07-23 17:22:05 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 07-23 17:22:10 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 07-23 17:22:11 [core.py:526] Waiting for init message from front-end.\n",
      "INFO 07-23 17:22:11 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='microsoft/DialoGPT-medium', speculative_config=None, tokenizer='microsoft/DialoGPT-medium', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=512, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=microsoft/DialoGPT-medium, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "ERROR 07-23 17:22:12 [core.py:586] EngineCore failed to start.\n",
      "ERROR 07-23 17:22:12 [core.py:586] Traceback (most recent call last):\n",
      "ERROR 07-23 17:22:12 [core.py:586]   File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 577, in run_engine_core\n",
      "ERROR 07-23 17:22:12 [core.py:586]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "ERROR 07-23 17:22:12 [core.py:586]   File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 404, in __init__\n",
      "ERROR 07-23 17:22:12 [core.py:586]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "ERROR 07-23 17:22:12 [core.py:586]   File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 75, in __init__\n",
      "ERROR 07-23 17:22:12 [core.py:586]     self.model_executor = executor_class(vllm_config)\n",
      "ERROR 07-23 17:22:12 [core.py:586]   File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 53, in __init__\n",
      "ERROR 07-23 17:22:12 [core.py:586]     self._init_executor()\n",
      "ERROR 07-23 17:22:12 [core.py:586]   File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 47, in _init_executor\n",
      "ERROR 07-23 17:22:12 [core.py:586]     self.collective_rpc(\"init_device\")\n",
      "ERROR 07-23 17:22:12 [core.py:586]   File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 57, in collective_rpc\n",
      "ERROR 07-23 17:22:12 [core.py:586]     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "ERROR 07-23 17:22:12 [core.py:586]   File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/utils/__init__.py\", line 2736, in run_method\n",
      "ERROR 07-23 17:22:12 [core.py:586]     return func(*args, **kwargs)\n",
      "ERROR 07-23 17:22:12 [core.py:586]   File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 606, in init_device\n",
      "ERROR 07-23 17:22:12 [core.py:586]     self.worker.init_device()  # type: ignore\n",
      "ERROR 07-23 17:22:12 [core.py:586]   File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 145, in init_device\n",
      "ERROR 07-23 17:22:12 [core.py:586]     raise ValueError(\n",
      "ERROR 07-23 17:22:12 [core.py:586] ValueError: Free memory on device (10.5/12.0 GiB) on startup is less than desired GPU memory utilization (0.9, 10.8 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process EngineCore_0:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 590, in run_engine_core\n",
      "    raise e\n",
      "  File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 577, in run_engine_core\n",
      "    engine_core = EngineCoreProc(*args, **kwargs)\n",
      "  File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 404, in __init__\n",
      "    super().__init__(vllm_config, executor_class, log_stats,\n",
      "  File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 75, in __init__\n",
      "    self.model_executor = executor_class(vllm_config)\n",
      "  File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 53, in __init__\n",
      "    self._init_executor()\n",
      "  File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 47, in _init_executor\n",
      "    self.collective_rpc(\"init_device\")\n",
      "  File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 57, in collective_rpc\n",
      "    answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "  File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/utils/__init__.py\", line 2736, in run_method\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 606, in init_device\n",
      "    self.worker.init_device()  # type: ignore\n",
      "  File \"/home/lius/miniconda3/envs/agent_ragenv/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 145, in init_device\n",
      "    raise ValueError(\n",
      "ValueError: Free memory on device (10.5/12.0 GiB) on startup is less than desired GPU memory utilization (0.9, 10.8 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text model also failed: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "vLLM may require GPU or specific model configurations\n"
     ]
    }
   ],
   "source": [
    "from vllm.assets.image import ImageAsset\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoProcessor\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# ImageAssetのテスト\n",
    "print(\"Testing ImageAsset...\")\n",
    "image_asset = ImageAsset(\"cherry_blossom\")\n",
    "print(f\"Image loaded successfully: {image_asset.pil_image.size}\")\n",
    "\n",
    "# prepare model (より軽量なモデルを使用)\n",
    "print(\"Loading model...\")\n",
    "model_id = \"microsoft/Phi-3.5-vision-instruct\"\n",
    "try:\n",
    "    llm = LLM(\n",
    "        model=model_id,  # より軽量なビジョンモデル\n",
    "        trust_remote_code=True,\n",
    "        max_model_len=2048,  # メモリ使用量を削減\n",
    "        max_num_seqs=1,      # 並列処理数を削減\n",
    "    )\n",
    "    print(\"Model loaded successfully\")\n",
    "    \n",
    "        # for best performance, use num_crops=4 for multi-frame, num_crops=16 for single-frame.\n",
    "    processor = AutoProcessor.from_pretrained(model_id,\n",
    "    trust_remote_code=True,\n",
    "    num_crops=4\n",
    "    )\n",
    "    \n",
    "    images = []\n",
    "    placeholder = \"\"\n",
    "\n",
    "    # Note: if OOM, you might consider reduce number of frames in this example.\n",
    "    for i in range(1,20):\n",
    "        url = f\"https://image.slidesharecdn.com/azureintroduction-191206101932/75/Introduction-to-Microsoft-Azure-Cloud-{i}-2048.jpg\"\n",
    "        images.append(Image.open(requests.get(url, stream=True).raw))\n",
    "        placeholder += f\"<|image_{i}|>\\n\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": placeholder+\"Summarize the deck of slides.\"},\n",
    "    ]\n",
    "\n",
    "    prompt = processor.tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = processor(prompt, images, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "    generation_args = {\n",
    "    \"max_new_tokens\": 1000,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "    }\n",
    "\n",
    "    generate_ids = model.generate(**inputs, \n",
    "    eos_token_id=processor.tokenizer.eos_token_id, \n",
    "    **generation_args\n",
    "    )\n",
    "\n",
    "    # remove input tokens \n",
    "    generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
    "    response = processor.batch_decode(generate_ids, \n",
    "    skip_special_tokens=True, \n",
    "    clean_up_tokenization_spaces=False)[0] \n",
    "\n",
    "    print(response)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Model loading failed: {e}\")\n",
    "    print(\"Trying alternative approach...\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_ragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
