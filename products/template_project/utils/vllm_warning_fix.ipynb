{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b9c2d3",
   "metadata": {},
   "source": [
    "# vLLM警告抑制と最適化ノートブック\n",
    "\n",
    "このノートブックでは、vLLMの使用時に表示される様々な警告を抑制し、パフォーマンスを最適化する方法を示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f5d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリをインポート\n",
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6e7f8",
   "metadata": {},
   "source": [
    "## 1. 警告抑制設定\n",
    "\n",
    "vLLMの警告を抑制するための設定を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7f8g9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_warning_suppression():\n",
    "    \"\"\"vLLM関連の警告を抑制する設定\"\"\"\n",
    "    \n",
    "    # 1. 環境変数による警告抑制\n",
    "    os.environ['VLLM_LOGGING_LEVEL'] = 'ERROR'  # ログレベルをERRORに設定\n",
    "    os.environ['VLLM_ALLOW_LONG_MAX_MODEL_LEN'] = '1'  # max_model_len制限を緩和\n",
    "    os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'  # マルチプロセス方法を明示\n",
    "    os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # トークナイザー並列処理警告を抑制\n",
    "    \n",
    "    # 2. Pythonの警告フィルター設定\n",
    "    warnings.filterwarnings('ignore', category=UserWarning)\n",
    "    warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "    warnings.filterwarnings('ignore', message='.*safetensors.*')\n",
    "    warnings.filterwarnings('ignore', message='.*max_num_batched_tokens.*')\n",
    "    warnings.filterwarnings('ignore', message='.*pin_memory.*')\n",
    "    warnings.filterwarnings('ignore', message='.*FlashInfer.*')\n",
    "    \n",
    "    # 3. ログレベル調整\n",
    "    logging.getLogger('vllm').setLevel(logging.ERROR)\n",
    "    logging.getLogger('transformers').setLevel(logging.ERROR)\n",
    "    \n",
    "    print(\"✅ 警告抑制設定完了\")\n",
    "\n",
    "# 警告抑制設定を実行\n",
    "setup_warning_suppression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8g9h0",
   "metadata": {},
   "source": [
    "## 2. モデル設定の最適化\n",
    "\n",
    "モデル設定を確認し、最適なパラメータを設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8g9h0i1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_config_optimized(model_name):\n",
    "    \"\"\"最適化されたモデル設定確認\"\"\"\n",
    "    try:\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        max_pos = getattr(config, 'max_position_embeddings', None)\n",
    "        model_max_len = getattr(config, 'max_length', None)\n",
    "        \n",
    "        # 警告を抑制して情報のみ表示\n",
    "        print(f\"📋 Model: {model_name}\")\n",
    "        if max_pos:\n",
    "            print(f\"   max_position_embeddings: {max_pos}\")\n",
    "        if model_max_len:\n",
    "            print(f\"   model_max_length: {model_max_len}\")\n",
    "        \n",
    "        return max_pos\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 設定確認エラー: {e}\")\n",
    "        return None\n",
    "\n",
    "# テスト用モデルの設定を確認\n",
    "check_model_config_optimized(\"microsoft/DialoGPT-medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g9h0i1j2",
   "metadata": {},
   "source": [
    "## 3. 最適化されたLLMインスタンス作成\n",
    "\n",
    "警告を最小限に抑えたLLMインスタンスを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h0i1j2k3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimized_llm(model_name, max_len=512, gpu_util=0.7):\n",
    "    \"\"\"警告を最小化したvLLMインスタンス作成\"\"\"\n",
    "    \n",
    "    # モデル設定確認\n",
    "    max_pos = check_model_config_optimized(model_name)\n",
    "    if max_pos and max_len > max_pos:\n",
    "        max_len = min(max_len, max_pos)\n",
    "        print(f\"🔧 max_model_lenを{max_len}に調整\")\n",
    "    \n",
    "    # 最適化されたLLM設定\n",
    "    llm_config = {\n",
    "        'model': model_name,\n",
    "        'trust_remote_code': True,\n",
    "        'max_model_len': max_len,\n",
    "        'max_num_seqs': 1,\n",
    "        'gpu_memory_utilization': gpu_util,\n",
    "        'enforce_eager': True,  # CUDAグラフ無効化（警告回避）\n",
    "        'disable_log_stats': True,  # 統計ログ無効化\n",
    "        'max_num_batched_tokens': max_len,  # バッチトークン数を調整\n",
    "        'disable_custom_all_reduce': True,  # カスタム削減無効化\n",
    "        'use_v2_block_manager': False,  # V2ブロックマネージャー無効化\n",
    "    }\n",
    "    \n",
    "    # WSL環境での最適化\n",
    "    if 'microsoft' in os.uname().release.lower():\n",
    "        llm_config['pin_memory'] = False\n",
    "        print(\"🐧 WSL環境を検出、pin_memory=Falseに設定\")\n",
    "    \n",
    "    return LLM(**llm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i1j2k3l4",
   "metadata": {},
   "source": [
    "## 4. 最適化されたvLLMテスト\n",
    "\n",
    "警告を抑制した状態でvLLMをテストします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j2k3l4m5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vllm_optimized(model_name, max_len=512, gpu_util=0.7):\n",
    "    \"\"\"最適化されたvLLMテスト\"\"\"\n",
    "    print(f\"\\n🚀 === 最適化テスト: {model_name} ===\")\n",
    "    \n",
    "    try:\n",
    "        # 最適化されたLLM作成\n",
    "        llm = create_optimized_llm(model_name, max_len, gpu_util)\n",
    "        print(f\"✅ {model_name} ロード成功（警告抑制済み）\")\n",
    "        \n",
    "        # テキスト生成テスト\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0.2,\n",
    "            max_tokens=32,\n",
    "            top_p=0.9\n",
    "        )\n",
    "        \n",
    "        outputs = llm.generate([\"Hello, how are you?\"], sampling_params)\n",
    "        result = outputs[0].outputs[0].text\n",
    "        print(f\"✅ テキスト生成成功: {result.strip()}\")\n",
    "        \n",
    "        # メモリクリーンアップ\n",
    "        del llm\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ {model_name} テスト失敗: {e}\")\n",
    "        torch.cuda.empty_cache()\n",
    "        return False\n",
    "\n",
    "# 軽量モデルでテスト\n",
    "test_vllm_optimized(\"gpt2\", max_len=512, gpu_util=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k3l4m5n6",
   "metadata": {},
   "source": [
    "## 5. FlashInfer警告の解決\n",
    "\n",
    "FlashInferに関する警告を解決するためのインストール手順を示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l4m5n6o7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def install_flashinfer():\n",
    "    \"\"\"FlashInfer警告を解決するためのインストール手順\"\"\"\n",
    "    print(\"\\n🔧 === FlashInfer警告解決方法 ===\")\n",
    "    print(\"以下のコマンドでFlashInferをインストールしてください：\")\n",
    "    print(\"pip install flashinfer -f https://flashinfer.ai/whl/cu121/torch2.4/\")\n",
    "    print(\"または\")\n",
    "    print(\"pip install flashinfer\")\n",
    "    print(\"\\n注意: CUDAバージョンとPyTorchバージョンに適合するものを選択してください\")\n",
    "\n",
    "# FlashInferインストール案内を表示\n",
    "install_flashinfer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m5n6o7p8",
   "metadata": {},
   "source": [
    "## 6. 複数モデルでの最適化テスト\n",
    "\n",
    "複数のモデルで最適化設定をテストします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n6o7p8q9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU情報表示\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"🖥️ GPU Memory: {gpu_memory:.1f} GB\")\n",
    "else:\n",
    "    print(\"❌ CUDA not available\")\n",
    "\n",
    "# テストモデル一覧（軽量順）\n",
    "test_models = [\n",
    "    (\"gpt2\", 512, 0.6),\n",
    "    (\"microsoft/DialoGPT-small\", 512, 0.7),\n",
    "    (\"microsoft/DialoGPT-medium\", 1024, 0.7),\n",
    "]\n",
    "\n",
    "print(\"🎯 === 複数モデルでの最適化テスト ===\")\n",
    "success_count = 0\n",
    "\n",
    "for model_name, max_len, gpu_util in test_models:\n",
    "    if test_vllm_optimized(model_name, max_len, gpu_util):\n",
    "        success_count += 1\n",
    "    else:\n",
    "        print(f\"⚠️ {model_name}をスキップして次のモデルをテスト\")\n",
    "\n",
    "print(f\"\\n🎉 === テスト結果: {success_count}/{len(test_models)} 成功 ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_ragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
