{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b9c2d3",
   "metadata": {},
   "source": [
    "# vLLMè­¦å‘ŠæŠ‘åˆ¶ã¨æœ€é©åŒ–ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€vLLMã®ä½¿ç”¨æ™‚ã«è¡¨ç¤ºã•ã‚Œã‚‹æ§˜ã€…ãªè­¦å‘Šã‚’æŠ‘åˆ¶ã—ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æœ€é©åŒ–ã™ã‚‹æ–¹æ³•ã‚’ç¤ºã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f5d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6e7f8",
   "metadata": {},
   "source": [
    "## 1. è­¦å‘ŠæŠ‘åˆ¶è¨­å®š\n",
    "\n",
    "vLLMã®è­¦å‘Šã‚’æŠ‘åˆ¶ã™ã‚‹ãŸã‚ã®è¨­å®šã‚’è¡Œã„ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7f8g9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_warning_suppression():\n",
    "    \"\"\"vLLMé–¢é€£ã®è­¦å‘Šã‚’æŠ‘åˆ¶ã™ã‚‹è¨­å®š\"\"\"\n",
    "    \n",
    "    # 1. ç’°å¢ƒå¤‰æ•°ã«ã‚ˆã‚‹è­¦å‘ŠæŠ‘åˆ¶\n",
    "    os.environ['VLLM_LOGGING_LEVEL'] = 'ERROR'  # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’ERRORã«è¨­å®š\n",
    "    os.environ['VLLM_ALLOW_LONG_MAX_MODEL_LEN'] = '1'  # max_model_lenåˆ¶é™ã‚’ç·©å’Œ\n",
    "    os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'  # ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹æ–¹æ³•ã‚’æ˜ç¤º\n",
    "    os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ä¸¦åˆ—å‡¦ç†è­¦å‘Šã‚’æŠ‘åˆ¶\n",
    "    \n",
    "    # 2. Pythonã®è­¦å‘Šãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®š\n",
    "    warnings.filterwarnings('ignore', category=UserWarning)\n",
    "    warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "    warnings.filterwarnings('ignore', message='.*safetensors.*')\n",
    "    warnings.filterwarnings('ignore', message='.*max_num_batched_tokens.*')\n",
    "    warnings.filterwarnings('ignore', message='.*pin_memory.*')\n",
    "    warnings.filterwarnings('ignore', message='.*FlashInfer.*')\n",
    "    \n",
    "    # 3. ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«èª¿æ•´\n",
    "    logging.getLogger('vllm').setLevel(logging.ERROR)\n",
    "    logging.getLogger('transformers').setLevel(logging.ERROR)\n",
    "    \n",
    "    print(\"âœ… è­¦å‘ŠæŠ‘åˆ¶è¨­å®šå®Œäº†\")\n",
    "\n",
    "# è­¦å‘ŠæŠ‘åˆ¶è¨­å®šã‚’å®Ÿè¡Œ\n",
    "setup_warning_suppression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8g9h0",
   "metadata": {},
   "source": [
    "## 2. ãƒ¢ãƒ‡ãƒ«è¨­å®šã®æœ€é©åŒ–\n",
    "\n",
    "ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’ç¢ºèªã—ã€æœ€é©ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8g9h0i1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_config_optimized(model_name):\n",
    "    \"\"\"æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«è¨­å®šç¢ºèª\"\"\"\n",
    "    try:\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        max_pos = getattr(config, 'max_position_embeddings', None)\n",
    "        model_max_len = getattr(config, 'max_length', None)\n",
    "        \n",
    "        # è­¦å‘Šã‚’æŠ‘åˆ¶ã—ã¦æƒ…å ±ã®ã¿è¡¨ç¤º\n",
    "        print(f\"ğŸ“‹ Model: {model_name}\")\n",
    "        if max_pos:\n",
    "            print(f\"   max_position_embeddings: {max_pos}\")\n",
    "        if model_max_len:\n",
    "            print(f\"   model_max_length: {model_max_len}\")\n",
    "        \n",
    "        return max_pos\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ è¨­å®šç¢ºèªã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return None\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆç”¨ãƒ¢ãƒ‡ãƒ«ã®è¨­å®šã‚’ç¢ºèª\n",
    "check_model_config_optimized(\"microsoft/DialoGPT-medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g9h0i1j2",
   "metadata": {},
   "source": [
    "## 3. æœ€é©åŒ–ã•ã‚ŒãŸLLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ\n",
    "\n",
    "è­¦å‘Šã‚’æœ€å°é™ã«æŠ‘ãˆãŸLLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h0i1j2k3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimized_llm(model_name, max_len=512, gpu_util=0.7):\n",
    "    \"\"\"è­¦å‘Šã‚’æœ€å°åŒ–ã—ãŸvLLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ\"\"\"\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«è¨­å®šç¢ºèª\n",
    "    max_pos = check_model_config_optimized(model_name)\n",
    "    if max_pos and max_len > max_pos:\n",
    "        max_len = min(max_len, max_pos)\n",
    "        print(f\"ğŸ”§ max_model_lenã‚’{max_len}ã«èª¿æ•´\")\n",
    "    \n",
    "    # æœ€é©åŒ–ã•ã‚ŒãŸLLMè¨­å®š\n",
    "    llm_config = {\n",
    "        'model': model_name,\n",
    "        'trust_remote_code': True,\n",
    "        'max_model_len': max_len,\n",
    "        'max_num_seqs': 1,\n",
    "        'gpu_memory_utilization': gpu_util,\n",
    "        'enforce_eager': True,  # CUDAã‚°ãƒ©ãƒ•ç„¡åŠ¹åŒ–ï¼ˆè­¦å‘Šå›é¿ï¼‰\n",
    "        'disable_log_stats': True,  # çµ±è¨ˆãƒ­ã‚°ç„¡åŠ¹åŒ–\n",
    "        'max_num_batched_tokens': max_len,  # ãƒãƒƒãƒãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’èª¿æ•´\n",
    "        'disable_custom_all_reduce': True,  # ã‚«ã‚¹ã‚¿ãƒ å‰Šæ¸›ç„¡åŠ¹åŒ–\n",
    "        'use_v2_block_manager': False,  # V2ãƒ–ãƒ­ãƒƒã‚¯ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ç„¡åŠ¹åŒ–\n",
    "    }\n",
    "    \n",
    "    # WSLç’°å¢ƒã§ã®æœ€é©åŒ–\n",
    "    if 'microsoft' in os.uname().release.lower():\n",
    "        llm_config['pin_memory'] = False\n",
    "        print(\"ğŸ§ WSLç’°å¢ƒã‚’æ¤œå‡ºã€pin_memory=Falseã«è¨­å®š\")\n",
    "    \n",
    "    return LLM(**llm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i1j2k3l4",
   "metadata": {},
   "source": [
    "## 4. æœ€é©åŒ–ã•ã‚ŒãŸvLLMãƒ†ã‚¹ãƒˆ\n",
    "\n",
    "è­¦å‘Šã‚’æŠ‘åˆ¶ã—ãŸçŠ¶æ…‹ã§vLLMã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j2k3l4m5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vllm_optimized(model_name, max_len=512, gpu_util=0.7):\n",
    "    \"\"\"æœ€é©åŒ–ã•ã‚ŒãŸvLLMãƒ†ã‚¹ãƒˆ\"\"\"\n",
    "    print(f\"\\nğŸš€ === æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ: {model_name} ===\")\n",
    "    \n",
    "    try:\n",
    "        # æœ€é©åŒ–ã•ã‚ŒãŸLLMä½œæˆ\n",
    "        llm = create_optimized_llm(model_name, max_len, gpu_util)\n",
    "        print(f\"âœ… {model_name} ãƒ­ãƒ¼ãƒ‰æˆåŠŸï¼ˆè­¦å‘ŠæŠ‘åˆ¶æ¸ˆã¿ï¼‰\")\n",
    "        \n",
    "        # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0.2,\n",
    "            max_tokens=32,\n",
    "            top_p=0.9\n",
    "        )\n",
    "        \n",
    "        outputs = llm.generate([\"Hello, how are you?\"], sampling_params)\n",
    "        result = outputs[0].outputs[0].text\n",
    "        print(f\"âœ… ãƒ†ã‚­ã‚¹ãƒˆç”ŸæˆæˆåŠŸ: {result.strip()}\")\n",
    "        \n",
    "        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
    "        del llm\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {model_name} ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}\")\n",
    "        torch.cuda.empty_cache()\n",
    "        return False\n",
    "\n",
    "# è»½é‡ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆ\n",
    "test_vllm_optimized(\"gpt2\", max_len=512, gpu_util=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k3l4m5n6",
   "metadata": {},
   "source": [
    "## 5. FlashInferè­¦å‘Šã®è§£æ±º\n",
    "\n",
    "FlashInferã«é–¢ã™ã‚‹è­¦å‘Šã‚’è§£æ±ºã™ã‚‹ãŸã‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †ã‚’ç¤ºã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l4m5n6o7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def install_flashinfer():\n",
    "    \"\"\"FlashInferè­¦å‘Šã‚’è§£æ±ºã™ã‚‹ãŸã‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †\"\"\"\n",
    "    print(\"\\nğŸ”§ === FlashInferè­¦å‘Šè§£æ±ºæ–¹æ³• ===\")\n",
    "    print(\"ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§FlashInferã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ï¼š\")\n",
    "    print(\"pip install flashinfer -f https://flashinfer.ai/whl/cu121/torch2.4/\")\n",
    "    print(\"ã¾ãŸã¯\")\n",
    "    print(\"pip install flashinfer\")\n",
    "    print(\"\\næ³¨æ„: CUDAãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨PyTorchãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«é©åˆã™ã‚‹ã‚‚ã®ã‚’é¸æŠã—ã¦ãã ã•ã„\")\n",
    "\n",
    "# FlashInferã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¡ˆå†…ã‚’è¡¨ç¤º\n",
    "install_flashinfer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m5n6o7p8",
   "metadata": {},
   "source": [
    "## 6. è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã§ã®æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ\n",
    "\n",
    "è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã§æœ€é©åŒ–è¨­å®šã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n6o7p8q9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUæƒ…å ±è¡¨ç¤º\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"ğŸ–¥ï¸ GPU Memory: {gpu_memory:.1f} GB\")\n",
    "else:\n",
    "    print(\"âŒ CUDA not available\")\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¸€è¦§ï¼ˆè»½é‡é †ï¼‰\n",
    "test_models = [\n",
    "    (\"gpt2\", 512, 0.6),\n",
    "    (\"microsoft/DialoGPT-small\", 512, 0.7),\n",
    "    (\"microsoft/DialoGPT-medium\", 1024, 0.7),\n",
    "]\n",
    "\n",
    "print(\"ğŸ¯ === è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã§ã®æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ ===\")\n",
    "success_count = 0\n",
    "\n",
    "for model_name, max_len, gpu_util in test_models:\n",
    "    if test_vllm_optimized(model_name, max_len, gpu_util):\n",
    "        success_count += 1\n",
    "    else:\n",
    "        print(f\"âš ï¸ {model_name}ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦æ¬¡ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆ\")\n",
    "\n",
    "print(f\"\\nğŸ‰ === ãƒ†ã‚¹ãƒˆçµæœ: {success_count}/{len(test_models)} æˆåŠŸ ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_ragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
